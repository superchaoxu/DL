---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(DNN)=
# 4 Deep Neural Networks

Analyze the key computations underlying deep learning, then use them to build and train deep neural networks for computer vision tasks.

**Learning Objectives**

- Describe the successive block structure of a deep neural network.

- Build a deep L-layer neural network.

- Analyze matrix and vector dimensions to check neural network implementations.

- Use a cache to pass information from forward to back propagation.

- Explain the role of hyperparameters in deep learning.

- Build a 2-layer neural network.

---


## Deep L-layer Neural Network

[Video](https://youtu.be/xkNf7Nqn9VI)

Logistic regression is a very "shallow" model, whereas the model, with five hidden layers for example, is a much deeper model, and shallow versus depth is a matter of degree. So neural network of a single hidden layer, this would be a 2 layer neural network. Remember when we count layers in a neural network, we don't count the input layer, we just count the hidden layers as was the output layer. So, a 2 layer neural network is still quite shallow, but not as shallow as logistic regression. Technically logistic regression is a one layer neural network, but over the last several years the AI, on the machine learning community, has realized that there are functions that very deep neural networks can learn that shallower models are often unable to.

```{figure} images/4-1.png
---
height: 400px
name: 4-1
---
```

Although for any given problem, it might be hard to predict in advance exactly how deep in your network you would want. So it would be reasonable to try logistic regression, try one and then two hidden layers, and view the number of hidden layers as another hyper parameter that you could try a variety of values of, and evaluate on all that across validation data, or on your development set. 

Let's now go through the notation we used to describe deep neural networks. Here's is a four layer neural network with three hidden layers:

```{figure} images/4-2.png
---
height: 300px
name: 4-2
---
```

Notation:

- Number of layers (L): L = 4
- Number of unites in layer $l$: $n^{[l]}$.
	- In this example: $n^{[1]} = 5, \quad n^{[2]} = 5, \quad n^{[3]} = 3, \quad n^{[4]} = n^{[L]}= 1, \quad n^{[0]} = n_x = 3$ 
- Activation in layer $l$: $a^{[l]} = g^{[l]}(\mathbf{z}^{[l]})$.
- Weights for $\mathbf{z}^{[l]}$: $\mathbf{w}^{[l]}, \quad b^{[l]}$.


## Forward Propagation in a Deep Network

[Video](https://youtu.be/JT5mKK93lII)

```{figure} images/4-2.png
---
height: 300px
name: 4-2copy
---
```

$$
\begin{aligned}
\mathbf{Z}^{[l]} &= \mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \\
\mathbf{A}^{[l]} &= g^{[l]}({\mathbf{Z}^{[l]}}) \\
\end{aligned}
$$ 


$$
\begin{aligned}
\mathbf{Z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{A}^{[1]} &= g^{[1]}({\mathbf{Z}^{[1]}}) \\
\mathbf{Z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{A}^{[2]} &= g^{[2]}({\mathbf{Z}^{[2]}})  \\
\mathbf{Z}^{[3]} &= \mathbf{W}^{[3]} \mathbf{A}^{[2]} + \mathbf{b}^{[3]} \\
\mathbf{A}^{[3]} &= g^{[3]}({\mathbf{Z}^{[3]}})  \\
\mathbf{Z}^{[4]} &= \mathbf{W}^{[4]} \mathbf{A}^{[3]} + \mathbf{b}^{[4]} \\
\mathbf{A}^{[4]} &= g^{[4]}({\mathbf{Z}^{[4]}}) 
\end{aligned}
$$ 

If you look at this implementation of vectorization, it looks like that there is going to be a `for` loop here, by using `for l=1...L`. 

I know that when implementing neural networks, we usually want to get rid of explicit `for` loops. But this is one place where I don't think there's any way to implement this without an explicit `for` loop. So when implementing forward propagation, it is perfectly okay to have a `for` loop to compute the activations for layer one, then layer two, then layer three, then layer four. I don't think there is any way to do this without a `for` loop that goes from one to capital L, from one through the total number of layers in the neural network. So, in this place, it's perfectly okay to have an explicit `for` loop. 

If the pieces we've seen so far looks a little bit familiar to you, that's because what we're seeing is taking a piece very similar to what you've seen in the neural network with a single hidden layer and just repeating that more times.

Now, it turns out that we implement a deep neural network, one of the ways to increase your odds of having a bug-free implementation is to think very systematic and carefully about the matrix dimensions you're working with. So, when I'm trying to debug my own code, I'll often pull a piece of paper, and just think carefully through, so the dimensions of the matrix I'm working with. 



## Getting your Matrix Dimensions Right

[Video](https://youtu.be/35_DNMcMW-w)


```{figure} images/4-3.png
---
height: 250px
name: 4-3
---
```

- For one sample:

$$
\begin{aligned}
\mathbf{w}^{[l]} &: \big(  n^{[l]}, n^{[l-1]} \big) \\
b^{[l]} &: \big(  n^{[l]}, 1 \big) \\
\mathrm{d} \mathbf{w}^{[l]} &: \big(  n^{[l]}, n^{[l-1]} \big) \\
\mathrm{d} b^{[l]} &: \big(  n^{[l]}, 1 \big) \\
\mathbf{z}^{[l]}, \mathbf{a}^{[l]} &: \big(  n^{[l]}, 1 \big) \\
\text{if } l = 0 &: \mathbf{a}^{[0]} = \mathbf{x} = \big(  n^{[0]}, 1 \big) \\
\end{aligned}
$$

- For $m$ samples and vectorized implementation:

$$
\begin{aligned}
\mathbf{W}^{[l]} &: \big(  n^{[l]}, n^{[l-1]} \big) \\
b^{[l]} &: \big(  n^{[l]}, m \big) \\
\mathrm{d} \mathbf{W}^{[l]} &: \big(  n^{[l]}, n^{[l-1]} \big) \\
\mathrm{d} b^{[l]} &: \big(  n^{[l]}, m \big) \\
\mathbf{Z}^{[l]}, \mathbf{A}^{[l]} &: \big(  n^{[l]}, m \big) \\
\text{if } l = 0 &: \mathbf{A}^{[0]} = \mathbf{X} = \big(  n^{[0]}, m \big) \\
\mathrm{d} \mathbf{Z}^{[l]}, \mathrm{d} \mathbf{A}^{[l]} &: \big(  n^{[l]}, m \big)
\end{aligned}
$$

When you implement a deep neural network if you keep straight the dimensions of these various matrices and vectors you're working with, hopefully, that will help you eliminate some class of possible bugs.


## Why Deep Representations?

[Video](https://youtu.be/52H13FH0yQo)

-  Intuition about deep representation

-  Circuit theory and deep learning

Circuit theory, informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute.


## Building Blocks of Deep Neural Networks

[Video](https://youtu.be/grQ1OvXLamY)


In the chapter 2 and 3, you have already learned the basic building blocks of forward propagation and back propagation, the key components you need to implement a deep neural network. Let's see how you can put these components together to build your deep net.

In a deep neural network, picking one layer (layer $l$) and looking at the computations focusing on just that layer for now.

```{figure} images/4-4.png
---
height: 200px
name: 4-4
---
```

For layer $l$, you have some parameters:

- $\mathbf{w}^{[l]}$, $\mathbf{b}^{[l]}$
- Forward: Input  $\mathbf{a}^{[l-1]}$ Output $\mathbf{a}^{[l]}$
	- $\mathbf{z}^{[l]} = \mathbf{w}^{[l]}  \mathbf{a}^{[l-1]}+ \mathbf{b}^{[l]}$, cache $\mathbf{z}^{[l]}$
	- $\mathbf{a}^{[l]} = g^{[l]}(\mathbf{z}^{[l]})$
- Backward: Input  $\mathrm{d} \mathbf{a}^{[l]}$ Output $\mathrm{d}\mathbf{a}^{[l-1]}$


To summarize, in layer $l$, you're going to have these steps show below:

```{figure} images/4-5.png
---
height: 360px
name: 4-5
---
```
So if you can implement these two functions then the basic computation of the neural network will be as follows. 

```{figure} images/4-6.png
---
height: 400px
name: 4-6
---
```
So that's one iteration of gradient descent for your neural network. 

```{admonition} Reminder
:class: hint
Conceptually, it will be useful to think of the cache here in the figure as storing the value of $\mathbf{z}$ for the backward functions. But when you implement this, and you see this in the programming exercise, When you implement this, you find that the cache may be a convenient way to get to this value of the parameters of $\mathbf{w}$, $\mathbf{b}$, into the backward function as well. So for this exercise you actually store in your cache to $\mathbf{z}$ as well as $\mathbf{w}$ and $\mathbf{b}$. But from an implementation standpoint, I just find it a convenient way to just get the parameters, copy to where you need to use them later when you're computing back propagation. So that's just an implementational detail that you see when you do the programming exercise. 
```

So you've now seen what are the basic building blocks for implementing a deep neural network. In each layer there's a forward propagation step and there's a corresponding backward propagation step. And has a cache to pass information from one to the other.


## Forward and Backward Propagation

[Video](https://youtu.be/8k5XEQDrT-8)

### Forward propogation for layer $l$

- Input $\mathbf{a}^{[l-1]}$
- Output $\mathbf{a}^{[l]}$, cache $\mathbf{z}^{[l]}$

$$
\begin{aligned}
\mathbf{z}^{[l]} &= \mathbf{w}^{[l]}  \mathbf{a}^{[l-1]}+ \mathbf{b}^{[l]} \\
\mathbf{a}^{[l]} &= g^{[l]}(\mathbf{z}^{[l]})
\end{aligned}
$$

Vectorised:

$$
\begin{aligned}
\mathbf{Z}^{[l]} &= \mathbf{W}^{[l]}  \mathbf{A}^{[l-1]}+ \mathbf{b}^{[l]} \\
\mathbf{A}^{[l]} &= g^{[l]}(\mathbf{Z}^{[l]})
\end{aligned}
$$


### Backward propogation for layer $l$

- Input $\mathrm{d} \mathbf{a}^{[l]}$
- Output $\mathrm{d}\mathbf{a}^{[l-1]}$, $\mathrm{d}\mathbf{W}^{[l]}$, $\mathrm{d}\mathbf{b}^{[l]}$

$$
\begin{aligned}
\mathrm{d}\mathbf{z}^{[l]} &= \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}z} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times  \dfrac{\mathrm{d}a}{\mathrm{d}z} = \mathrm{d}\mathbf{a}^{[l]} *  g^{[l]'}(\mathbf{z}^{[l]}) \\
\mathrm{d}\mathbf{W}^{[l]} &= \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}W} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}W} = \mathrm{d}\mathbf{z}^{[l]} *  \mathbf{a}^{[l-1]} \\
\mathrm{d}\mathbf{b}^{[l]} &= \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}b} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}b} = \mathrm{d}\mathbf{z}^{[l]} \\
\mathrm{d}\mathbf{a}^{[l-1]} &= \mathbf{W}^{[l]T} \cdot \mathrm{d}\mathbf{z}^{[l]}
\end{aligned}
$$

Vectorised:

$$
\begin{aligned}
\mathrm{d}\mathbf{Z}^{[l]} &= \mathrm{d}\mathbf{A}^{[l]} *  g^{[l]'}(\mathbf{Z}^{[l]}) \\
\mathrm{d}\mathbf{W}^{[l]} &= \dfrac{1}{m} \mathrm{d}\mathbf{Z}^{[l]} \cdot \mathbf{A}^{[l-1]T} \\
\mathrm{d}\mathbf{b}^{[l]} &= \dfrac{1}{m} \text{np.sum}(\mathrm{d}\mathbf{Z}^{[l]} \text{, axis = }1 \text{, keepdims = True} )\\
\mathrm{d}\mathbf{A}^{[l-1]} &= \mathrm{d}\mathbf{W}^{[l]T} \cdot \mathrm{d}\mathbf{Z}^{[l]} 
\end{aligned}
$$

For a more in depth explaination of Feedforward Neural Networks:

[Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations](https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/)

[Feedforward Neural Networks in Depth, Part 2: Activation Functions](https://jonaslalin.com/2021/12/21/feedforward-neural-networks-part-2/)

[Feedforward Neural Networks in Depth, Part 3: Cost Functions](https://jonaslalin.com/2021/12/22/feedforward-neural-networks-part-3/)


## Parameters and Hyperparameters

[Video](https://youtu.be/RGScA6l3KiM)

Being effective in developing your deep Neural Nets requires that you not only organize your parameters well but also your hyper parameters. 

- Parameters: $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \cdots$

- Hyperparameters:
	- Learning rate $\alpha$
	- number of iterations
	- number of hidden layers
	- number of hidden units $n^{[1]}, n^{[2]}, \cdots$
	- choice of activation function

So all of these things are things that you need to tell your learning algorithm and so these are parameters that control the ultimate parameters $W$ and $b$ and so we call all of these things below hyper parameters.

We call these things hyper parameters, because it is the hyper parameters that somehow determine the final value of the parameters $W$ and $b$ that you end up with. So when you're training a deep net for your own application you find that there may be a lot of possible settings for the hyper parameters that you need to just try out. 


## Summary with formula

```{important} 

**Forward propogation**:

$$
\begin{aligned}
\mathbf{Z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{A}^{[1]} &= g^{[1]}({\mathbf{Z}^{[1]}}) \\
\mathbf{Z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{A}^{[2]} &= g^{[2]}({\mathbf{Z}^{[2]}}) \\
\vdots \\
\mathbf{Z}^{[L]} &= \mathbf{W}^{[L]} \mathbf{A}^{[L-1]} + \mathbf{b}^{[L]} \\
\mathbf{A}^{[L]} &= g^{[L]}({\mathbf{Z}^{[L]}}) = \hat{Y}
\end{aligned}
$$ 

**Backward propogation**:

$$
\begin{aligned}
\mathrm{d}\mathbf{Z}^{[l]} &= \mathrm{d}\mathbf{A}^{[l]} *  g^{[l]'}(\mathbf{Z}^{[l]}) \\
\mathrm{d}\mathbf{W}^{[l]} &= \dfrac{1}{m} \mathrm{d}\mathbf{Z}^{[l]} \cdot \mathbf{A}^{[l-1]T} \\
\mathrm{d}\mathbf{b}^{[l]} &= \dfrac{1}{m} \text{np.sum}(\mathrm{d}\mathbf{Z}^{[l]} \text{, axis = }1 \text{, keepdims = True} )\\
\mathrm{d}\mathbf{Z}^{[l-1]} &= \mathrm{d}\mathbf{W}^{[l]T} \cdot \mathrm{d}\mathbf{Z}^{[l]} * g^{[l-1]'}(\mathbf{Z}^{[l-1]}) \\
\vdots \\
\mathrm{d}\mathbf{Z}^{[1]} &= \mathrm{d}\mathbf{W}^{[2]T} \cdot \mathrm{d}\mathbf{Z}^{[2]} * g^{[1]'}(\mathbf{Z}^{[1]}) \\
\mathrm{d}\mathbf{W}^{[1]} &= \dfrac{1}{m} \mathrm{d}\mathbf{Z}^{[1]} \cdot \mathbf{A}^{[0]T} \\
\mathrm{d}\mathbf{b}^{[1]} &= \dfrac{1}{m} \text{np.sum}(\mathrm{d}\mathbf{Z}^{[1]} \text{, axis = }1 \text{, keepdims = True} )\\
\end{aligned}
$$ 


- Note that $*$ denotes element-wise multiplication.
- Note that $\mathbf{A}^{[0]T}$ ia another way to denote the input features, which is also written as $\mathbf{X}^{T}$.
```







## Quiz

1. **True/False** We use the "cache" in our implementation of forward and backward propagation to pass useful values to the next layer in the forward propagation.   __________


2.  Among the following, which ones are "hyperparameters"? (Check all that apply.)

    A. Size of the hidden layers $n^{[l]}$

    B. number of iterations

    C. Weight matrices $W^{[l]}$

    D. Bias veactors $b^{[l]}$
    
    E. Activation values $a^{[l]}$
    
    F. Number of layers $L$ in the neural network
    
    G. Learning rate $\alpha$

3. Which of the following is more likely related to the early layers of a deep neural network?

    ```{figure} images/4-q3.png
    ---
    height: 220px
    name: 4-q3
    ---
    ```

4. **True/False** Vectorization allows us to compute $a^{[l]}$ for all the examples on a batch at the same time without using a for loop.  __________

5.  Consider the following neural network:
    
    ```{figure} images/4-q5.png
    ---
    height: 160px
    name: 4-q5
    ---
    ```
    How many layers does this network have?

    A. The number of layers $L$ is 4. The number of hidden layers is 4.

    B. The number of layers $L$ is 5. The number of hidden layers is 4.

    C. The number of layers $L$ is 4. The number of hidden layers is 3.

    D. The number of layers $L$ is 3. The number of hidden layers is 3.

6.  **True/False** During forward propagation, in the forward function for a layer $l$ you need to know what is the activation function in a layer (sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer $l$, since the gradient depends on it.   __________

7.  **True/False** For any mathematical function you can compute with an L-layered deep neural network with N hidden units there is a shallow neural network that requires only $\text{log}(N)$ units, but it is very difficult to train.  __________
 
8.  Consider the following 2 hidden layer neural network:
    
    ```{figure} images/4-q8.png
    ---
    height: 260px
    name: 4-q8
    ---
    ```
    
    Which of the following statements are True? (Check all that apply).
    
    A. $b^{[3]}$ will have shape $(1,1)$

    B. $W^{[2]}$ will have shape $(3,4)$

    C. $W^{[3]}$ will have shape $(1,3)$

    D. $b^{[2]}$ will have shape $(1,1)$  
    
    E. $W^{[3]}$ will have shape $(3,1)$  
    
    F. $b^{[3]}$ will have shape $(3,1)$ 
    
    G. $b^{[1]}$ will have shape $(3,1)$
    
    H. $W^{[1]}$ will have shape $(4,4)$ 
    
    I. $b^{[1]}$ will have shape $(4,1)$  
    
    J. $b^{[2]}$ will have shape $(3,1)$ 
    
    K. $W^{[1]}$ will have shape $(3,4)$
    
    L. $W^{[2]}$ will have shape $(3,1)$ 
    
    
9. Whereas the previous question used a specific network, in the general case what is the dimension of $b^{[l]}$, the bias vector associated with layer $l$?

      A. $b^{[l]}$ has shape $(n^{[l]},1)$

      B. $b^{[l]}$ has shape $(n^{[l+1]},1)$

      C. $b^{[l]}$ has shape $(1, n^{[l-1]})$

      D. $b^{[l]}$ has shape $(1, n^{[l]})$

10. What is the "cache" used for in our implementation of forward propagation and backward propagation?

      A. It is used to keep track of the hyperparameters that we are searching over, to speed up computation.
      
      B. We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations.
      
      C. It is used to cache the intermediate values of the cost function during training.

      D. We use it to pass $\mathbf{z}$ computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.

11. **True/False** During the backpropagation process, we use gradient descent to change the hyperparameters.   __________

12. **True/False** We can not use vectorization to calculate $\mathrm{d}A^{[l]}$ in backpropagation, we must use a for loop over all the examples.  __________

13. Consider the following neural network:  What are all the values of $n^{[0]}$, $n^{[1]}$, $n^{[2]}$, $n^{[3]}$ and $n^{[4]}$.
    
    ```{figure} images/4-q13.png
    ---
    height: 260px
    name: 4-q13
    ---
    ```

14. **True/False** During forward propagation, to calculate $\mathrm{d}A^{[l]}$, you use the activation function $g^{[l]}$ with the values of $Z^{[l]}$. During backward propagation, you calculate $\mathrm{d}A^{[l]}$ from $Z^{[l]}$.  __________  


15. **True/False** A shallow neural network with a single hidden layer and 6 hidden units can compute any function that a neural network with 2 hidden layers and 6 hidden units can compute.  __________  


16. Suppose `W[i]` is the array with the weights of the i-th layer, `b[i]` is the vector of biases of the i-th layer, and `g` is the activation function used in all layers. Which of the following calculates the forward propagation for the neural network with L layers.

A. 

```python      
for i in range(1, L+1):
    Z[i] = W[i] * A[i-1] + b[i] 
    A[i] = g(Z[i])
```

B. 

```python      
for i in range(L):
    Z[i] = W[i] * X + b[i]
    A[i] = g(Z[i])
```
      
C. 

```python      
for i in range(1, L):
    Z[i] = W[i] * A[i-1] + b[i]
    A[i] = g(Z[i])
```

D. 
      
```python      
for i in range(L):
    Z[i+1] = W[i+1] * A[i+1] + b[i+1]
    A[i+1] = g(Z[i+1])
```

17. Which of the following are “parameters” of a neural network? (Check all that apply.)

      A. $W^{[l]}$ the weight matrices.
      
      B. $b^{[l]}$ the bias vector. 
            
      C. $g^{[l]}$ the activation functions.

      D. $L$ the number of layers of the neural network.

18. Which of the following statements is true?

      A. The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.
      
      B. The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.
      
19. In the general case if we are training with $m$ examples what is the shape of $A^{[l]}$?

      A. $(m, n^{[l+1]})$

      B. $(m, n^{[l]})$

      C. $(n^{[l]}, m)$

      D. $(n^{[l+1]}, m)$



:::{admonition} Click here for answers!
:class: tip, dropdown

1. False </br>

    The "cache" is used in our implementation to store values computed during forward propagation to be used in backward propagation. </br>

2. ABFG </br>

3. C </br>
    
    The early layer of a neural network usually computes simple features such as edges and lines. </br>

4. True </br>

     Vectorization allows us to compute the activation for all the training examples at the same time, avoiding the use of a for loop.  </br>

5. C </br>

    C. Yes.  As seen in lecture, the number of layers is counted as the number of hidden layers + 1. The input and output layers are not counted as hidden layers. </br>
    
6. True </br>

    Yes, as you've seen in chapter 3 each activation has a different derivative. Thus, during backpropagation you need to know which activation was used in the forward propagation to be able to compute the correct derivative. </br>
    
7. False </br>

    On the contrary, some mathematical functions can be computed using an L-layered neural network and a given number of hidden units; but using a shallow neural network the number of necessary hidden units grows exponentially. </br>

8. ABCHIG </br>

   More generally, the shape of $W^{[l]}$ is $(n^{[l]}, n^{[l-1]})$, and the the shape of $b^{[l]}$ is $(n^{[l]}, m)$</br>
    
9. A </br>

    A. Yes. $b^{[l]}$ is a column vector with the same number of rows as units in the respective layer. </br>
    
10. D </br>

      D. Correct, the "cache" records values from the forward propagation units and are used in backward propagation units because it is needed to compute the chain rule derivatives.
    
11. False </br>

    During backpropagation, we use gradient descent to compute new values of $W^{[l]}$ and $b^{[l]}$. These are the parameters of the network. </br>    

12. False </br>

     We can use vectorization in backpropagation to calculate $\mathrm{d}A^{[l]}$ for each layer. This computation is done over all the training examples. </br>  
     
13. 4, 4, 3, 2, 1 </br>       

14. False </br>

     During backward propagation we are interested in computing $\mathrm{d}W^{[l]}$ and $\mathrm{d}b^{[l]}$. For that we use  $g^{'[l]}$, $\mathrm{d}Z^{[l]}$, $Z^{[l]}$ and $W^{[l]}$. </br> 

15. False </br>

     As seen during the lectures there are functions you can compute with a "small" L-layer deep neural network that shallower networks require exponentially more hidden units to compute. </br> 

16. A </br>

      A. Remember that the range omits the last number thus the range from 1 to L calculates only the A up to the L-1 layer.

17. AB </br>

      AB. The weight matrices and the bias vectors are the parameters of the network. </br>

18. A  </br>

19. C </br>

      The number of rows in $A^{[l]}$ corresponds to the number of units in the $l$-th layer. </br>

:::

