---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

(Intro2DL)=
# 1 Introduction to Deep Learning

Analyze the major trends driving the rise of deep learning, and give examples of where and how it is applied today.

**Learning Objectives**

- Discuss the major trends driving the rise of deep learning.

- Explain how deep learning is applied to supervised learning.

- ***List the major categories of models (CNNs, RNNs, etc.), and when they should be applied.***

- Assess appropriate use cases for deep learning.

---

What is a Neural Network? Please watch this [video](https://youtu.be/wkoIzKuwM_k) to find more information.

```{figure} images/1-1.png
---
height: 300px
name: 1-1
---
```

Here, you have a neural network with four inputs. So the input features might be the size, number of bedrooms, the zip code or postal code, and the wealth of the neighbourhood. And so given these input features, the job of the neural network will be to predict the price $y$. Notice also that each of these circles, are called ***hidden units*** in the neural network, that each of them takes its inputs all four input features. So for example, rather than saying this first node represents family size and family size depends only on the features $x_1$ and $x_2$. Instead, we're going to say, well neural network, you decide whatever you want this node to be. And we'll give you all four input features to compute whatever you want. So the input layer and this layer in the middle of the neural network are ***densely connected***. Because every input feature is connected to every one of these circles in the middle. And the remarkable thing about neural networks is that, given enough data about $x$ and $y$, given enough training examples with both $x$ and $y$, neural networks are remarkably good at figuring out functions that accurately map from $x$ to $y$.

```{figure} images/1-2.png
---
height: 300px
name: 1-2
---
```

> **Practice Quiz**: True or false? As explained in this lecture, every input layer feature is interconnected with every hidden layer feature.  ____________<br/>


## Supervised Learning with Neural Networks

[Video](https://youtu.be/-dAVnnI2dUo)

### Examples

There's been a lot of hype about neural networks. And perhaps some of that hype is justified, given how well they're working. But it turns out that so far, almost all the economic value created by neural networks has been through one type of machine learning, called supervised learning. Let's see what that means, and let's go over some examples. 


| Input (x)         | Output (y)                 | Application         | Method          |
|:------------------|:---------------------------|:--------------------|:----------------|
| Home features     | Price                      | Real Estate         | Standard NN     |
| Ad, Users Info    | Click on ad? (0/1)         | Online Advertising  | Standard NN     |
| Image             | Object (1, ..., 1000)      | Photo tagging       | CNN             |
| Audio             | Text transcript            | Speech recognition  | RNN             |
| English           | Chinese                    | Machine translation | RNN             |
| Image, Radar Info | Position of the other cars | Autonomous driving  | Custom / Hybrid |

In supervised learning, you have some input $x$, and you want to learn a function mapping to some output $y$. So for example, just now we saw the housing price prediction application where you input some features of a home and try to output or estimate the price $y$. 

- Here are some other examples of how neural networks have been applied very effectively. 

    - Possibly the single most lucrative application of deep learning today is online advertising. It may not be the most inspiring, but it is certainly very lucrative, by inputting information about an ad into the website it's thinking of showing you, some information about the user, neural networks have gotten very good at predicting whether or not you click on an ad. And by showing you and showing users the ads that you are most likely to click on, this has been an incredibly lucrative application of neural networks at multiple companies. The ability to show you ads that you're more likely to click on has a direct impact on the bottom line of some of the very large online advertising companies.
    
    - Computer vision has also made huge strides in the last several years, mostly due to deep learning. So you might input an image and want to output an index, say from $1$ to $1,000$ trying to tell you if this picture, might be any one of, say 1000 different images. So, you might use that for photo tagging. 

    - I think the recent progress in speech recognition has also been very exciting, where you can now input an audio clip to a neural network, and have it output a text transcript. 

    - Machine translation has also made huge strides thanks to deep learning where now you can have a neural network input an English sentence and directly output say, a Chinese sentence. 

    - In autonomous driving, you might input an image, say a picture of what's in front of your car as well as some information from a radar, and based on that, maybe a neural network can be trained to tell you the position of the other cars on the road. So this becomes a key component in autonomous driving systems. So a lot of the value creation through neural networks has been through cleverly selecting what should be $x$ and what should be $y$ for your particular problem, and then fitting this supervised learning component into often a bigger system such as an autonomous vehicle. 

- It turns out that slightly different types of neural networks are useful for different applications. For example, in the real estate application, we use a universally standard neural network architecture. Maybe for real estate and online advertising might be a relatively standard neural network, like the one that we saw.

    - For image applications, we'll often use convolutional neural networks, often abbreviated CNN.

    - And for sequence data. So for example, audio has a temporal component, right? Audio is played out over time, so audio is most naturally represented as a one-dimensional time series or as a one-dimensional temporal sequence. And so for sequence data, you often use an RNN, a recurrent neural network. Language, English and Chinese, the alphabet or the words come one at a time. So language is also most naturally represented as sequence data. And so more complex versions of RNNs are often used for these applications. 

    - And then, for more complex applications, like autonomous driving, where you have an image, that might suggest more of a CNN, convolution neural network, structure and radar info which is something quite different. You might end up with a more custom, or some more complex, hybrid neural network architecture.
 
### Standard NN, CNN and RNN

```{figure} images/1-3.png
---
height: 200px
name: 1-3
---
```

Just to be a bit more concrete about what are the ***standard***, ***CNN*** and ***RNN*** architectures. So in the literature you might have seen pictures like this. Here shows an example of a standard Neural Network, Convolutional Neural Network and Recurrent Neural Network, and we'll see in a later course exactly what this picture means and how can you implement this. 

- ***Convolutional networks*** are often used for image data.
  
- ***Recurrent neural networks*** are very good for this type of one-dimensional sequence data that has maybe a temporal component.

### Structured and unstructured data

You might also have heard about applications of machine learning to both ***Structured Data*** and ***Unstructured Data***. Here's what the terms mean. 

- Structured Data means basically databases of data.

  - For example, in housing price prediction, you might have a database or a column that tells you the size and the number of bedrooms. So, this is structured data, or in predicting whether or not a user will click on an ad, you might have information about the user, such as the age, some information about the ad, and then labels why that you're trying to predict. So that's structured data, meaning that each of the features, such as the size of the house, the number of bedrooms, or the age of a user, has a very **well-defined** meaning.
 
- Unstructured data refers to things like audio, raw audio, or images where you might want to recognize what's in the image or text. Here the features might be the pixel values in an image or the individual words in a piece of text.

- Historically, it has been much harder for computers to make sense of unstructured data compared to structured data. And in fact the human race has evolved to be very good at understanding audio cues as well as images. And then the text was a more recent invention, but people are just good at interpreting unstructured data.

- And so one of the most exciting things about the rise of neural networks is that, thanks to deep learning, thanks to neural networks, computers are now much better at interpreting unstructured data as well compared to just a few years ago. And this creates opportunities for many new exciting applications that use speech recognition, image recognition, and natural language processing on text, much more than was possible even just two or three years ago. I think because people have a natural empathy for understanding unstructured data, you might hear about neural network successes on unstructured data more in the media because it's just cool when the neural network recognizes a cat. We all like that, and we all know what that means. 

- But it turns out that a lot of short term economic value that neural networks are creating has also been on structured data, such as much better advertising systems, much better profit recommendations, and just a much better ability to process the giant databases that many companies have to make accurate predictions from them. So in this course, a lot of the techniques we'll go over will apply to both structured data and to unstructured data. For the purposes of explaining the algorithms, we will draw a little bit more on examples that use unstructured data. But as you think through applications of neural networks within your own team I hope you find both uses for them in both structured and unstructured data. 


## Why is Deep Learning taking off?

[Video](https://youtu.be/0iephiCi4Gk)

```{figure} images/1-4.png
---
height: 360px
name: 1-4
---
```

### Importance of Scale

- If you want to hit a very **high level of performance** then you need two things:

    - 1. often you need to be able to train a big enough neural network to take advantage of the huge amount of data;
    - 2. you do need a lot of data;
    - 3. **summary: Both the size of the neural network and the scale of data are crucial for achieving high performance**.

    so we often say that ***scale*** has been driving deep learning progress. ***Scale*** means ***both the size of the neural network***, meaning just a new network, a lot of hidden units, a lot of parameters, a lot of connections, ***as well as the scale of the data***.

In fact, today one of the most reliable ways to get better performance in a neural network is often to either train a bigger network or throw more data at it and that only works up to a point because eventually you run out of data or eventually then your network is so big that it takes too long to train. 

1. Small Training Data Regime:

    - In cases with limited training data, the effectiveness of machine learning algorithms is less predictable.
    - Performance often relies more on the practitioner's skill in feature engineering.
    - Example: Support Vector Machines (SVM) may outperform larger neural networks in small data scenarios due to better feature engineering.

2. Large Training Data Regime:

    - With substantial training data, large neural networks consistently outperform other algorithms.
    - The "Very Large $m$ Regime" shows a clear dominance of large neural networks in handling big data.


### Drives of Deep Learning

- **Data Availability**: The digital era has led to an exponential increase in data.

- **Computation**
  
- **Algorithm Performance**: Traditional algorithms vs. neural networks in handling large data sets.
    - As data increases, traditional algorithms plateau in performance, while neural networks continue to improve.
 

1. Early Days of Modern Deep Learning:

    - Focus on 'Scaled Data' and 'Computational Scale': The ability to process large data sets with advanced computational power (CPUs/GPUs) was foundational.
    - Highlighting the Importance: These factors were crucial for training large neural networks efficiently.

But increasingly, especially in the last several years, we've seen tremendous algorithmic innovation. Interestingly, many of the algorithmic innovations have been about trying to make neural networks run much faster.

2. The Shift to Algorithmic Innovation:

    - Recent Years: Marked by significant improvements in the algorithms themselves.
    - The Goal: To enhance the speed and efficiency of neural network training and operation.


:::{admonition} Case Study: Activation Functions in Neural Networks. 

- **Problem with Sigmoid Function**: Slow learning due to gradients nearing zero in certain regions.
- **Solution**: Switching to the Rectified Linear Unit (ReLU) function.
- **Impact**: Faster gradient descent, leading to quicker learning and more efficient computation.

As a concrete example, one of the huge breakthroughs in neural networks has been switching from a sigmoid function. It turns out that one of the problems of using sigmoid functions and machine learning is that there are these regions here where the slope of the function where the gradient is nearly zero and so learning becomes really slow, because when you implement gradient descent and the gradient is zero the parameters just change very slowly. And so, learning is very slow whereas by changing what's called the activation function the neural network uses this function called the value function of the rectified linear unit, or RELU, the gradient is equal to $1$ for all positive values of input. And so, the gradient is much less likely to gradually shrink to $0$ and the gradient here. the slope of this line is $0$ on the left but it turns out that just switching from the sigmoid function to the RELU function has made an algorithm called gradient descent work much faster and so this is an example of maybe relatively simple algorithmic innovation. Ultimately, the impact of this algorithmic innovation was it really helped computation. So there are actually quite a lot of examples like this of where we ***change the algorithm because it allows that code to run much faster and this allows us to train bigger neural networks***, or to do so the reason will decline even when we have a large network roam all the data.

:::


The other reason that fast computation is important is that it turns out the process of training your network is very intuitive. Often, you have an idea for a neural network architecture and so you implement your idea and code. Implementing your idea then lets you run an experiment which tells you how well your neural network does and then by looking at it you go back to change the details of your new network and then you go around this circle over and over and when your new network takes a long time to train it just takes a long time to go around this cycle and there's a huge difference in your productivity. 

Building effective neural networks when you can have an idea and try it and see the work in ten minutes, or maybe at most a day, versus if you've to train your neural network for a month, which sometimes does happen, because you get a result back you know in ten minutes or maybe in a day you should just try a lot more ideas and be much more likely to discover in your network. And it works well for your application and so faster computation has really helped in terms of speeding up the rate at which you can get an experimental result back and this has really helped both practitioners of neural networks as well as researchers working and deep learning iterate much faster and improve your ideas much faster. 

      
4. Practical Implications:

    - Faster Training Cycles: Allows for more rapid testing and refinement of neural network models.
    - Increased Productivity: Shorter training times enable quicker experimentation and innovation.
      
5. The Cycle of Neural Network Development:

    - The Iterative Process: From conceptualization to implementation, experimentation, and refinement.
    - Real-World Impact: Faster computation allows for more agile development cycles.


So, all this has also been a huge boon to the entire deep learning research community which has been incredible with just inventing new algorithms and making nonstop progress on that front. So these are some of the forces powering the rise of deep learning but the good news is that these forces are still working powerfully to make deep learning even better. Take data... society is still throwing out more digital data. Or take computation, with the rise of specialized hardware like GPUs and faster networking many types of hardware, I'm actually quite confident that our ability to do very large neural networks from a computation point of view will keep on getting better and take algorithms relative to learning research communities are continuously phenomenal at elevating on the algorithms front.



## Information

About this course introduction also can see this [video](https://youtu.be/LzcySrHxa40).

The lecture notes are available on our community platform. If you're already a member, log in to your account and access the lecture notes [here](https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862).

**[Important]** The [Community](https://community.deeplearning.ai/c/course-q-a/deep-learning-specialization/6)

- Ask for help on assignments and other course content.

- Discuss course topics.

- Share your knowledge with other learners.

- Build your network 

- Find out about exciting DeepLearning.AI news, events and competitions!  


## Quiz

1. Which of the following are reasons that didn't allow Deep Learning to be developed during the '80s? Choose all that apply.

    A. People were afraid of a machine rebellion.

    B. The theoretical tools didn’t exist during the 80’s.

    C. Interesting applications such as image recognition require large amounts of data that were not available.

    D. Limited computational power.

2. When building a neural network to predict housing price from features like size, the number of bedrooms, zip code, and wealth, it is necessary to come up with other features in between input and output like family size and school quality. True/False? __________

3. Images for cat recognition is an example of “structured” data, because it is represented as a structured array in a computer. True/False? __________

4. A dataset is composed of age and weight data for several people. This dataset is an example of "structured" data because it is represented as an array in a computer. True/False? __________

5. Why can an RNN (Recurrent Neural Network) be used to create English captions to French movies? Choose all that apply.

    A. The RNN requires a small number of examples.

    B. RNNs are much more powerful than a Convolutional neural Network (CNN).

    C. The RNN is applicable since the input and output of the problem are sequences.

    D. It can be trained as a supervised learning problem.

Answer Questions 6 - 7 according the figure shows below:
   
```{figure} images/1-4.png
---
height: 360px
---
```

6. From the given diagram, we can deduce that Large NN models are always better than traditional learning algorithms. True/False? __________

7. Assuming the trends described in the figure are accurate. The performance of a NN depends only on the size of the NN. True/False? __________



:::{admonition} Click here for answers!
:class: tip, dropdown
1. CD </br>

   C. Yes. Many resources used today to train Deep Learning projects come from the fact that our society digitizes almost everything, creating a large dataset to train Deep Learning models.</br>
   
   D. Yes. Deep Learning methods need a lot of computational power, and only recently the use of GPUs has accelerated the experimentation with Deep Learning.</br>

2. False </br>

   A neural network figures out by itself the "features" in between using the samples used to train it. </br>

3. False </br>

   Yes. Images for cat recognition are examples of “unstructured” data. </br>
   
4. True </br>

   Yes, the sequences can be represented as arrays in a computer. This is an example of structured data. </br>

5. CD </br>

   C. Yes, an RNN can map from a sequence of sounds (or audio files) to a sequence of words (the caption).</br>
   
   D. Yes, the data can be used as x (movie audio) to y (caption text).</br>
   
6. False </br>

   Yes, when the amount of data is not large the performance of traditional learning algorithms is shown to be the same as NN. </br>
   
7. False </br>

:::


