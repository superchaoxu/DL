
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Practical 7: Gradient Checking &#8212; Deep Learning Specialization</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'C5_Gradient_Checking';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Practical 6: Regularization" href="C5_Regularization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Deep Learning Specialization - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Deep Learning Specialization - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Deep Learning Specialization
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="C1.html">1 Introduction to Deep Learning</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C2.html">2 Neural Networks Basics</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C2_Practical.html">Pre-Practical: Python Basics with Numpy (optional assignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="C2_Practical_Test.html">Practicel 1: Logistic Regression with a Neural Network mindset</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C3.html">3 Shallow Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C3_Practical_Test.html">Practical 2: Planar data classification with one hidden layer</a></li>


</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C4.html">4 Deep Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C4_Practical_Test_P1.html">Practical 3: Building your Deep Neural Network: Step by Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="C4_Practical_Test_P2.html">Practical 4: Deep Neural Network for Image Classification: Application</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="C5.html">5 Practical Aspects of Deep Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="C5_Initialization.html">Practical 5: Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="C5_Regularization.html">Practical 6: Regularization</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Practical 7: Gradient Checking</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/superchaoxu/DL/tree/gh-pages" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/superchaoxu/DL/tree/gh-pages/issues/new?title=Issue%20on%20page%20%2FC5_Gradient_Checking.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/C5_Gradient_Checking.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Practical 7: Gradient Checking</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packages">1 - Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-statement">2 - Problem Statement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-gradient-checking-work">3 - How does Gradient Checking work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensional-gradient-checking">4 - 1-Dimensional Gradient Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-forward-propagation">Exercise 1 - forward_propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-backward-propagation">Exercise 2 - backward_propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-gradient-check">Exercise 3 - gradient_check</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-dimensional-gradient-checking">5 - N-Dimensional Gradient Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-gradient-check-n">Exercise 4 - gradient_check_n</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="practical-7-gradient-checking">
<h1>Practical 7: Gradient Checking<a class="headerlink" href="#practical-7-gradient-checking" title="Link to this heading">#</a></h1>
<p>Welcome to the final assignment for this week! In this assignment you’ll be implementing gradient checking.</p>
<p>By the end of this notebook, you’ll be able to:</p>
<p>Implement gradient checking to verify the accuracy of your backprop implementation.</p>
<section id="packages">
<h2>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">testCases_c2w1a3</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">gc_utils_c2w1a3</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">dictionary_to_vector</span><span class="p">,</span> <span class="n">vector_to_dictionary</span><span class="p">,</span> <span class="n">gradients_to_vector</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-statement">
<h2>2 - Problem Statement<a class="headerlink" href="#problem-statement" title="Link to this heading">#</a></h2>
<p>You are part of a team working to make mobile payments available globally, and are asked to build a deep learning model to detect fraud–whenever someone makes a payment, you want to see if the payment might be fraudulent, such as if the user’s account has been taken over by a hacker.</p>
<p>You already know that backpropagation is quite challenging to implement, and sometimes has bugs. Because this is a mission-critical application, your company’s CEO wants to be really certain that your implementation of backpropagation is correct. Your CEO says, “Give me proof that your backpropagation is actually working!” To give this reassurance, you are going to use “gradient checking.”</p>
<p>Let’s do it!</p>
</section>
<section id="how-does-gradient-checking-work">
<h2>3 - How does Gradient Checking work?<a class="headerlink" href="#how-does-gradient-checking-work" title="Link to this heading">#</a></h2>
<p>Backpropagation computes the gradients <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> denotes the parameters of the model. <span class="math notranslate nohighlight">\(J\)</span> is computed using forward propagation and your loss function.</p>
<p>Because forward propagation is relatively easy to implement, you’re confident you got that right, and so you’re almost 100% sure that you’re computing the cost <span class="math notranslate nohighlight">\(J\)</span> correctly. Thus, you can use your code for computing <span class="math notranslate nohighlight">\(J\)</span> to verify the code for computing <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span>.</p>
<p>Let’s look back at the definition of a derivative (or gradient):</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}\]</div>
<p>If you’re not familiar with the “<span class="math notranslate nohighlight">\(\displaystyle \lim_{\varepsilon \to 0}\)</span>” notation, it’s just a way of saying “when <span class="math notranslate nohighlight">\(\varepsilon\)</span> is really, really small.”</p>
<p>You know the following:</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span> is what you want to make sure you’re computing correctly.</p>
<p>You can compute <span class="math notranslate nohighlight">\(J(\theta + \varepsilon)\)</span> and <span class="math notranslate nohighlight">\(J(\theta - \varepsilon)\)</span> (in the case that <span class="math notranslate nohighlight">\(\theta\)</span> is a real number), since you’re confident your implementation for <span class="math notranslate nohighlight">\(J\)</span> is correct.</p>
<p>Let’s use equation (1) and a small value for <span class="math notranslate nohighlight">\(\varepsilon\)</span> to convince your CEO that your code for computing <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span> is correct!</p>
</section>
<section id="dimensional-gradient-checking">
<h2>4 - 1-Dimensional Gradient Checking<a class="headerlink" href="#dimensional-gradient-checking" title="Link to this heading">#</a></h2>
<p>Consider a 1D linear function <span class="math notranslate nohighlight">\(J(\theta) = \theta x\)</span>. The model contains only a single real-valued parameter <span class="math notranslate nohighlight">\(\theta\)</span>, and takes <span class="math notranslate nohighlight">\(x\)</span> as input.</p>
<p>You will implement code to compute <span class="math notranslate nohighlight">\(J(.)\)</span> and its derivative <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span>. You will then use gradient checking to make sure your derivative computation for <span class="math notranslate nohighlight">\(J\)</span> is correct.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/1Dgrad_kiank.png"><img alt="_images/1Dgrad_kiank.png" src="_images/1Dgrad_kiank.png" style="width: 600px; height: 250px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">1D linear model</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The diagram above shows the key computation steps: First start with <span class="math notranslate nohighlight">\(x\)</span>, then evaluate the function <span class="math notranslate nohighlight">\(J(x)\)</span> (“forward propagation”). Then compute the derivative <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span> (“backward propagation”).</p>
<section id="exercise-1-forward-propagation">
<h3>Exercise 1 - forward_propagation<a class="headerlink" href="#exercise-1-forward-propagation" title="Link to this heading">#</a></h3>
<p>Implement <code class="docutils literal notranslate"><span class="pre">forward</span> <span class="pre">propagation</span></code>. For this simple function compute <span class="math notranslate nohighlight">\(J(.)\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: forward_propagation</span>

<span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    x -- a real-valued input</span>
<span class="sd">    theta -- our parameter, a real number as well</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    J -- the value of function J, computed using the formula J(theta) = theta * x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># (approx. 1 line)</span>
    <span class="c1"># J = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">J</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">*</span> <span class="n">x</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">J</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">J</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;J = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">J</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>J = 8
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-2-backward-propagation">
<h3>Exercise 2 - backward_propagation<a class="headerlink" href="#exercise-2-backward-propagation" title="Link to this heading">#</a></h3>
<p>Now, implement the <code class="docutils literal notranslate"><span class="pre">backward</span> <span class="pre">propagation</span></code> step (derivative computation) of Figure 1. That is, compute the derivative of <span class="math notranslate nohighlight">\(J(\theta) = \theta x\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>. To save you from doing the calculus, you should get <code class="docutils literal notranslate"><span class="pre">dtheta</span></code> <span class="math notranslate nohighlight">\( = \frac { \partial J }{ \partial \theta} = x\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: backward_propagation</span>

<span class="k">def</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the derivative of J with respect to theta (see Figure 1).</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    x -- a real-valued input</span>
<span class="sd">    theta -- our parameter, a real number as well</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    dtheta -- the gradient of the cost with respect to theta</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># (approx. 1 line)</span>
    <span class="c1"># dtheta = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dtheta</span> <span class="o">=</span> <span class="n">x</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">dtheta</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">dtheta</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dtheta = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dtheta</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dtheta = 3
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-3-gradient-check">
<h3>Exercise 3 - gradient_check<a class="headerlink" href="#exercise-3-gradient-check" title="Link to this heading">#</a></h3>
<p>To show that the <code class="docutils literal notranslate"><span class="pre">backward_propagation()</span></code> function is correctly computing the gradient <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta}\)</span>, let’s implement gradient checking.</p>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p>First compute “gradapprox” using the formula above (1) and a small value of <span class="math notranslate nohighlight">\(\varepsilon\)</span>. Here are the Steps to follow:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\theta^{+} = \theta + \varepsilon\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\theta^{-} = \theta - \varepsilon\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J^{+} = J(\theta^{+})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(J^{-} = J(\theta^{-})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{gradapprox} = \dfrac{J^{+} - J^{-}}{2  \varepsilon}\)</span></p></li>
</ol>
</li>
<li><p>Then compute the gradient using backward propagation, and store the result in a variable “grad”</p></li>
<li><p>Finally, compute the relative difference between “gradapprox” and the “grad” using the following formula:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \text{difference} = \frac {\mid\mid \text{grad} - \text{gradapprox} \mid\mid_2}{\mid\mid \text{grad} \mid\mid_2 + \mid\mid \text{gradapprox} \mid\mid_2} \tag{2}\]</div>
<p>You will need 3 Steps to compute this formula:</p>
<ul class="simple">
<li><p>1’. compute the numerator using np.linalg.norm(…)</p></li>
<li><p>2’. compute the denominator. You will need to call np.linalg.norm(…) twice.</p></li>
<li><p>3’. divide them.</p></li>
<li><p>If this difference is small (say less than <span class="math notranslate nohighlight">\(10^{-7}\)</span>), you can be quite confident that you have computed your gradient correctly. Otherwise, there may be a mistake in the gradient computation.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: gradient_check</span>

<span class="k">def</span> <span class="nf">gradient_check</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">print_msg</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the gradient checking presented in Figure 1.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    x -- a float input</span>
<span class="sd">    theta -- our parameter, a float as well</span>
<span class="sd">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    difference -- difference (2) between the approximated gradient and the backward propagation gradient. Float output</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Compute gradapprox using right side of formula (1). epsilon is small enough, you don&#39;t need to worry about the limit.</span>
    <span class="c1"># (approx. 5 lines)</span>
    <span class="c1"># theta_plus =                                 # Step 1</span>
    <span class="c1"># theta_minus =                                # Step 2</span>
    <span class="c1"># J_plus =                                    # Step 3</span>
    <span class="c1"># J_minus =                                   # Step 4</span>
    <span class="c1"># gradapprox =                                # Step 5</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">theta_plus</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">epsilon</span>
    <span class="n">theta_minus</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">-</span> <span class="n">epsilon</span>
    <span class="n">J_plus</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_plus</span><span class="p">)</span>
    <span class="n">J_minus</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_minus</span><span class="p">)</span>
    <span class="n">gradapprox</span> <span class="o">=</span> <span class="p">(</span><span class="n">J_plus</span> <span class="o">-</span> <span class="n">J_minus</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Check if gradapprox is close enough to the output of backward_propagation()</span>
    <span class="c1">#(approx. 1 line) DO NOT USE &quot;grad = gradapprox&quot;</span>
    <span class="c1"># grad =</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta_plus</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1">#(approx. 3 lines)</span>
    <span class="c1"># numerator =                                 # Step 1&#39;</span>
    <span class="c1"># denominator =                               # Step 2&#39;</span>
    <span class="c1"># difference =                                # Step 3&#39;</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span> <span class="o">-</span> <span class="n">gradapprox</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradapprox</span><span class="p">)</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="k">if</span> <span class="n">print_msg</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">difference</span> <span class="o">&gt;</span> <span class="mf">2e-7</span><span class="p">:</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[93m&quot;</span> <span class="o">+</span> <span class="s2">&quot;There is a mistake in the backward propagation! difference = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[92m&quot;</span> <span class="o">+</span> <span class="s2">&quot;Your backward propagation works perfectly fine! difference = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span>
<span class="n">difference</span> <span class="o">=</span> <span class="n">gradient_check</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">print_msg</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Your backward propagation works perfectly fine! difference = 7.814075313343006e-11
</pre></div>
</div>
</div>
</div>
<p>Congrats, the difference is smaller than the <span class="math notranslate nohighlight">\(2 \times 10^{-7}\)</span> threshold. So you can have high confidence that you’ve correctly computed the gradient in <code class="docutils literal notranslate"><span class="pre">backward_propagation()</span></code>.</p>
<p>Now, in the more general case, your cost function <span class="math notranslate nohighlight">\(J\)</span> has more than a single 1D input. When you are training a neural network, <span class="math notranslate nohighlight">\(\theta\)</span> actually consists of multiple matrices <span class="math notranslate nohighlight">\(W^{[l]}\)</span> and biases <span class="math notranslate nohighlight">\(b^{[l]}\)</span>! It is important to know how to do a gradient check with higher-dimensional inputs. Let’s do it!</p>
</section>
</section>
<section id="n-dimensional-gradient-checking">
<h2>5 - N-Dimensional Gradient Checking<a class="headerlink" href="#n-dimensional-gradient-checking" title="Link to this heading">#</a></h2>
<p>The following figure describes the forward and backward propagation of your fraud detection model.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/NDgrad_kiank.png"><img alt="_images/NDgrad_kiank.png" src="_images/NDgrad_kiank.png" style="width: 600px; height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Deep neural network. LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let’s look at your implementations for <code class="docutils literal notranslate"><span class="pre">forward_propagation</span></code> and <code class="docutils literal notranslate"><span class="pre">backward_propagation</span></code>.</p>
<p>Below is the code provided for <code class="docutils literal notranslate"><span class="pre">forward_propagation_n</span></code>. Note here that <code class="docutils literal notranslate"><span class="pre">n</span></code> in the name implies it is for n-dimensions</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_propagation_n</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the forward propagation (and computes the cost) presented in Figure 3.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- training set for m examples</span>
<span class="sd">    Y -- labels for m examples </span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (5, 4)</span>
<span class="sd">                    b1 -- bias vector of shape (5, 1)</span>
<span class="sd">                    W2 -- weight matrix of shape (3, 5)</span>
<span class="sd">                    b2 -- bias vector of shape (3, 1)</span>
<span class="sd">                    W3 -- weight matrix of shape (1, 3)</span>
<span class="sd">                    b3 -- bias vector of shape (1, 1)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    cost -- the cost function (logistic cost for m examples)</span>
<span class="sd">    cache -- a tuple with the intermediate values (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># retrieve parameters</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W3&quot;</span><span class="p">]</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b3&quot;</span><span class="p">]</span>

    <span class="c1"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>
    <span class="n">Z3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="n">A3</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z3</span><span class="p">)</span>

    <span class="c1"># Cost</span>
    <span class="n">log_probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A3</span><span class="p">),</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">A3</span><span class="p">),</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">Z3</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">cost</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</div>
</div>
</div>
<p>Now, let’s look at the code for backward propagation.</p>
<p>Below is the code provided for <code class="docutils literal notranslate"><span class="pre">backward_propagation_n</span></code>. Note here that <code class="docutils literal notranslate"><span class="pre">n</span></code> in the name implies it is for n-dimensions</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">backward_propagation_n</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward propagation presented in figure 2.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input datapoint, of shape (input size, 1)</span>
<span class="sd">    Y -- true &quot;label&quot;</span>
<span class="sd">    cache -- cache output from forward_propagation_n()</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">Z3</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="n">dZ3</span> <span class="o">=</span> <span class="n">A3</span> <span class="o">-</span> <span class="n">Y</span>
    <span class="n">dW3</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ3</span><span class="p">,</span> <span class="n">A2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db3</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">dA2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ3</span><span class="p">)</span>
    <span class="n">dZ2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">dA1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span>
    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A1</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="mf">4.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dZ3&quot;</span><span class="p">:</span> <span class="n">dZ3</span><span class="p">,</span> <span class="s2">&quot;dW3&quot;</span><span class="p">:</span> <span class="n">dW3</span><span class="p">,</span> <span class="s2">&quot;db3&quot;</span><span class="p">:</span> <span class="n">db3</span><span class="p">,</span>
                 <span class="s2">&quot;dA2&quot;</span><span class="p">:</span> <span class="n">dA2</span><span class="p">,</span> <span class="s2">&quot;dZ2&quot;</span><span class="p">:</span> <span class="n">dZ2</span><span class="p">,</span> <span class="s2">&quot;dW2&quot;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span> <span class="s2">&quot;db2&quot;</span><span class="p">:</span> <span class="n">db2</span><span class="p">,</span>
                 <span class="s2">&quot;dA1&quot;</span><span class="p">:</span> <span class="n">dA1</span><span class="p">,</span> <span class="s2">&quot;dZ1&quot;</span><span class="p">:</span> <span class="n">dZ1</span><span class="p">,</span> <span class="s2">&quot;dW1&quot;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span> <span class="s2">&quot;db1&quot;</span><span class="p">:</span> <span class="n">db1</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">gradients</span>
</pre></div>
</div>
</div>
</div>
<p>If you had just implemented these functions, you might not have high confidence whether they work correctly. So let’s implement gradient checking to help verify the performance.</p>
<p><strong>How does gradient checking work?</strong>.</p>
<p>As in Section 3 and 4, you want to compare “gradapprox” to the gradient computed by backpropagation. The formula is still:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial J}{\partial \theta} = \lim_{\varepsilon \to 0} \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon} \tag{1}\]</div>
<p>However, <span class="math notranslate nohighlight">\(\theta\)</span> is not a scalar anymore. It is a dictionary called “parameters”. The  function “<code class="docutils literal notranslate"><span class="pre">dictionary_to_vector()</span></code>” has been implemented for you. It converts the “parameters” dictionary into a vector called “values”, obtained by reshaping all parameters (W1, b1, W2, b2, W3, b3) into vectors and concatenating them.</p>
<p>The inverse function is “<code class="docutils literal notranslate"><span class="pre">vector_to_dictionary</span></code>” which outputs back the “parameters” dictionary.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/dictionary_to_vector.png"><img alt="_images/dictionary_to_vector.png" src="_images/dictionary_to_vector.png" style="width: 600px; height: 360px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">dictionary_to_vector() and vector_to_dictionary(). You will need these functions in gradient_check_n()</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The “gradients” dictionary has also been converted into a vector “grad” using gradients_to_vector(), so you don’t need to worry about that.</p>
<p>Now, for every single parameter in your vector, you will apply the same procedure as for the gradient_check exercise. You will store each gradient approximation in a vector <code class="docutils literal notranslate"><span class="pre">gradapprox</span></code>. If the check goes as expected, each value in this approximation must match the real gradient values stored in the <code class="docutils literal notranslate"><span class="pre">grad</span></code> vector.</p>
<p>Note that <code class="docutils literal notranslate"><span class="pre">grad</span></code> is calculated using the function <code class="docutils literal notranslate"><span class="pre">gradients_to_vector</span></code>, which uses the gradients outputs of the <code class="docutils literal notranslate"><span class="pre">backward_propagation_n</span></code> function.</p>
<section id="exercise-4-gradient-check-n">
<h3>Exercise 4 - gradient_check_n<a class="headerlink" href="#exercise-4-gradient-check-n" title="Link to this heading">#</a></h3>
<p>Implement the function below.</p>
<p><strong>Instructions</strong>: Here is pseudo-code that will help you implement the gradient check.</p>
<p>For each i in num_parameters:</p>
<ul class="simple">
<li><p>To compute <code class="docutils literal notranslate"><span class="pre">J_plus[i]</span></code>:</p>
<ol class="arabic simple">
<li><p>Set <span class="math notranslate nohighlight">\(\theta^{+}\)</span> to <code class="docutils literal notranslate"><span class="pre">np.copy(parameters_values)</span></code></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\theta^{+}_i\)</span> to <span class="math notranslate nohighlight">\(\theta^{+}_i + \varepsilon\)</span></p></li>
<li><p>Calculate <span class="math notranslate nohighlight">\(J^{+}_i\)</span> using to <code class="docutils literal notranslate"><span class="pre">forward_propagation_n(x,</span> <span class="pre">y,</span> <span class="pre">vector_to_dictionary(</span></code><span class="math notranslate nohighlight">\(\theta^{+}\)</span> <code class="docutils literal notranslate"><span class="pre">))</span></code>.</p></li>
</ol>
</li>
<li><p>To compute <code class="docutils literal notranslate"><span class="pre">J_minus[i]</span></code>: do the same thing with <span class="math notranslate nohighlight">\(\theta^{-}\)</span></p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\text{gradapprox}[i] = \dfrac{J^{+}_i - J^{-}_i}{2 \varepsilon}\)</span></p></li>
</ul>
<p>Thus, you get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to <code class="docutils literal notranslate"><span class="pre">parameter_values[i]</span></code>. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1’, 2’, 3’), compute:</p>
<div class="math notranslate nohighlight">
\[ \text{difference} = \frac {\| \text{grad} - \text{gradapprox} \|_2}{\| \text{grad} \|_2 + \| \text{gradapprox} \|_2 } \tag{3}\]</div>
<p><strong>Note</strong>: Use <code class="docutils literal notranslate"><span class="pre">np.linalg.norm</span></code> to get the norms</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: gradient_check_n</span>

<span class="k">def</span> <span class="nf">gradient_check_n</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">print_msg</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;</span>
<span class="sd">    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters </span>
<span class="sd">    X -- input datapoint, of shape (input size, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot;</span>
<span class="sd">    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    difference -- difference (2) between the approximated gradient and the backward propagation gradient</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Set-up variables</span>
    <span class="n">parameters_values</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">dictionary_to_vector</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
    
    <span class="n">grad</span> <span class="o">=</span> <span class="n">gradients_to_vector</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
    <span class="n">num_parameters</span> <span class="o">=</span> <span class="n">parameters_values</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">J_plus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_parameters</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">J_minus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_parameters</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">gradapprox</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_parameters</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># Compute gradapprox</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_parameters</span><span class="p">):</span>
        
        <span class="c1"># Compute J_plus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_plus[i]&quot;.</span>
        <span class="c1"># &quot;_&quot; is used because the function you have outputs two parameters but we only care about the first one</span>
        <span class="c1">#(approx. 3 lines)</span>
        <span class="c1"># theta_plus =                                        # Step 1</span>
        <span class="c1"># theta_plus[i] =                                     # Step 2</span>
        <span class="c1"># J_plus[i], _ =                                     # Step 3</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">theta_plus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">parameters_values</span><span class="p">)</span>
        <span class="n">theta_plus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">epsilon</span>
        <span class="n">J_plus</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward_propagation_n</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">vector_to_dictionary</span><span class="p">(</span><span class="n">theta_plus</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Compute J_minus[i]. Inputs: &quot;parameters_values, epsilon&quot;. Output = &quot;J_minus[i]&quot;.</span>
        <span class="c1">#(approx. 3 lines)</span>
        <span class="c1"># theta_minus =                                    # Step 1</span>
        <span class="c1"># theta_minus[i] =                                 # Step 2        </span>
        <span class="c1"># J_minus[i], _ =                                 # Step 3</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">theta_minus</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">parameters_values</span><span class="p">)</span>
        <span class="n">theta_minus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-=</span> <span class="n">epsilon</span>
        <span class="n">J_minus</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">forward_propagation_n</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">vector_to_dictionary</span><span class="p">(</span><span class="n">theta_minus</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Compute gradapprox[i]</span>
        <span class="c1"># (approx. 1 line)</span>
        <span class="c1"># gradapprox[i] = </span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">gradapprox</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">J_plus</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">J_minus</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">epsilon</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Compare gradapprox to backward propagation gradients by computing difference.</span>
    <span class="c1"># (approx. 3 line)</span>
    <span class="c1"># numerator =                                             # Step 1&#39;</span>
    <span class="c1"># denominator =                                           # Step 2&#39;</span>
    <span class="c1"># difference =                                            # Step 3&#39;</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span> <span class="o">-</span> <span class="n">gradapprox</span><span class="p">)</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">gradapprox</span><span class="p">)</span>
    <span class="n">difference</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="k">if</span> <span class="n">print_msg</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">difference</span> <span class="o">&gt;</span> <span class="mf">2e-7</span><span class="p">:</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[93m&quot;</span> <span class="o">+</span> <span class="s2">&quot;There is a mistake in the backward propagation! difference = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[92m&quot;</span> <span class="o">+</span> <span class="s2">&quot;Your backward propagation works perfectly fine! difference = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[0m&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">difference</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">gradient_check_n_test_case</span><span class="p">()</span>

<span class="n">cost</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation_n</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
<span class="n">gradients</span> <span class="o">=</span> <span class="n">backward_propagation_n</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
<span class="n">difference</span> <span class="o">=</span> <span class="n">gradient_check_n</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">gradients</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">expected_values</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2850931567761623</span><span class="p">,</span> <span class="mf">1.1890913024229996e-07</span><span class="p">]</span>
<span class="k">assert</span> <span class="ow">not</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">difference</span><span class="p">)</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">),</span> <span class="s2">&quot;You are not using np.linalg.norm for numerator or denominator&quot;</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">difference</span><span class="p">,</span> <span class="n">expected_values</span><span class="p">)),</span> <span class="s2">&quot;Wrong value. It is not one of the expected values&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>There is a mistake in the backward propagation! difference = 0.28509315677616237
</pre></div>
</div>
</div>
</div>
<p>It seems that there were errors in the <code class="docutils literal notranslate"><span class="pre">backward_propagation_n</span></code> code! Good thing you’ve implemented the gradient check. Go back to <code class="docutils literal notranslate"><span class="pre">backward_propagation_n</span></code> and try to find/correct the errors <em>(Hint: check dW2 and db1)</em>. Rerun the gradient check when you think you’ve fixed it. Remember, you’ll need to re-execute the cell defining <code class="docutils literal notranslate"><span class="pre">backward_propagation_n()</span></code> if you modify the code.</p>
<p>Can you get gradient check to declare your derivative computation correct? Even though this part of the assignment isn’t graded, you should try to find the bug and re-run gradient check until you’re convinced backprop is now correctly implemented.</p>
<p><strong>Notes</strong></p>
<ul class="simple">
<li><p>Gradient Checking is slow! Approximating the gradient with <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial \theta} \approx  \frac{J(\theta + \varepsilon) - J(\theta - \varepsilon)}{2 \varepsilon}\)</span> is computationally costly. For this reason, we don’t run gradient checking at every iteration during training. Just a few times to check if the gradient is correct.</p></li>
<li><p>Gradient Checking, at least as we’ve presented it, doesn’t work with dropout. You would usually run the gradient check algorithm without dropout to make sure your backprop is correct, then add dropout.</p></li>
</ul>
<p>Congrats! Now you can be confident that your deep learning model for fraud detection is working correctly! You can even use this to convince your CEO. :)</p>
<div class="warning admonition">
<p class="admonition-title">In summary</p>
<p><strong>What you should remember from this notebook</strong>:</p>
<ul class="simple">
<li><p>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).</p></li>
<li><p>Gradient checking is slow, so you don’t want to run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.</p></li>
</ul>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="C5_Regularization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Practical 6: Regularization</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packages">1 - Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-statement">2 - Problem Statement</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-gradient-checking-work">3 - How does Gradient Checking work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensional-gradient-checking">4 - 1-Dimensional Gradient Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-forward-propagation">Exercise 1 - forward_propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-backward-propagation">Exercise 2 - backward_propagation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-gradient-check">Exercise 3 - gradient_check</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#n-dimensional-gradient-checking">5 - N-Dimensional Gradient Checking</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-4-gradient-check-n">Exercise 4 - gradient_check_n</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>