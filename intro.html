
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deep Learning Specialization</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=1a96265c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'intro';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="None - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="None - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Welcome to Deep Learning Specialization
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">





</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Welcome to Deep Learning Specialization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C1">1 Introduction to Deep Learning</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C2">2 Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C2_Practical">Pre-Practical: Python Basics with Numpy (optional assignment)</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C2_Practical_Test">Practicel 1: Logistic Regression with a Neural Network mindset</a></li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C3">3 Shallow Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C3_Practical_Test">Practical 2: Planar data classification with one hidden layer</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#packages">1 - Packages</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#load-the-dataset">2 - Load the Dataset</a></li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C4">4 Deep Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C4_Practical_Test_P1">Practical 3: Building your Deep Neural Network: Step by Step</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C4_Practical_Test_P2">Practical 4: Deep Neural Network for Image Classification: Application</a></li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5">5 Practical Aspects of Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5_Initialization">Practical 5: Initialization</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5_Regularization">Practical 6: Regularization</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5_Gradient_Checking">Practical 7: Gradient Checking</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="welcome-to-deep-learning-specialization">
<h1>Welcome to Deep Learning Specialization<a class="headerlink" href="#welcome-to-deep-learning-specialization" title="Link to this heading">#</a></h1>
<p>As you probably know, deep learning has already transformed traditional internet businesses like web search and advertising. But deep learning is also enabling brand new products and businesses and ways of helping people to be created. Everything ranging from better healthcare, where deep learning is getting really good at reading X-ray images to delivering personalized education, to precision agriculture, to even self-driving cars and many others. If you want to learn the tools of deep learning and be able to apply them to build these amazing things, I want to help you get there.</p>
<p>Over the next decade, I think all of us have an opportunity to build an amazing world, an amazing society, that is AI-powered, and I hope that you will play a big role in the creation of this AI-powered society.</p>
<p>I think that <strong>AI is the new electricity</strong>. Starting about 100 years ago, the <strong>electrification of our society transformed every major industry, everything ranging from transportation, and manufacturing, to healthcare, to communications and many more</strong>. And I think that today, we see a surprisingly clear path for <em><strong>AI to bring about an equally big transformation</strong></em>. And of course, the part of AI that is rising rapidly and driving a lot of these developments is deep learning. So today, deep learning is one of the most highly sought-after skills in the technology world.</p>
<p>And through this course, and a few courses after this one, I want to help you to gain and master those skills. So here is what you will learn in this sequence of courses also called a specialization.</p>
<p><strong>What you will learn</strong></p>
<ul class="simple">
<li><p>Neural Networks and Deep Learning.</p>
<ul>
<li><p>In this first course, from <a class="reference internal" href="intro.html#intro2dl"><span class="std std-ref">Chapter 1 Introduction to Deep Learning</span></a> to <a class="reference internal" href="intro.html#dnn"><span class="std std-ref">Chapter 4 Deep Neural Networks</span></a>, you will learn how to build a neural network, including a deep neural network, and how to train it on data. And at the end of this course, you will be able to build a deep neural network to recognize, guess what? Cats. For some reason, there is a cat meme running around in deep learning. And so, following tradition in this first course, we will build a cat recognizer.</p></li>
</ul>
</li>
<li><p>Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization.</p>
<ul>
<li><p>In the second course, you will learn about the practical aspects of deep learning. So you will learn, now that you have built a neural network, how actually to get it to perform well. So you learn about hyperparameter tuning, regularization, how to diagnose bias and variants, and advanced optimization algorithms, like momentum, armrest, prop, and the ad authorization algorithm. Sometimes it seems like there is a lot of tuning, even some black magic in how you build a new network.</p></li>
</ul>
</li>
<li><p>Structuring your Machine Learning project.</p>
<ul>
<li><p>In the third course, you will learn how to structure your machine learning project. It turns out that the strategy for building a machine learning system has changed in the era of deep learning. So for example, the way you split your data into train, development or dev, also called holdout cross-validation sets, and test sets, has changed in the era of deep learning. So what are the new best practices for doing that? Whether your training set and your test come from different distributions, that is happening a lot more in the era of deep learning. So, how do you deal with that?</p></li>
<li><p>And if you have heard of end-to-end deep learning, you will also learn more about that in this third course, and see when you should use it and maybe when you shouldn’t. The material in this third course is relatively unique. I am going to share with you a lot of the hard-won lessons that I have learned, building and shipping quite a lot of deep-learning products. As far as I know, this is largely material that is not taught in most universities that have deep learning courses. But I think it will really help you to get your deep learning systems to work well.</p></li>
</ul>
</li>
<li><p>Convolutional Neural Networks.</p>
<ul>
<li><p>Convolutional networks, or convolutional neural networks, often abbreviated CNNs, are often applied to images. So you will learn how to build these models in course four.</p></li>
</ul>
</li>
<li><p>Natural Language Processing: Building sequence models.</p>
<ul>
<li><p>In course five, you will learn sequence models and how to apply them to natural language processing and other problems. So sequence models include models like recurrent neural networks, abbreviated RNNs, and LSTM models, which stand for a long short-term memory model. You will learn what these terms mean in course five and be able to apply them to natural language processing problems.</p></li>
<li><p>So you will learn these models in course five and be able to apply them to sequence data. So for example, natural language is just a sequence of words, and you will also understand how these models can be applied to speech recognition, to music generation, and other problems.</p></li>
</ul>
</li>
</ul>
<p>So through these courses, you will learn the tools of deep learning, you will be able to apply them to build amazing things, and I hope many of you through this will also be able to advance your career. So with that, let’s get started.</p>
<div class="toctree-wrapper compound">
<span id="document-C1"></span><section class="tex2jax_ignore mathjax_ignore" id="introduction-to-deep-learning">
<span id="intro2dl"></span><h2>1 Introduction to Deep Learning<a class="headerlink" href="#introduction-to-deep-learning" title="Link to this heading">#</a></h2>
<p>Analyze the major trends driving the rise of deep learning, and give examples of where and how it is applied today.</p>
<p><strong>Learning Objectives</strong></p>
<ul class="simple">
<li><p>Discuss the major trends driving the rise of deep learning.</p></li>
<li><p>Explain how deep learning is applied to supervised learning.</p></li>
<li><p><em><strong>List the major categories of models (CNNs, RNNs, etc.), and when they should be applied.</strong></em></p></li>
<li><p>Assess appropriate use cases for deep learning.</p></li>
</ul>
<hr class="docutils" />
<p>What is a Neural Network? Please watch this <a class="reference external" href="https://youtu.be/wkoIzKuwM_k">video</a> to find more information.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/1-1.png"><img alt="_images/1-1.png" src="_images/1-1.png" style="height: 300px;" /></a>
</figure>
<p>Here, you have a neural network with four inputs. So the input features might be the size, number of bedrooms, the zip code or postal code, and the wealth of the neighbourhood. And so given these input features, the job of the neural network will be to predict the price <span class="math notranslate nohighlight">\(y\)</span>. Notice also that each of these circles, are called <em><strong>hidden units</strong></em> in the neural network, that each of them takes its inputs all four input features. So for example, rather than saying this first node represents family size and family size depends only on the features <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span>. Instead, we’re going to say, well neural network, you decide whatever you want this node to be. And we’ll give you all four input features to compute whatever you want. So the input layer and this layer in the middle of the neural network are <em><strong>densely connected</strong></em>. Because every input feature is connected to every one of these circles in the middle. And the remarkable thing about neural networks is that, given enough data about <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, given enough training examples with both <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, neural networks are remarkably good at figuring out functions that accurately map from <span class="math notranslate nohighlight">\(x\)</span> to <span class="math notranslate nohighlight">\(y\)</span>.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/1-2.png"><img alt="_images/1-2.png" src="_images/1-2.png" style="height: 300px;" /></a>
</figure>
<blockquote>
<div><p><strong>Practice Quiz</strong>: True or false? As explained in this lecture, every input layer feature is interconnected with every hidden layer feature.  ____________<br/></p>
</div></blockquote>
<section id="supervised-learning-with-neural-networks">
<h3>Supervised Learning with Neural Networks<a class="headerlink" href="#supervised-learning-with-neural-networks" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/-dAVnnI2dUo">Video</a></p>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h4>
<p>There’s been a lot of hype about neural networks. And perhaps some of that hype is justified, given how well they’re working. But it turns out that so far, almost all the economic value created by neural networks has been through one type of machine learning, called supervised learning. Let’s see what that means, and let’s go over some examples.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Input (x)</p></th>
<th class="head text-left"><p>Output (y)</p></th>
<th class="head text-left"><p>Application</p></th>
<th class="head text-left"><p>Method</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>Home features</p></td>
<td class="text-left"><p>Price</p></td>
<td class="text-left"><p>Real Estate</p></td>
<td class="text-left"><p>Standard NN</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Ad, Users Info</p></td>
<td class="text-left"><p>Click on ad? (0/1)</p></td>
<td class="text-left"><p>Online Advertising</p></td>
<td class="text-left"><p>Standard NN</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Image</p></td>
<td class="text-left"><p>Object (1, …, 1000)</p></td>
<td class="text-left"><p>Photo tagging</p></td>
<td class="text-left"><p>CNN</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Audio</p></td>
<td class="text-left"><p>Text transcript</p></td>
<td class="text-left"><p>Speech recognition</p></td>
<td class="text-left"><p>RNN</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>English</p></td>
<td class="text-left"><p>Chinese</p></td>
<td class="text-left"><p>Machine translation</p></td>
<td class="text-left"><p>RNN</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Image, Radar Info</p></td>
<td class="text-left"><p>Position of the other cars</p></td>
<td class="text-left"><p>Autonomous driving</p></td>
<td class="text-left"><p>Custom / Hybrid</p></td>
</tr>
</tbody>
</table>
<p>In supervised learning, you have some input <span class="math notranslate nohighlight">\(x\)</span>, and you want to learn a function mapping to some output <span class="math notranslate nohighlight">\(y\)</span>. So for example, just now we saw the housing price prediction application where you input some features of a home and try to output or estimate the price <span class="math notranslate nohighlight">\(y\)</span>.</p>
<ul class="simple">
<li><p>Here are some other examples of how neural networks have been applied very effectively.</p>
<ul>
<li><p>Possibly the single most lucrative application of deep learning today is online advertising. It may not be the most inspiring, but it is certainly very lucrative, by inputting information about an ad into the website it’s thinking of showing you, some information about the user, neural networks have gotten very good at predicting whether or not you click on an ad. And by showing you and showing users the ads that you are most likely to click on, this has been an incredibly lucrative application of neural networks at multiple companies. The ability to show you ads that you’re more likely to click on has a direct impact on the bottom line of some of the very large online advertising companies.</p></li>
<li><p>Computer vision has also made huge strides in the last several years, mostly due to deep learning. So you might input an image and want to output an index, say from <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(1,000\)</span> trying to tell you if this picture, might be any one of, say 1000 different images. So, you might use that for photo tagging.</p></li>
<li><p>I think the recent progress in speech recognition has also been very exciting, where you can now input an audio clip to a neural network, and have it output a text transcript.</p></li>
<li><p>Machine translation has also made huge strides thanks to deep learning where now you can have a neural network input an English sentence and directly output say, a Chinese sentence.</p></li>
<li><p>In autonomous driving, you might input an image, say a picture of what’s in front of your car as well as some information from a radar, and based on that, maybe a neural network can be trained to tell you the position of the other cars on the road. So this becomes a key component in autonomous driving systems. So a lot of the value creation through neural networks has been through cleverly selecting what should be <span class="math notranslate nohighlight">\(x\)</span> and what should be <span class="math notranslate nohighlight">\(y\)</span> for your particular problem, and then fitting this supervised learning component into often a bigger system such as an autonomous vehicle.</p></li>
</ul>
</li>
<li><p>It turns out that slightly different types of neural networks are useful for different applications. For example, in the real estate application, we use a universally standard neural network architecture. Maybe for real estate and online advertising might be a relatively standard neural network, like the one that we saw.</p>
<ul>
<li><p>For image applications, we’ll often use convolutional neural networks, often abbreviated CNN.</p></li>
<li><p>And for sequence data. So for example, audio has a temporal component, right? Audio is played out over time, so audio is most naturally represented as a one-dimensional time series or as a one-dimensional temporal sequence. And so for sequence data, you often use an RNN, a recurrent neural network. Language, English and Chinese, the alphabet or the words come one at a time. So language is also most naturally represented as sequence data. And so more complex versions of RNNs are often used for these applications.</p></li>
<li><p>And then, for more complex applications, like autonomous driving, where you have an image, that might suggest more of a CNN, convolution neural network, structure and radar info which is something quite different. You might end up with a more custom, or some more complex, hybrid neural network architecture.</p></li>
</ul>
</li>
</ul>
</section>
<section id="standard-nn-cnn-and-rnn">
<h4>Standard NN, CNN and RNN<a class="headerlink" href="#standard-nn-cnn-and-rnn" title="Link to this heading">#</a></h4>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/1-3.png"><img alt="_images/1-3.png" src="_images/1-3.png" style="height: 200px;" /></a>
</figure>
<p>Just to be a bit more concrete about what are the <em><strong>standard</strong></em>, <em><strong>CNN</strong></em> and <em><strong>RNN</strong></em> architectures. So in the literature you might have seen pictures like this. Here shows an example of a standard Neural Network, Convolutional Neural Network and Recurrent Neural Network, and we’ll see in a later course exactly what this picture means and how can you implement this.</p>
<ul class="simple">
<li><p><em><strong>Convolutional networks</strong></em> are often used for image data.</p></li>
<li><p><em><strong>Recurrent neural networks</strong></em> are very good for this type of one-dimensional sequence data that has maybe a temporal component.</p></li>
</ul>
</section>
<section id="structured-and-unstructured-data">
<h4>Structured and unstructured data<a class="headerlink" href="#structured-and-unstructured-data" title="Link to this heading">#</a></h4>
<p>You might also have heard about applications of machine learning to both <em><strong>Structured Data</strong></em> and <em><strong>Unstructured Data</strong></em>. Here’s what the terms mean.</p>
<ul class="simple">
<li><p>Structured Data means basically databases of data.</p>
<ul>
<li><p>For example, in housing price prediction, you might have a database or a column that tells you the size and the number of bedrooms. So, this is structured data, or in predicting whether or not a user will click on an ad, you might have information about the user, such as the age, some information about the ad, and then labels why that you’re trying to predict. So that’s structured data, meaning that each of the features, such as the size of the house, the number of bedrooms, or the age of a user, has a very <strong>well-defined</strong> meaning.</p></li>
</ul>
</li>
<li><p>Unstructured data refers to things like audio, raw audio, or images where you might want to recognize what’s in the image or text. Here the features might be the pixel values in an image or the individual words in a piece of text.</p></li>
<li><p>Historically, it has been much harder for computers to make sense of unstructured data compared to structured data. And in fact the human race has evolved to be very good at understanding audio cues as well as images. And then the text was a more recent invention, but people are just good at interpreting unstructured data.</p></li>
<li><p>And so one of the most exciting things about the rise of neural networks is that, thanks to deep learning, thanks to neural networks, computers are now much better at interpreting unstructured data as well compared to just a few years ago. And this creates opportunities for many new exciting applications that use speech recognition, image recognition, and natural language processing on text, much more than was possible even just two or three years ago. I think because people have a natural empathy for understanding unstructured data, you might hear about neural network successes on unstructured data more in the media because it’s just cool when the neural network recognizes a cat. We all like that, and we all know what that means.</p></li>
<li><p>But it turns out that a lot of short term economic value that neural networks are creating has also been on structured data, such as much better advertising systems, much better profit recommendations, and just a much better ability to process the giant databases that many companies have to make accurate predictions from them. So in this course, a lot of the techniques we’ll go over will apply to both structured data and to unstructured data. For the purposes of explaining the algorithms, we will draw a little bit more on examples that use unstructured data. But as you think through applications of neural networks within your own team I hope you find both uses for them in both structured and unstructured data.</p></li>
</ul>
</section>
</section>
<section id="why-is-deep-learning-taking-off">
<h3>Why is Deep Learning taking off?<a class="headerlink" href="#why-is-deep-learning-taking-off" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/0iephiCi4Gk">Video</a></p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/1-4.png"><img alt="_images/1-4.png" src="_images/1-4.png" style="height: 360px;" /></a>
</figure>
<section id="importance-of-scale">
<h4>Importance of Scale<a class="headerlink" href="#importance-of-scale" title="Link to this heading">#</a></h4>
<ul>
<li><p>If you want to hit a very <strong>high level of performance</strong> then you need two things:</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p>often you need to be able to train a big enough neural network to take advantage of the huge amount of data;</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>you do need a lot of data;</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p><strong>summary: Both the size of the neural network and the scale of data are crucial for achieving high performance</strong>.</p></li>
</ol>
</li>
</ul>
<p>so we often say that <em><strong>scale</strong></em> has been driving deep learning progress. <em><strong>Scale</strong></em> means <em><strong>both the size of the neural network</strong></em>, meaning just a new network, a lot of hidden units, a lot of parameters, a lot of connections, <em><strong>as well as the scale of the data</strong></em>.</p>
</li>
</ul>
<p>In fact, today one of the most reliable ways to get better performance in a neural network is often to either train a bigger network or throw more data at it and that only works up to a point because eventually you run out of data or eventually then your network is so big that it takes too long to train.</p>
<ol class="arabic simple">
<li><p>Small Training Data Regime:</p>
<ul class="simple">
<li><p>In cases with limited training data, the effectiveness of machine learning algorithms is less predictable.</p></li>
<li><p>Performance often relies more on the practitioner’s skill in feature engineering.</p></li>
<li><p>Example: Support Vector Machines (SVM) may outperform larger neural networks in small data scenarios due to better feature engineering.</p></li>
</ul>
</li>
<li><p>Large Training Data Regime:</p>
<ul class="simple">
<li><p>With substantial training data, large neural networks consistently outperform other algorithms.</p></li>
<li><p>The “Very Large <span class="math notranslate nohighlight">\(m\)</span> Regime” shows a clear dominance of large neural networks in handling big data.</p></li>
</ul>
</li>
</ol>
</section>
<section id="drives-of-deep-learning">
<h4>Drives of Deep Learning<a class="headerlink" href="#drives-of-deep-learning" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Data Availability</strong>: The digital era has led to an exponential increase in data.</p></li>
<li><p><strong>Computation</strong></p></li>
<li><p><strong>Algorithm Performance</strong>: Traditional algorithms vs. neural networks in handling large data sets.</p>
<ul>
<li><p>As data increases, traditional algorithms plateau in performance, while neural networks continue to improve.</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple">
<li><p>Early Days of Modern Deep Learning:</p>
<ul class="simple">
<li><p>Focus on ‘Scaled Data’ and ‘Computational Scale’: The ability to process large data sets with advanced computational power (CPUs/GPUs) was foundational.</p></li>
<li><p>Highlighting the Importance: These factors were crucial for training large neural networks efficiently.</p></li>
</ul>
</li>
</ol>
<p>But increasingly, especially in the last several years, we’ve seen tremendous algorithmic innovation. Interestingly, many of the algorithmic innovations have been about trying to make neural networks run much faster.</p>
<ol class="arabic simple" start="2">
<li><p>The Shift to Algorithmic Innovation:</p>
<ul class="simple">
<li><p>Recent Years: Marked by significant improvements in the algorithms themselves.</p></li>
<li><p>The Goal: To enhance the speed and efficiency of neural network training and operation.</p></li>
</ul>
</li>
</ol>
<div class="admonition-case-study-activation-functions-in-neural-networks admonition">
<p class="admonition-title">Case Study: Activation Functions in Neural Networks.</p>
<ul class="simple">
<li><p><strong>Problem with Sigmoid Function</strong>: Slow learning due to gradients nearing zero in certain regions.</p></li>
<li><p><strong>Solution</strong>: Switching to the Rectified Linear Unit (ReLU) function.</p></li>
<li><p><strong>Impact</strong>: Faster gradient descent, leading to quicker learning and more efficient computation.</p></li>
</ul>
<p>As a concrete example, one of the huge breakthroughs in neural networks has been switching from a sigmoid function. It turns out that one of the problems of using sigmoid functions and machine learning is that there are these regions here where the slope of the function where the gradient is nearly zero and so learning becomes really slow, because when you implement gradient descent and the gradient is zero the parameters just change very slowly. And so, learning is very slow whereas by changing what’s called the activation function the neural network uses this function called the value function of the rectified linear unit, or RELU, the gradient is equal to <span class="math notranslate nohighlight">\(1\)</span> for all positive values of input. And so, the gradient is much less likely to gradually shrink to <span class="math notranslate nohighlight">\(0\)</span> and the gradient here. the slope of this line is <span class="math notranslate nohighlight">\(0\)</span> on the left but it turns out that just switching from the sigmoid function to the RELU function has made an algorithm called gradient descent work much faster and so this is an example of maybe relatively simple algorithmic innovation. Ultimately, the impact of this algorithmic innovation was it really helped computation. So there are actually quite a lot of examples like this of where we <em><strong>change the algorithm because it allows that code to run much faster and this allows us to train bigger neural networks</strong></em>, or to do so the reason will decline even when we have a large network roam all the data.</p>
</div>
<p>The other reason that fast computation is important is that it turns out the process of training your network is very intuitive. Often, you have an idea for a neural network architecture and so you implement your idea and code. Implementing your idea then lets you run an experiment which tells you how well your neural network does and then by looking at it you go back to change the details of your new network and then you go around this circle over and over and when your new network takes a long time to train it just takes a long time to go around this cycle and there’s a huge difference in your productivity.</p>
<p>Building effective neural networks when you can have an idea and try it and see the work in ten minutes, or maybe at most a day, versus if you’ve to train your neural network for a month, which sometimes does happen, because you get a result back you know in ten minutes or maybe in a day you should just try a lot more ideas and be much more likely to discover in your network. And it works well for your application and so faster computation has really helped in terms of speeding up the rate at which you can get an experimental result back and this has really helped both practitioners of neural networks as well as researchers working and deep learning iterate much faster and improve your ideas much faster.</p>
<ol class="arabic simple" start="4">
<li><p>Practical Implications:</p>
<ul class="simple">
<li><p>Faster Training Cycles: Allows for more rapid testing and refinement of neural network models.</p></li>
<li><p>Increased Productivity: Shorter training times enable quicker experimentation and innovation.</p></li>
</ul>
</li>
<li><p>The Cycle of Neural Network Development:</p>
<ul class="simple">
<li><p>The Iterative Process: From conceptualization to implementation, experimentation, and refinement.</p></li>
<li><p>Real-World Impact: Faster computation allows for more agile development cycles.</p></li>
</ul>
</li>
</ol>
<p>So, all this has also been a huge boon to the entire deep learning research community which has been incredible with just inventing new algorithms and making nonstop progress on that front. So these are some of the forces powering the rise of deep learning but the good news is that these forces are still working powerfully to make deep learning even better. Take data… society is still throwing out more digital data. Or take computation, with the rise of specialized hardware like GPUs and faster networking many types of hardware, I’m actually quite confident that our ability to do very large neural networks from a computation point of view will keep on getting better and take algorithms relative to learning research communities are continuously phenomenal at elevating on the algorithms front.</p>
</section>
</section>
<section id="information">
<h3>Information<a class="headerlink" href="#information" title="Link to this heading">#</a></h3>
<p>About this course introduction also can see this <a class="reference external" href="https://youtu.be/LzcySrHxa40">video</a>.</p>
<p>The lecture notes are available on our community platform. If you’re already a member, log in to your account and access the lecture notes <a class="reference external" href="https://community.deeplearning.ai/t/dls-course-1-lecture-notes/11862">here</a>.</p>
<p><strong>[Important]</strong> The <a class="reference external" href="https://community.deeplearning.ai/c/course-q-a/deep-learning-specialization/6">Community</a></p>
<ul class="simple">
<li><p>Ask for help on assignments and other course content.</p></li>
<li><p>Discuss course topics.</p></li>
<li><p>Share your knowledge with other learners.</p></li>
<li><p>Build your network</p></li>
<li><p>Find out about exciting <a class="reference external" href="http://DeepLearning.AI">DeepLearning.AI</a> news, events and competitions!</p></li>
</ul>
</section>
<section id="quiz">
<h3>Quiz<a class="headerlink" href="#quiz" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Which of the following are reasons that didn’t allow Deep Learning to be developed during the ’80s? Choose all that apply.</p>
<p>A. People were afraid of a machine rebellion.</p>
<p>B. The theoretical tools didn’t exist during the 80’s.</p>
<p>C. Interesting applications such as image recognition require large amounts of data that were not available.</p>
<p>D. Limited computational power.</p>
</li>
<li><p>When building a neural network to predict housing price from features like size, the number of bedrooms, zip code, and wealth, it is necessary to come up with other features in between input and output like family size and school quality. True/False? __________</p></li>
<li><p>Images for cat recognition is an example of “structured” data, because it is represented as a structured array in a computer. True/False? __________</p></li>
<li><p>A dataset is composed of age and weight data for several people. This dataset is an example of “structured” data because it is represented as an array in a computer. True/False? __________</p></li>
<li><p>Why can an RNN (Recurrent Neural Network) be used to create English captions to French movies? Choose all that apply.</p>
<p>A. The RNN requires a small number of examples.</p>
<p>B. RNNs are much more powerful than a Convolutional neural Network (CNN).</p>
<p>C. The RNN is applicable since the input and output of the problem are sequences.</p>
<p>D. It can be trained as a supervised learning problem.</p>
</li>
</ol>
<p>Answer Questions 6 - 7 according the figure shows below:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/1-4.png"><img alt="_images/1-4.png" src="_images/1-4.png" style="height: 360px;" /></a>
</figure>
<ol class="arabic simple" start="6">
<li><p>From the given diagram, we can deduce that Large NN models are always better than traditional learning algorithms. True/False? __________</p></li>
<li><p>Assuming the trends described in the figure are accurate. The performance of a NN depends only on the size of the NN. True/False? __________</p></li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Click here for answers!</p>
<ol class="arabic">
<li><p>CD </br></p>
<p>C. Yes. Many resources used today to train Deep Learning projects come from the fact that our society digitizes almost everything, creating a large dataset to train Deep Learning models.</br></p>
<p>D. Yes. Deep Learning methods need a lot of computational power, and only recently the use of GPUs has accelerated the experimentation with Deep Learning.</br></p>
</li>
<li><p>False </br></p>
<p>A neural network figures out by itself the “features” in between using the samples used to train it. </br></p>
</li>
<li><p>False </br></p>
<p>Yes. Images for cat recognition are examples of “unstructured” data. </br></p>
</li>
<li><p>True </br></p>
<p>Yes, the sequences can be represented as arrays in a computer. This is an example of structured data. </br></p>
</li>
<li><p>CD </br></p>
<p>C. Yes, an RNN can map from a sequence of sounds (or audio files) to a sequence of words (the caption).</br></p>
<p>D. Yes, the data can be used as x (movie audio) to y (caption text).</br></p>
</li>
<li><p>False </br></p>
<p>Yes, when the amount of data is not large the performance of traditional learning algorithms is shown to be the same as NN. </br></p>
</li>
<li><p>False </br></p></li>
</ol>
</div>
</section>
</section>
<span id="document-C2"></span><section class="tex2jax_ignore mathjax_ignore" id="neural-networks-basics">
<span id="nnb"></span><h2>2 Neural Networks Basics<a class="headerlink" href="#neural-networks-basics" title="Link to this heading">#</a></h2>
<p>Set up a machine learning problem with a neural network mindset and use vectorization to speed up your models.</p>
<p><strong>Learning Objectives</strong></p>
<ul class="simple">
<li><p>Build a logistic regression model structured as a shallow neural network.</p></li>
<li><p>Build the general architecture of a learning algorithm, including parameter initialization, cost function and gradient calculation, and optimization implementation (gradient descent).</p></li>
<li><p>Implement computationally efficient and highly vectorized versions of models.</p></li>
<li><p>Compute derivatives for logistic regression, using a backpropagation mindset.</p></li>
<li><p>Use Numpy functions and <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> matrix/vector operations.</p></li>
<li><p>Work with <code class="docutils literal notranslate"><span class="pre">iPython</span></code> Notebooks.</p></li>
<li><p>Implement vectorization across multiple training examples.</p></li>
<li><p>Explain the concept of broadcasting.</p></li>
</ul>
<hr class="docutils" />
<section id="logistic-regreesion-as-a-neural-network">
<h3>Logistic Regreesion as a Neural Network<a class="headerlink" href="#logistic-regreesion-as-a-neural-network" title="Link to this heading">#</a></h3>
<section id="binary-classification">
<h4>Binary Classification<a class="headerlink" href="#binary-classification" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/kbrSXl43iPM">Video</a></p>
<p>Logistic regression is an algorithm for binary classification. So let’s start by setting up the problem.</p>
<p>Here’s an example of a binary classification problem. You might have an input of an image, and want to output a label to recognize this image as either being a cat, in which case you output 1, or not-cat in which case you output 0, and we’re going to use <span class="math notranslate nohighlight">\(y\)</span> to denote the output label. Let’s look at how an image is represented in a computer.</p>
<p>To store an image your computer stores three separate matrices corresponding to the red, green, and blue color channels of this image.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/2-1.png"><img alt="_images/2-1.png" src="_images/2-1.png" style="height: 300px;" /></a>
</figure>
<p>So if your input image is 64 pixels by 64 pixels (<span class="math notranslate nohighlight">\(64\times64\)</span>), then you would have <strong>three</strong> 64 by 64 matrices corresponding to the red, green and blue pixel intensity values for your images, which can be presented as <span class="math notranslate nohighlight">\(64\times64\times3\)</span>. Although to make this as an small example here, I drew these as much smaller matrices, so these are actually 5 by 4 matrices rather than 64 by 64 (<span class="math notranslate nohighlight">\(5\times4\times3\)</span>).</p>
<p>So to turn these pixel intensity values into a feature vector, what we’re going to do is <em><strong>unroll</strong></em> all of these pixel values into an input feature vector <span class="math notranslate nohighlight">\(x\)</span>. So to unroll all these pixel intensity values into a feature vector, what we’re going to do is define a feature vector <span class="math notranslate nohighlight">\(x\)</span> corresponding to this image as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 x = \left[
\begin{matrix}
255 \\
231 \\
42 \\
\vdots \\
124 \\
255 \\
134 \\
202 \\
\vdots \\
94 \\
255 \\
134 \\
93 \\
\vdots \\
142
\end{matrix}
\right], \quad y = 1
\end{split}\]</div>
<ul class="simple">
<li><p>We’re just going to take all the pixel values 255, 231, and so on until we’ve listed all the red pixels.</p></li>
<li><p>And then eventually 255, 134 and so on until we get a long feature vector listing out all the red, green and blue pixel intensity values of this image.</p></li>
</ul>
<p>If this image is a 64 by 64 image, the total dimension of this vector <span class="math notranslate nohighlight">\(x\)</span> will be <span class="math notranslate nohighlight">\(64\times64\times3\)</span> because that’s the total numbers we have in all of these matrixes. Which in this case, turns out to be 12,288, that’s what you get if you multiply all those numbers. Using <span class="math notranslate nohighlight">\(n_x = 12,288\)</span> to represent the dimension of the input features <span class="math notranslate nohighlight">\(x\)</span>. And sometimes for brevity, I will also just use lowercase <span class="math notranslate nohighlight">\(n\)</span> to represent the dimension of this input feature vector.</p>
<p>So in binary classification, our goal is to learn a classifier that can input an image represented by this feature vector <span class="math notranslate nohighlight">\(x\)</span>. And predict whether the corresponding label <span class="math notranslate nohighlight">\(y\)</span> is 1 or 0, that is, whether this is a cat image or a non-cat image.</p>
<p>Let’s now lay out some of the notation that we’ll use throughout the rest of this book.</p>
<ul class="simple">
<li><p>A single training example is represented by a pair <span class="math notranslate nohighlight">\((x,y)\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is an <span class="math notranslate nohighlight">\(x\)</span>-dimensional feature vector <span class="math notranslate nohighlight">\((x \in  \mathbb{R}^{n_x})\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, the label, is either 0 or 1 <span class="math notranslate nohighlight">\((y \in \{0,1\})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> training examples:  <span class="math notranslate nohighlight">\(\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)}) \}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(m_{\text{train}}\)</span> denotes the number of training samples; <span class="math notranslate nohighlight">\(m_{\text{test}}\)</span> denotes the number of test samples.</p></li>
<li><p><span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n_x \times m}\)</span> is the input matrix. In python, <code class="docutils literal notranslate"><span class="pre">X.shape</span></code> is <span class="math notranslate nohighlight">\((n_x,m)\)</span>. <span class="math notranslate nohighlight">\(n_x\)</span> rows and m columns.</p></li>
<li><p><span class="math notranslate nohighlight">\(x^{(i)} \in \mathbb{R}^{n_x}\)</span> is the <span class="math notranslate nohighlight">\(i^{th}\)</span> example represented as a column vector.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}X= \left[
\begin{matrix}
\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \\
\vdots &amp; \vdots &amp; \cdots &amp; \vdots 
\end{matrix}
\right]
\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y \in \mathbb{R}^{1 \times m}\)</span> is the label matrix. In python, <code class="docutils literal notranslate"><span class="pre">Y.shape</span></code> is <span class="math notranslate nohighlight">\((1,m)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Y=[y^{(1)}, y^{(2)}, \cdots, y^{(m)}]\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the output label for the <span class="math notranslate nohighlight">\(i^{th}\)</span> example.</p></li>
</ul>
</section>
<section id="logistic-regression">
<h4>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">#</a></h4>
<p>Logistic regression is a learning algorithm used in a supervised learning problem when the output <span class="math notranslate nohighlight">\(y\)</span> are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.</p>
<p><a class="reference external" href="https://youtu.be/4u0_TJhNGY8">Video</a></p>
<p>In this section, we will go over logistic regression. This is a learning algorithm that you use when the output labels <span class="math notranslate nohighlight">\(Y\)</span> in a supervised learning problem are all either zero or one, so for binary classification problems.</p>
<p>Example: Cat vs No-cat</p>
<ul class="simple">
<li><p>Given an input feature vector <span class="math notranslate nohighlight">\(x\)</span> maybe corresponding to an image that you want to recognize as either a cat picture or not a cat picture, you want an algorithm that can output a prediction, which we will call y hat <span class="math notranslate nohighlight">\((\hat{y})\)</span>, which is your estimate of <span class="math notranslate nohighlight">\(y\)</span>.  More formally, you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be the <em><strong>probability of the chance</strong></em> that, <span class="math notranslate nohighlight">\(\hat{y} = \mathrm{P}(y=1\mid x)\)</span> (<span class="math notranslate nohighlight">\(y\)</span> is equal to one given the input features <span class="math notranslate nohighlight">\(x\)</span>).  So in other words, if <span class="math notranslate nohighlight">\(x\)</span> is a picture, you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to tell you, what is the chance that this is a cat picture.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Given } x, \quad \hat{y} = \mathrm{P}(y=1\mid x), \quad \text{where } 0 \leq \hat{y} \leq 1\]</div>
<p>The parameters used in Logistic regression are:</p>
<ul class="simple">
<li><p>The input features vector: <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n_x}\)</span>, where <span class="math notranslate nohighlight">\(n_x\)</span> is the number of features. That is, <span class="math notranslate nohighlight">\(x\)</span> is an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector.</p></li>
<li><p>The training label: <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span>.</p></li>
<li><p>The weights: <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}\)</span>. <span class="math notranslate nohighlight">\(w\)</span> is also an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector.</p></li>
<li><p>The threshold: <span class="math notranslate nohighlight">\(b \in \mathbb{R}\)</span></p></li>
<li><p>Summary: given that the parameters of logistic regression will be <span class="math notranslate nohighlight">\(w\)</span> which is also an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector <span class="math notranslate nohighlight">\((w \in \mathbb{R}^{n_x})\)</span>, together with <span class="math notranslate nohighlight">\(b\)</span> which is just a real number <span class="math notranslate nohighlight">\((b \in \mathbb{R})\)</span>.</p></li>
<li><p>So given an input <span class="math notranslate nohighlight">\(x\)</span> and the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, how do we generate the output <span class="math notranslate nohighlight">\(\hat{y}\)</span>?</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-2-1">
<span class="eqno">(1)<a class="headerlink" href="#equation-2-1" title="Link to this equation">#</a></span>\[\hat{y} = \text{sigmoid} (z) = \sigma(z) = \sigma(w^T x + b)\]</div>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/2-2-sigmoid.png"><img alt="_images/2-2-sigmoid.png" src="_images/2-2-sigmoid.png" style="height: 200px;" /></a>
</figure>
<div class="important admonition">
<p class="admonition-title">Why we are using the sigmoid function here?</p>
<p><span class="math notranslate nohighlight">\((w^T x + b)\)</span> is a linear function <span class="math notranslate nohighlight">\((ax+b)\)</span>, but since we are looking for a probability constraint between <span class="math notranslate nohighlight">\([0,1]\)</span>, the sigmoid function is used. The function is bounded between <span class="math notranslate nohighlight">\([0,1]\)</span> as shown in the graph above.</p>
</div>
<p>This is what the sigmoid function looks like.</p>
<div class="math notranslate nohighlight" id="equation-2-2">
<span class="eqno">(2)<a class="headerlink" href="#equation-2-2" title="Link to this equation">#</a></span>\[\sigma(z) = \dfrac{1}{1+e^{(-z)}}\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(z\)</span> is a very large positive number, then <span class="math notranslate nohighlight">\(\sigma(z)\)</span> will be close to 1.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(z\)</span> is a very large negative number, then <span class="math notranslate nohighlight">\(\sigma(z)\)</span> will be close to 0.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(z = 0\)</span>, then  <span class="math notranslate nohighlight">\(\sigma(z) = 0.5\)</span></p></li>
</ul>
<p>So when you implement logistic regression, your job is to try to learn parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> so that <span class="math notranslate nohighlight">\(\hat{y}\)</span> becomes a good estimate of the chance of <span class="math notranslate nohighlight">\(y\)</span> being equal to one.</p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p>Q: <strong>What are the parameters of logistic regression?</strong>  </br>
A. <span class="math notranslate nohighlight">\(w\)</span>, an identity vector, and <span class="math notranslate nohighlight">\(b\)</span>, a real number.  </br>
B. <span class="math notranslate nohighlight">\(w\)</span>, an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector, and <span class="math notranslate nohighlight">\(b\)</span>, a real number.  </br>
C. <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>,  both <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector.  </br>
D. <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>,  both real number.</p>
</div>
</section>
<section id="logistic-regression-cost-function">
<h4>Logistic Regression Cost Function<a class="headerlink" href="#logistic-regression-cost-function" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/9K62xu8MKMk">Video</a></p>
<p>In the previous section, you saw the logistic regression model to train the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, of logistic regression model. You need to define a cost function, let’s take a look at the cost function.</p>
<p>To learn parameters for your model, you’re given a training set of training examples and it seems natural that you want to find parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.  So that at least on the training set, the outputs you have the predictions you have on the training set, which that the preicition values will be close to the true labels y that you got in the training set.</p>
<ul class="simple">
<li><p>Given <span class="math notranslate nohighlight">\(m\)</span> examples: <span class="math notranslate nohighlight">\(\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)}) \}\)</span>, want <span class="math notranslate nohighlight">\(\hat{y}^{(i)} \approx y^{(i)}\)</span></p></li>
<li><p>And of course for each training example, we’re using these superscripts with round brackets with parentheses to index into different train examples. Your prediction on training example <span class="math notranslate nohighlight">\(i\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-2-3">
<span class="eqno">(3)<a class="headerlink" href="#equation-2-3" title="Link to this equation">#</a></span>\[\hat{y}^{(i)} = \sigma(w^T x^{(i)} + b), \quad \text{where } \sigma(z^{(i)}) = \dfrac{1}{1+e^{(-z^{(i)})}}\]</div>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(i\)</span>-th example: <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, <span class="math notranslate nohighlight">\(z^{(i)}\)</span>.</p></li>
</ul>
<p>Now let’s see what loss function or an error function we can use to measure how well our algorithm is doing.</p>
<p><strong>Loss (error) function:</strong></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = - \Big( y^{(i)} \  \text{log}(\hat{y}^{(i)}) + (1-y^{(i)}) \ \text{log}(1-\hat{y}^{(i)}) \Big)\]</div>
<div class="math notranslate nohighlight" id="equation-2-4">
<span class="eqno">(4)<a class="headerlink" href="#equation-2-4" title="Link to this equation">#</a></span>\[\mathcal{L}(\hat{y}, y) = - \Big( y \  \text{log}(\hat{y}) + (1-y) \ \text{log}(1-\hat{y}) \Big)\]</div>
<p>This function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is called the <em><strong>loss function</strong></em> is a function will need to define to measure how good our output <span class="math notranslate nohighlight">\(\hat{y}\)</span> is when the true label is <span class="math notranslate nohighlight">\(y\)</span>. The loss function measures the discrepancy between the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the desired output <span class="math notranslate nohighlight">\(y\)</span>. In other words, the loss function computes the error for a single training example.</p>
<p>Keep in mind that if we are using squared error then you want to square error to be as small as possible. And with this logistic regression, lost function will also want this to be as small as possible. To understand why this makes sense, let’s look at the two cases:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y = 1\)</span>, then <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y) = - \text{log}(\hat{y})\)</span>. So you want <span class="math notranslate nohighlight">\(- \text{log}(\hat{y})\)</span> to be as small as possible, that means you want <span class="math notranslate nohighlight">\(\text{log}(\hat{y})\)</span> to be as big as possible, and that means you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be large. But because <span class="math notranslate nohighlight">\(\hat{y}\)</span> is you know the sigmoid function, it can never be bigger than one. So this is saying that if <span class="math notranslate nohighlight">\(y = 1\)</span>, you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be as big as possible, but it can’t ever be bigger than one. So saying you want, <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be <strong>close to one</strong> as well.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(y = 0\)</span>, then <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y) = - \text{log}(1-\hat{y})\)</span>. So if in your learning procedure you try to make the loss function small. What this means is that you want, <span class="math notranslate nohighlight">\(\text{log}(1-\hat{y})\)</span> to be large. And then through a similar piece of reasoning, you can conclude that this loss function is trying to make <span class="math notranslate nohighlight">\(\hat{y}\)</span> as small as possible, and again, because <span class="math notranslate nohighlight">\(\hat{y} \in \{0,1\}\)</span> . This is saying that if <span class="math notranslate nohighlight">\(y = 0\)</span> then your loss function will push the parameters to make <span class="math notranslate nohighlight">\(\hat{y}\)</span> as <strong>close to zero</strong> as possible.</p></li>
</ul>
<p>We just gave here a somewhat informal justification for this particular loss function, we will provide more details later to give a more formal justification for <span class="math notranslate nohighlight">\(y\)</span>. In logistic regression, we like to use the loss function with this particular form.</p>
<div class="important admonition">
<p class="admonition-title">Why DO NOT use the squared error, <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y)= \dfrac{1}{2}(\hat{y}-y)^2\)</span>, in the loss function?</p>
<p>It turns out that you could do this, but in logistic regression people don’t usually do this. Because when you come to learn the parameters, you find that the optimization problem, which becomes non-convex. So you end up with optimization problem, you are with multiple local optima. So gradient descent, may not find a global optimum.</p>
<p>Squared eror seems like it might be a reasonable choice except that it makes great in descent not work well. So in logistic regression were actually define a different loss function that plays a similar role as squared error but will give us an optimization problem that is convex.</p>
</div>
<p>The loss function was defined with respect to a single training example. It measures how well you’re doing on a single training example, I’m now going to define something called the cost function, which measures how are you doing on the entire training set.</p>
<p><strong>Cost function:</strong></p>
<p>The cost function is the average of the loss function of the entire training set. We are going to find the parameters <span class="math notranslate nohighlight">\(w\)</span> 𝑎𝑛𝑑 <span class="math notranslate nohighlight">\(b\)</span> that minimize the overall cost function.</p>
<div class="math notranslate nohighlight" id="equation-2-5">
<span class="eqno">(5)<a class="headerlink" href="#equation-2-5" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{J}(w,b) 
&amp;= \dfrac{1}{m} \sum_{i=1}^{m}  \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) \\
&amp;= - \dfrac{1}{m} \sum_{i=1}^{m} \Big[ \Big( y^{(i)} \  \text{log}(\hat{y}^{(i)}) + (1-y^{(i)}) \ \text{log}(1-\hat{y}^{(i)}) \Big) \Big]
\end{aligned}\end{split}\]</div>
<p>So the cost function <span class="math notranslate nohighlight">\(\boldsymbol{J}\)</span>, which is applied to your parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, is going to be the average, one of the <span class="math notranslate nohighlight">\(m\)</span> of the sum of the loss function apply to each of the training examples in turn.</p>
<p>The terminology I’m going to use is that the loss function is applied to just a single training example, check out equation <a class="reference internal" href="#equation-2-4">(4)</a>. And the cost function is the cost of your parameters, so in training your logistic regression model, we’re going to try to find parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. That minimize the overall cost function <span class="math notranslate nohighlight">\(\boldsymbol{J}\)</span>, written at the equation <a class="reference internal" href="#equation-2-5">(5)</a>.</p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p>Q: <strong>What is the difference between the cost function and the loss function for logistic regression?</strong>  </br>
A. The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.  </br>
B. The cost function computes the error for a single training example; the loss function is the average of the cost functions of the entire training set.  </br>
C. They are different names for the same function.  </br></p>
</div>
</section>
<section id="explanation-of-logisitic-regression-cost-function-optional">
<h4>Explanation of Logisitic Regression Cost Function (Optional)<a class="headerlink" href="#explanation-of-logisitic-regression-cost-function-optional" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/2vzqUcivE0A">Video</a></p>
<p>This is an optional section. In this part, we will have a quick justification for why we like to use that cost function for logistic regression.</p>
<section id="logisitc-regression-cost-function">
<h5>Logisitc regression cost function<a class="headerlink" href="#logisitc-regression-cost-function" title="Link to this heading">#</a></h5>
<p>In logistic regression,</p>
<div class="math notranslate nohighlight">
\[\hat{y} = \sigma(w^T + b), \quad \text{where } \sigma (z) = \dfrac{1}{1+e^{-z}}\]</div>
<p>The interpretation is <span class="math notranslate nohighlight">\(\hat{y} = \mathrm{P}(y=1\mid x)\)</span>. So we want our algorithm to output <span class="math notranslate nohighlight">\(\hat{y}\)</span> has the chance that <span class="math notranslate nohighlight">\(y = 1\)</span> for a given set of input features <span class="math notranslate nohighlight">\(x\)</span>. So another way to say this is that if <span class="math notranslate nohighlight">\(y\)</span> is equal to 1 then the chance of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> is equal to <span class="math notranslate nohighlight">\(\hat{y}\)</span>,  and conversely if <span class="math notranslate nohighlight">\(y\)</span> is equal to 0 then the chance that y was 0 was <span class="math notranslate nohighlight">\(1-\hat{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{If } y = 1: \qquad \mathrm{P}(y\mid x) &amp;= \hat{y} \\
\text{If } y = 0: \qquad \mathrm{P}(y\mid x) &amp;= 1-\hat{y} 
\end{aligned}
\end{split}\]</div>
<p>So if <span class="math notranslate nohighlight">\(\hat{y}\)</span> was a chance that <span class="math notranslate nohighlight">\(y = 1\)</span>, then <span class="math notranslate nohighlight">\(1-\hat{y}\)</span> is the chance that <span class="math notranslate nohighlight">\(y = 0\)</span>.</p>
<p>So what I’m going to do is take these two equations which basically define <span class="math notranslate nohighlight">\(\mathrm{P}(y \mid x)\)</span> for the two cases of <span class="math notranslate nohighlight">\(y = 0\)</span> or <span class="math notranslate nohighlight">\(y = 1\)</span>. And then take these two equations and summarize them into a single equation. And just to point out <span class="math notranslate nohighlight">\(y\)</span> has to be either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span> because in binary cost equations, <span class="math notranslate nohighlight">\(y = 0\)</span> or <span class="math notranslate nohighlight">\(y = 1\)</span> are the only two possible cases. When someone take these two equations and summarize them as follows:</p>
<div class="math notranslate nohighlight">
\[\mathrm{P}(y \mid x) = \hat{y}^y \  (1-\hat{y})^{(1-y)}\]</div>
<p>Now, because the <span class="math notranslate nohighlight">\(\text{log}\)</span> function is a strictly monotonically increasing function, your maximizing <span class="math notranslate nohighlight">\(\text{log} \ \mathrm{P}(y \mid x)\)</span> should give you a similar result as optimizing <span class="math notranslate nohighlight">\(\mathrm{P}(y \mid x)\)</span>. So:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{log} \ \mathrm{P}(y \mid x)
&amp;=  y \  \text{log}(\hat{y}) + (1-y) \ \text{log}(1-\hat{y})  \\
&amp;=  - \mathcal{L}(\hat{y}, y)
\end{aligned}
\end{split}\]</div>
<p>So this is actually negative of the loss function that we had to find previously. And there is a negative sign there because usually if you are training a learning algorithm, you want to make probabilities large. Whereas in logistic regression, we want to minimize the loss function. So minimizing the loss corresponds to maximizing the log of the probability. So this is what the loss function on a single example looks like.</p>
<p>So this is what the loss function on a single example looks like. How about the cost function, the overall cost function on the entire training set on m examples? Let’s figure that out.</p>
</section>
<section id="cost-on-m-example">
<h5>Cost on m example<a class="headerlink" href="#cost-on-m-example" title="Link to this heading">#</a></h5>
<p>So, the probability of all the labels In the training set. If you assume that the training examples I’ve drawn independently or drawn IID, identically independently distributed, then the probability of the example is the product of probabilities.</p>
<div class="math notranslate nohighlight">
\[\mathrm{P}(\text{labels in training set}) = \prod_{i=1}^{m} \mathrm{P}(y^{(i)} \mid x^{(i)})\]</div>
<p>And so if you want to carry out maximum likelihood estimation, then you want to find the parameters that maximizes the chance of your observations at training set. But maximizing this is the same as maximizing the log, so we just put logs on both sides:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{log} \ \mathrm{P}
&amp;=  \sum_{i=1}^{m} \text{log} \  \mathrm{P}(y^{(i)} \mid x^{(i)})\\
&amp;=  - \sum_{i=1}^{m} \ \mathcal{L}(\hat{y}^{(i)}, y^{(i)})
\end{aligned}
\end{split}\]</div>
<p>And so in statistics, there’s a principle called the principle of maximum likelihood estimation, which just means to choose the parameters that maximizes <span class="math notranslate nohighlight">\(\text{log} \ \mathrm{P}\)</span>. Or in other words, that maximizes this <span class="math notranslate nohighlight">\(\text{log} \ \mathrm{P}\)</span>.</p>
<p>So this justifies the cost we had for logistic regression which is <span class="math notranslate nohighlight">\(\boldsymbol{J}(w,b) \)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{J}(w,b) = \dfrac{1}{m} \sum_{i=1}^{m}  \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = - \dfrac{1}{m} \text{log} \ \mathrm{P}\]</div>
<p>Because we now want to minimize the cost instead of maximizing likelihood, we’ve got to rid of the minus sign. And then finally for convenience, to make sure that our quantities are better scale, we just add a 1 over <span class="math notranslate nohighlight">\(m\)</span> extra scaling factor there. But so to summarize, by minimizing this cost function <span class="math notranslate nohighlight">\(\boldsymbol{J}(w,b) \)</span> we’re really carrying out maximum likelihood estimation with the logistic regression model. Under the assumption that our training examples were IID, or identically independently distributed.</p>
<p>I hope this gives you a sense of why we use the cost function we do for logistic regression.</p>
</section>
</section>
<section id="gradient-descent">
<h4>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/Nuxx65l5pUI">Video</a></p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p><strong>True or False</strong>: A convex function always has multiple local optima.  __________  </br></p>
</div>
</section>
<section id="derivatives">
<h4>Derivatives<a class="headerlink" href="#derivatives" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/Mz4VYF4CxGg">Video</a></p>
</section>
<section id="more-derivative-examples">
<h4>More Derivative Examples<a class="headerlink" href="#more-derivative-examples" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/6PSzTakWAO0">Video</a></p>
<ol class="arabic simple">
<li><p><strong>Derivative just means slope of a line.</strong></p>
<ul class="simple">
<li><p>The derivative of the function just means the slope of a function. The slope of a function can be different at different points on the function.</p></li>
<li><p>In our first example where <span class="math notranslate nohighlight">\(f(a) = 3a\)</span> is a straight line. The derivative was the same everywhere, it was three everywhere. For other functions like <span class="math notranslate nohighlight">\(f(a) = a^2\)</span> or <span class="math notranslate nohighlight">\(f(a) = \text{log}(a)\)</span>, the slope of the line varies. So, the slope or the derivative can be different at different points on the curve.</p></li>
</ul>
</li>
<li><p>If you want to look up the derivative of a function, you can flip open your calculus textbook or look up Wikipedia and often get a formula for the slope of these functions at different points.</p></li>
</ol>
</section>
<section id="computation-graph">
<h4>Computation Graph<a class="headerlink" href="#computation-graph" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/mS-SIq7ENuQ">Video</a></p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p>Q: <strong>One step of ________ propagation on a computation graph yields derivative of final output variable.</strong>  </br>
A. Backward  </br>
B. Forward </br></p>
</div>
</section>
<section id="derivatives-with-a-computation-graph">
<h4>Derivatives with a Computation Graph<a class="headerlink" href="#derivatives-with-a-computation-graph" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/VTlwSyuVIXU">Video</a></p>
</section>
<section id="logistic-regression-gradient-descent">
<h4>Logistic Regression Gradient Descent<a class="headerlink" href="#logistic-regression-gradient-descent" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/hZqWRCRu9N0">Video</a></p>
<div class="math notranslate nohighlight" id="equation-2-6">
<span class="eqno">(6)<a class="headerlink" href="#equation-2-6" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
z &amp;= w^T x + b \\
\hat{y} &amp;= a = \sigma(z) = \dfrac{1}{1+e^{-z}} \\
\mathcal{L}(a, y) &amp;= - \Big( y \  \text{log}(a) + (1-y) \ \text{log}(1-a) \Big)
\end{aligned}\end{split}\]</div>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/2-3.png"><img alt="_images/2-3.png" src="_images/2-3.png" style="height: 120px;" /></a>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Python coding convention <code class="docutils literal notranslate"><span class="pre">dvar</span></code> represents:
The derivative of a final output variable with respect to various intermediate quantities.</p>
</div>
<p><strong>Derivatives:</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">da</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} = -\dfrac{y}{a} + \dfrac{1-y}{1-a}\)</span></p>
<p>Using the chain rule to calculate <code class="docutils literal notranslate"><span class="pre">dz</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">dz</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}z} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times  \dfrac{\mathrm{d}a}{\mathrm{d}z}\)</span></p>
<div class="math notranslate nohighlight" id="equation-2-7">
<span class="eqno">(7)<a class="headerlink" href="#equation-2-7" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\dfrac{\mathrm{d}a}{\mathrm{d}z} &amp;=  \dfrac{\mathrm{d}}{\mathrm{d}z} \sigma(z)  \\
&amp;= \dfrac{\mathrm{d}}{\mathrm{d}z} \Bigg( \dfrac{1}{1+e^{-z}} \Bigg) \\
&amp;= \dfrac{e^{-z}}{(1+e^{-z})^2}
\end{aligned}\end{split}\]</div>
<p>From equation <span class="math notranslate nohighlight">\(a = \dfrac{1}{1+e^{-z}}\)</span>, we can derive:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac{\mathrm{d}a}{\mathrm{d}z} &amp;= \dfrac{e^{-z}}{(1+e^{-z})^2} \\
&amp;= \dfrac{1}{1+e^{-z}} \cdot \Big(1- \dfrac{1}{1+e^{-z}} \Big) \\
&amp;= a(1-a)
\end{aligned}\end{split}\]</div>
<p><code class="docutils literal notranslate"><span class="pre">dz</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times  \dfrac{\mathrm{d}a}{\mathrm{d}z} = \Bigg( -\dfrac{y}{a} + \dfrac{1-y}{1-a}\Bigg) \times a(1-a) = a-y\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">dw1</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}w_1} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}w_1} = (a-y)x_1\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">dw2</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}w_2} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}w_2} = (a-y)x_2\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">db</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}b} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}b} = a-y\)</span></p>
</section>
<section id="gradient-descent-on-m-examples">
<h4>Gradient Descent on m Examples<a class="headerlink" href="#gradient-descent-on-m-examples" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/4ckUNzRGgDI">Video</a></p>
</section>
</section>
<section id="python-and-vectoraization">
<h3>Python and Vectoraization<a class="headerlink" href="#python-and-vectoraization" title="Link to this heading">#</a></h3>
<section id="vectorization">
<h4>Vectorization<a class="headerlink" href="#vectorization" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/vMFJQbMIaQc">Video</a></p>
<p>Vectorization is basically the art of getting rid of explicit for loops in your code. In the deep learning era, especially in deep learning in practice, you often find yourself training on relatively large data sets, because that’s when deep learning algorithms tend to shine. And so, it’s important that your code very quickly because otherwise, if it’s training a big data set, your code might take a long time to run then you just find yourself waiting a very long time to get the result. So in the deep learning era, I think the ability to perform vectorization has become a key skill. Let’s start with an example.</p>
<p>In logistic regreesion, you need to solve this kind of problem:</p>
<div class="math notranslate nohighlight">
\[z = w^T x + b \quad \text{where } w \in \mathbb{R}^{n_x},  x \in  \mathbb{R}^{n_x}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> was this column vector and <span class="math notranslate nohighlight">\(x\)</span> is also this vector. Maybe they are very large vectors if you have a lot of features. So, <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(x\)</span> were both <span class="math notranslate nohighlight">\(\mathbb{R}^{n_x}\)</span> dimensional vectors.</p>
<p>So, to compute <span class="math notranslate nohighlight">\(w\)</span> transpose <span class="math notranslate nohighlight">\(x\)</span>, if you had a <em><strong>non-vectorized</strong></em> implementation, you would do something like <code class="docutils literal notranslate"><span class="pre">for</span></code> loop:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n_x</span><span class="p">):</span>
	<span class="n">z</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">z</span> <span class="o">+=</span> <span class="n">b</span>
</pre></div>
</div>
<p>That’s a non-vectorized implementation. Then you find that that’s going to be really slow. In contrast, a <em><strong>vectorized</strong></em> implementation would just compute <span class="math notranslate nohighlight">\(w\)</span> transpose <span class="math notranslate nohighlight">\(x\)</span> directly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>And you find that this is much faster. Let’s actually illustrate this with a little demo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1 2 3 4]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>

<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vectorized version: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
	<span class="n">c</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Non-Vectorized version: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c = 249881.3198297877
Vectorized version: 1.0280609130859375ms
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c = 249881.3198297821
Non-Vectorized version: 211.3649845123291ms
</pre></div>
</div>
</div>
</div>
<p>In both cases, the vectorize version and the non-vectorize version computed the same values, 249979, so on. The vectorize version took 0.4 milliseconds. The explicit for loop and non-vectorize version took about 255, almost 260 milliseconds. The non-vectorize version took something like 600 times longer than the vectorize version. With this example you see that if only you remember to vectorize your code, your code actually runs over 600 times faster.</p>
<p>If the engine slows down, it’s the difference between your code taking maybe one minute to run versus taking say five hours to run. And when you are implementing deep learning algorithms, you can really get a result back faster. It will be much faster if you vectorize your code.</p>
<p>Some of you might have heard that a lot of scaleable deep learning implementations are done on a GPU or a graphics processing unit. But all the demos I did just now in the Jupiter notebook where actually on the CPU. And it turns out that both GPU and CPU have parallelization instructions. They’re sometimes called SIMD instructions. This stands for a single instruction multiple data. But what this basically means is that, if you use built-in functions such as this <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code> function or other functions that don’t require you explicitly implementing a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. It enables Phyton <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> to take much better advantage of parallelism to do your computations much faster.  And this is true both computations on CPUs and computations on GPUs. It’s just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that. Maybe just not as good as GPUs.</p>
<p>You’re seeing how vectorization can significantly speed up your code. The rule of thumb to remember is <em><strong>whenever possible, avoid using explicit for loops</strong></em>.</p>
</section>
<section id="more-vectorization-examples">
<h4>More Vectorization Examples<a class="headerlink" href="#more-vectorization-examples" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/QLyqDRZrNm0">Video</a></p>
<div class="hint admonition">
<p class="admonition-title">Neural Network programming guideline</p>
<p>Whenever possible, avoid explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
</div>
<p>And it’s not always possible to never use a for-loop, but when you can use a built in function or find some other way to compute whatever you need, you’ll often go faster than if you have an explicit for-loop.</p>
<p>Here is an another example, if ever you want to compute a vector <span class="math notranslate nohighlight">\(u\)</span> as the product of the matrix <span class="math notranslate nohighlight">\(A\)</span>, and another vector <span class="math notranslate nohighlight">\(v\)</span>, then the definition of our matrix multiply is that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
u &amp;= Av \\
u_i &amp;= \sum_j A_{ij} v_j
\end{aligned}\end{split}\]</div>
<p>Non-vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="o">...</span> <span class="p">:</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="o">...</span> <span class="p">:</span> 
	 <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
<p>Vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<section id="vectors-and-matrix-valued-functions">
<h5>Vectors and matrix valued functions<a class="headerlink" href="#vectors-and-matrix-valued-functions" title="Link to this heading">#</a></h5>
<p>Say you need to apply the exponential operation on every element of matrix/vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}v = \left[
\begin{matrix}
v_1 \\
\vdots \\
v_n
\end{matrix}
\right], \quad
u = \left[
\begin{matrix}
e^{v_1} \\
\vdots \\
e^{v_n} 
\end{matrix}
\right]
\end{split}\]</div>
<p>Non-vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
     <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
<p>Vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<p>And so, notice that, whereas previously you had that explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, with just one line of code here, just <span class="math notranslate nohighlight">\(v\)</span> as an input vector <span class="math notranslate nohighlight">\(u\)</span> as an output vector, you’ve gotten rid of the explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, and the implementation on the right will be much faster that the one needing an explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
<p>In fact, the NumPy library has many of the vector value functions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>   <span class="c1">#compute the element-wise log</span>
<span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">v</span><span class="o">**</span><span class="mi">2</span>
<span class="mi">1</span><span class="o">/</span><span class="n">v</span>
</pre></div>
</div>
<p>So, whenever you are tempted to write a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop take a look, and see if there’s a way to call a <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> built-in function to do it without that <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
</section>
<section id="gradient-descent-implementation">
<h5>Gradient descent implementation<a class="headerlink" href="#gradient-descent-implementation" title="Link to this heading">#</a></h5>
<p>So, let’s take all of these learnings and apply it to our logistic regression gradient descent implementation, and see if we can at least get rid of one of the two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops we had. So here’s our code for computing the derivatives for logistic regression, and we had two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">m</span> <span class="p">:</span>
	<span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
	<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
	<span class="c1"># in this example we only have 2 features, if yo had more features, see below</span>
	<span class="n">dw1</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">dw2</span> <span class="o">+=</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
	<span class="c1"># more features</span>
	<span class="k">for</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">nx</span> <span class="p">:</span>
		<span class="n">dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
</pre></div>
</div>
<p>So the way we’ll do so is that instead of explicitly initializing <code class="docutils literal notranslate"><span class="pre">dw1</span></code>, <code class="docutils literal notranslate"><span class="pre">dw2</span></code>, and so on to zeros, we’re going to get rid of this and instead make dw a vector.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_x</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">m</span> <span class="p">:</span>
	<span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
	<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
	<span class="n">dw</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
</pre></div>
</div>
<p>So now we’ve gone from having two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops to just one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. We still have this one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop that loops over the individual training examples.</p>
<p>So I hope this section gives you a sense of vectorization. And by getting rid of one for-loop your code will already run faster. But it turns out we could do even better. So the next section will talk about how to vectorize logistic aggression even further. And you see a pretty surprising result, that without using any for-loops, without needing a for-loop over the training examples, you could write code to process the entire training sets. So, pretty much all at the same time.</p>
</section>
</section>
<section id="vectorizting-logistic-regression">
<h4>Vectorizting Logistic Regression<a class="headerlink" href="#vectorizting-logistic-regression" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/A9Ag0PtZDLA">Video</a></p>
<p>To compute these predictions on our <span class="math notranslate nohighlight">\(m\)</span> training examples, there is a way to do so, without needing an explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
<ul class="simple">
<li><p>First, remember that we defined a matrix capital X to be your training inputs, stacked together in different columns like this.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}X= \left[
\begin{matrix}
\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \\
\vdots &amp; \vdots &amp; \cdots &amp; \vdots 
\end{matrix}
\right]
\end{split}\]</div>
<p>So, this is a matrix, that is a <span class="math notranslate nohighlight">\(n_x \times m\)</span> dimensional matrix. Now, the first thing I want to do is show how you can compute <span class="math notranslate nohighlight">\(z^{(1)}\)</span>, <span class="math notranslate nohighlight">\(z^{(2)}\)</span>, <span class="math notranslate nohighlight">\(z^{(3)}\)</span> and so on, all in one step, in fact, with one line of code. So, I’m going to construct a <span class="math notranslate nohighlight">\(1 \times m\)</span> matrix that’s really a row vector while I’m going to compute <span class="math notranslate nohighlight">\(z^{(1)}\)</span>, <span class="math notranslate nohighlight">\(z^{(2)}\)</span>, and so on, down to <span class="math notranslate nohighlight">\(z^{(m)}\)</span>, all at the same time.</p>
<div class="math notranslate nohighlight">
\[Z = [z^{(1)}, z^{(2)}, \cdots, z^{(m)}] = w^T X+ [b, b, \cdots, b] = [w^T x^{(1)} + b, w^T x^{(2)} + b, \cdots, w^T x^{(m)} + b]\]</div>
<p>The <span class="math notranslate nohighlight">\(w^T\)</span> will be a row vector. <span class="math notranslate nohighlight">\([b, b, \cdots, b]\)</span> is a <span class="math notranslate nohighlight">\(1 \times m\)</span> row vector. So you end up with another <span class="math notranslate nohighlight">\(1 \times m\)</span> vector.</p>
<p>So just as <span class="math notranslate nohighlight">\(X\)</span> was once obtained, when you took your training examples and stacked them next to each other, stacked them horizontally. I’m going to define capital <span class="math notranslate nohighlight">\(Z\)</span> to be this where you take the lowercase <span class="math notranslate nohighlight">\(z\)</span>’s and stack them horizontally. So when you stack the lower case <span class="math notranslate nohighlight">\(x\)</span>’s corresponding to a different training examples, horizontally you get this variable capital <span class="math notranslate nohighlight">\(X\)</span> and the same way when you take these lowercase <span class="math notranslate nohighlight">\(z\)</span> variables, and stack them horizontally, you get this variable capital <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>And it turns out, that in order to implement this, the <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> command is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>Now there is a subtlety in Python, which is at here <code class="docutils literal notranslate"><span class="pre">b</span></code> is a real number or if you want to say you know <span class="math notranslate nohighlight">\(1 \times 1\)</span> matrix, is just a normal real number. But, when you add this vector to a real number, Python automatically takes this real number <code class="docutils literal notranslate"><span class="pre">b</span></code> and expands it out to this  <span class="math notranslate nohighlight">\(1 \times m\)</span> row vector (i.e. <span class="math notranslate nohighlight">\([b, b, \cdots, b]\)</span>). So in case this operation seems a little bit mysterious, this is called <strong>broadcasting</strong> in Python.</p>
<ul class="simple">
<li><p>Second, what we would like to do next, is find a way to compute <span class="math notranslate nohighlight">\(a^{(1)}\)</span>, <span class="math notranslate nohighlight">\(a^{(2)}\)</span> and so on to <span class="math notranslate nohighlight">\(a^{(m)}\)</span>, all at the same time, and just as stacking lowercase <span class="math notranslate nohighlight">\(x\)</span>’s resulted in capital <span class="math notranslate nohighlight">\(X\)</span> and stacking horizontally lowercase <span class="math notranslate nohighlight">\(z\)</span>’s resulted in capital <span class="math notranslate nohighlight">\(Z\)</span>, stacking lower case <span class="math notranslate nohighlight">\(a\)</span>, is going to result in a new variable, which we are going to define as capital <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[A = [a^{(1)}, a^{(2)}, \cdots, a^{(m)}] = \sigma(Z)\]</div>
<p>And in the program assignment, you see how to implement a vector valued sigmoid function, so that the sigmoid function, inputs this capital <span class="math notranslate nohighlight">\(Z\)</span> as a variable and very efficiently outputs capital <span class="math notranslate nohighlight">\(A\)</span>. So you see the details of that in the programming assignment.</p>
<p>What we’ve seen in this section is that instead of needing to loop over <span class="math notranslate nohighlight">\(m\)</span> training examples to compute lowercase <span class="math notranslate nohighlight">\(z\)</span> and lowercase <span class="math notranslate nohighlight">\(a\)</span>, one of the time, you can implement this one line of code, to compute all these <span class="math notranslate nohighlight">\(z\)</span>’s at the same time. And then, this one line of code, with appropriate implementation of lowercase Sigma to compute all the lowercase <span class="math notranslate nohighlight">\(a\)</span>’s all at the same time. So this is how you implement a vectorize implementation of the forward propagation for all <span class="math notranslate nohighlight">\(m\)</span> training examples at the same time.</p>
<p>So to summarize, you’ve just seen how you can use vectorization to very efficiently compute all of the activations, all the lowercase <span class="math notranslate nohighlight">\(a\)</span>’s at the same time. Next, it turns out, you can also use vectorization very efficiently to compute the backward propagation, to compute the gradients.</p>
</section>
<section id="vectorizing-logistic-regresion-s-gradient-output">
<h4>Vectorizing Logistic Regresion’s Gradient Output<a class="headerlink" href="#vectorizing-logistic-regresion-s-gradient-output" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/Y5LsQZY-0QM">Video</a></p>
<p>In the previous section, you learned how you can use vectorization to compute their predictions. In this section, you will learn how you can use vectorization to also perform the gradient computations for all <span class="math notranslate nohighlight">\(m\)</span> training samples. Again, all sort of at the same time. And then at the end of this part, we will put it all together and show how you can derive a very efficient implementation of logistic regression.</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}Z = [\mathrm{d}z^{(1)}, \mathrm{d}z^{(2)}, \cdots, \mathrm{d}z^{(m)}] = [a^{(1)} - y^{(1)}, a^{(2)} - y^{(2)}, \cdots, a^{(m)} - y^{(m)}]\]</div>
<p>It’s <span class="math notranslate nohighlight">\(1 \times m\)</span> matrix or alternatively a <span class="math notranslate nohighlight">\(m\)</span> dimensional row vector.</p>
<div class="math notranslate nohighlight">
\[A = [a^{(1)}, a^{(2)}, \cdots, a^{(m)}]\]</div>
<div class="math notranslate nohighlight">
\[Y = [y^{(1)}, y^{(2)}, \cdots, y^{(m)}]\]</div>
<p>So, based on these definitions, maybe you can see for yourself that <span class="math notranslate nohighlight">\(\mathrm{d}Z\)</span> can be computed as just <span class="math notranslate nohighlight">\(A-Y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}Z = A-Y = [a^{(1)} - y^{(1)}, a^{(2)} - y^{(2)}, \cdots, a^{(m)} - y^{(m)}]\]</div>
<p>So, with just one line of code, you can compute all of this at the same time.</p>
<p>Now, in the previous implementation, we’ve gotten rid of one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop already but we still had this second <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over training examples.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}w &amp;= 0 \\
\mathrm{d}w &amp;\ += x^{(1)}\mathrm{d}z^{(1)} \\
\mathrm{d}w &amp;\ += x^{(2)}\mathrm{d}z^{(2)} \\
&amp; \vdots \\
\mathrm{d}w  &amp;\ /= m
\end{aligned} \ \qquad  \qquad 
\begin{aligned}
\mathrm{d}b &amp;= 0 \\
\mathrm{d}b &amp;\ += \mathrm{d}z^{(1)} \\
\mathrm{d}b &amp;\ += \mathrm{d}z^{(2)} \\
&amp; \vdots \\
\mathrm{d}b  &amp;\ /= m
\end{aligned}\end{split}\]</div>
<p>So that’s what we had in the previous implementation. We’d already got rid of one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. So, at least now <code class="docutils literal notranslate"><span class="pre">dw</span></code> is a vector and we went separately updating <code class="docutils literal notranslate"><span class="pre">dw1</span></code>, <code class="docutils literal notranslate"><span class="pre">dw2</span></code> and so on. So, we got rid of that already but we still had the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over the <span class="math notranslate nohighlight">\(m\)</span> examples in the training set. So, let’s take these operations and vectorize them.</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}b = \dfrac{1}{m} \sum_{i=1}^{m} \mathrm{d}z^{(i)}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">)</span>
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}w 
&amp;= \dfrac{1}{m} X (\mathrm{d}Z)^T \\
&amp;= \dfrac{1}{m} \left[
\begin{matrix}
\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \\
\vdots &amp; \vdots &amp; \cdots &amp; \vdots 
\end{matrix}
\right] \left[
\begin{matrix}
\mathrm{d}z^{(1)} \\
\vdots \\
\mathrm{d}z^{(m)}
\end{matrix}
\right] \\
&amp;= \dfrac{1}{m} [x^{(1)}\mathrm{d}z^{(1)} + \cdots + x^{(m)} \mathrm{d}z^{(m)}]
\end{aligned}\end{split}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dw</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dZ</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p>So now, let’s put all together into how you would actually implement logistic regression. This is our original, highly inefficient non vectorize implementation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">m</span> <span class="p">:</span>
	<span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
	<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">dw1</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">dw2</span> <span class="o">+=</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
</pre></div>
</div>
<p>But now, we will see that the whole vectorization process without expilcit two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">dZ</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">Y</span>
<span class="n">dw</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dZ</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">)</span>
</pre></div>
</div>
<p>So, you’ve just done forward propagation and back propagation, really computing the predictions and computing the derivatives on all <span class="math notranslate nohighlight">\(m\)</span> training examples without using a for loop. And so the gradient descent update then would be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dw</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> \<span class="n">alpha</span> <span class="n">dw</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> \<span class="n">alpha</span> <span class="n">db</span>
</pre></div>
</div>
<p>You have just implemented a <strong>single iteration</strong> of gradient descent for logistic regression.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Now, we talked about before we should get rid of explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loops whenever you can. However, if you want to implement multiple iterations as a gradient descent then you still need a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over the number of iterations. So, if you want to have a thousand iterations of gradient descent, you might still need a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over the iteration number. There is an outermost for loop like that then I don’t think there is any way to get rid of that for loop.</p>
<div class="python docutils">
<p>for iter in range(1000):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Z = np.dot(w.T, X) + b
A = sigmoid(Z)
dZ = A - Y
dw = 1/m * np.dot(X, dZ.T)
db = 1/m * np.sum(dZ)

dw = w - \alpha dw
db = b - \alpha db
</pre></div>
</div>
</div>
</div>
<p>So, that’s it you now have a highly vectorize and highly efficient implementation of gradient descent for logistic regression. There is just one more detail that I want to talk about in the section, which is in our description here I briefly alluded to this technique called broadcasting. Broadcasting turns out to be a technique that Python and numpy allows you to use to make certain parts of your code also much more efficient.</p>
</section>
<section id="broadcasting-in-python">
<h4>Broadcasting in Python<a class="headerlink" href="#broadcasting-in-python" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/hnBXZO5JHUc">Video</a></p>
<p>In the previous section, we noticed that broadcasting is another technique that you can use to make your Python code run faster. In this section, let’s delve into how broadcasting in Python actually works. Let’s explore broadcasting with an example.</p>
<p>In this matrix, it shows the number of calories from carbohydrates, proteins, and fats in 100 grams of four different foods:</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/2-4.png"><img alt="_images/2-4.png" src="_images/2-4.png" style="height: 100px;" /></a>
</figure>
<p>So for example, a 100 grams of apples turns out, has 56 calories from carbs, and much less from proteins and fats. Whereas, in contrast, a 100 grams of beef has 104 calories from protein and 135 calories from fat.</p>
<p>Now, let’s say your goal is to calculate the percentage of calories from carbs, proteins and fats for each of the four foods. So, for example, if you look at this column and add up the numbers in that column you get that 100 grams of apple has 56 plus 1.2 plus 1.8 so that’s 59 calories. And so as a percentage the percentage of calories from carbohydrates in an apple would be 56 over 59, that’s about 94.9%. So most of the calories in an apple come from carbs, whereas in contrast, most of the calories of beef come from protein and fat and so on. So the calculation you want is really to sum up each of the four columns of this matrix to get the total number of calories in 100 grams of apples, beef, eggs, and potatoes. And then to divide throughout the matrix, so as to get the percentage of calories from carbs, proteins and fats for each of the four foods. So the question is, can you do this without an explicit for-loop?</p>
<p>What I’m going to do is show you how you can set, say this matrix equal to three by four matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<ul class="simple">
<li><p>And then with one line of Python code we’re going to sum down the columns. So we’re going to get four numbers corresponding to the total number of calories in these four different types of foods, 100 grams of these four different types of foods.</p></li>
<li><p>Using a second line of Python code to divide each of the four columns by their corresponding sum.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">56.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="mf">68.0</span><span class="p">],</span>
	      <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">104.0</span><span class="p">,</span> <span class="mf">52.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
	      <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">99.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
		     
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">cal</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cal</span><span class="p">)</span>

<span class="n">percentage</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">A</span> <span class="o">/</span> <span class="n">cal</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">percentage</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 56.    0.    4.4  68. ]
 [  1.2 104.   52.    8. ]
 [  1.8 135.   99.    0.9]]
[ 59.  239.  155.4  76.9]
[[94.91525424  0.          2.83140283 88.42652796]
 [ 2.03389831 43.51464435 33.46203346 10.40312094]
 [ 3.05084746 56.48535565 63.70656371  1.17035111]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>To add a bit of detail this parameter, <code class="docutils literal notranslate"><span class="pre">(axis</span> <span class="pre">=</span> <span class="pre">0)</span></code>, means that you want Python to sum vertically. So if this is axis 0 this means to sum vertically, where as the horizontal axis is axis 1. So be able to write axis 1 or sum horizontally instead of sum vertically.</p></li>
<li><p>So this is a three by four matrix and you divide it by a one by four matrix. And technically, after this first line of codes <code class="docutils literal notranslate"><span class="pre">cal</span></code>, the variable <code class="docutils literal notranslate"><span class="pre">cal</span></code>, is already a one by four matrix. So technically you don’t need to call reshape here again, so that’s actually a little bit redundant. But when I’m writing Python codes if I’m not entirely sure what matrix, whether the dimensions of a matrix I often would just call a reshape command just to make sure that it’s the right column vector or the row vector or whatever you want it to be. The reshape command is a constant time. It’s a order one operation that’s very cheap to call. So don’t be shy about using the reshape command to make sure that your matrices are the size you need it to be.</p></li>
</ul>
<section id="broadcasting-examples">
<h5>Broadcasting examples<a class="headerlink" href="#broadcasting-examples" title="Link to this heading">#</a></h5>
<p>Now, let’s explain in greater detail how this type of operation works. We had a three by four matrix and we divided it by a one by four matrix. So, how can you divide a three by four matrix by a one by four matrix? Or by one by four vector? Let’s go through a few more examples of broadcasting.</p>
<ol class="arabic simple">
<li><p>If you take a 4 by 1 vector and add it to a number, what Python will do is take this number and auto-expand it into a four by one vector as well, as follows. And so the vector [1, 2, 3, 4] plus the number 100 ends up with that vector on the right. You’re adding a 100 to every element, and in fact we use this form of broadcasting where that constant was the parameter <span class="math notranslate nohighlight">\(b\)</span> in an earlier section. And this type of broadcasting works with both column vectors and row vectors, and in fact we use a similar form of broadcasting earlier with the constant we’re adding to a vector being the parameter <span class="math notranslate nohighlight">\(b\)</span> in logistic regression.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1 \\
2 \\
3 \\
4
\end{matrix}
\right] + 100 = 
\left[
\begin{matrix}
1 \\
2 \\
3 \\
4
\end{matrix}
\right]  + 
\left[
\begin{matrix}
100 \\
100 \\
100 \\
100
\end{matrix}
\right] =
\left[
\begin{matrix}
101 \\
102 \\
103 \\
104
\end{matrix}
\right] \end{split}\]</div>
<ol class="arabic simple" start="2">
<li><p>Here’s another example. Let’s say you have a two by three matrix and you add it to this one by <span class="math notranslate nohighlight">\(n\)</span> matrix. So the general case would be if you have some <span class="math notranslate nohighlight">\((m,n)\)</span> matrix here and you add it to a <span class="math notranslate nohighlight">\((1,n)\)</span> matrix. What Python will do is copy the matrix <span class="math notranslate nohighlight">\(m\)</span> times to turn this into <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix, so instead of this one by three matrix it’ll copy it twice in this example to turn it into this. Also, two by three matrix and we’ll add these so you’ll end up with the sum on the right. So you taken, you added 100 to the first column, added 200 to second column, added 300 to the third column. And this is basically what we did on the previous steps, except that we use a division operation instead of an addition operation.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right] + 
\left[
\begin{matrix}
100 &amp;  200  &amp; 300
\end{matrix}
\right] = 
\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right]  + 
\left[
\begin{matrix}
100 &amp;  200  &amp; 300 \\
100 &amp;  200  &amp; 300
\end{matrix}
\right]   =
\left[
\begin{matrix}
101 &amp;  202  &amp; 303 \\
104 &amp;  205  &amp; 306
\end{matrix}
\right] \end{split}\]</div>
<ol class="arabic simple" start="3">
<li><p>So one last example, whether you have a <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and you add this to a <span class="math notranslate nohighlight">\((m,1)\)</span> vector or <span class="math notranslate nohighlight">\((m,1)\)</span> matrix. Then just copy this <span class="math notranslate nohighlight">\(n\)</span> times horizontally. So you end up with an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix. So as you can imagine you copy it horizontally three times. And you add those. So when you add them you end up with this. So we’ve added 100 to the first row and added 200 to the second row.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right] + 
\left[
\begin{matrix}
100 \\ 
200
\end{matrix}
\right] = 
\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right]  + 
\left[
\begin{matrix}
100 &amp;  100  &amp; 100 \\
200 &amp;  200  &amp; 200
\end{matrix}
\right]   =
\left[
\begin{matrix}
101 &amp;  102  &amp; 103 \\
204 &amp;  205  &amp; 206
\end{matrix}
\right] \end{split}\]</div>
</section>
<section id="general-principle">
<h5>General Principle<a class="headerlink" href="#general-principle" title="Link to this heading">#</a></h5>
<ol class="arabic">
<li><p>Here’s the more general principle of broadcasting in Python. If you have an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and you add or subtract or multiply or divide with a <span class="math notranslate nohighlight">\((1,n)\)</span> matrix, then this will copy it <span class="math notranslate nohighlight">\(m\)</span> times into an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix. And then apply the addition, subtraction, and multiplication of division element wise.</p>
<p>If conversely, you were to take the <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and add, subtract, multiply, divide by an <span class="math notranslate nohighlight">\((m,1)\)</span> matrix, then also this would copy it now <span class="math notranslate nohighlight">\(n\)</span> times. And turn that into an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and then apply the operation element wise.</p>
</li>
<li><p>Just one of the broadcasting, which is if you have an <span class="math notranslate nohighlight">\((m,1)\)</span> matrix, so that’s really a column vector like <span class="math notranslate nohighlight">\([1,2,3]^T\)</span>, and you add, subtract, multiply or divide by a row number. So maybe a <span class="math notranslate nohighlight">\((1,1)\)</span> matrix. So such as that plus 100, then you end up copying this real number <span class="math notranslate nohighlight">\(m\)</span> times until you’ll also get another <span class="math notranslate nohighlight">\((m,1)\)</span> matrix. And then you perform the operation such as addition on this example element-wise. And something similar also works for row vectors.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1 \\
2 \\
3 
\end{matrix}
\right] + 100 = 
\left[
\begin{matrix}
101 \\
102 \\
103 
\end{matrix}
\right] \end{split}\]</div>
<div class="math notranslate nohighlight">
\[\left[
\begin{matrix}
1  &amp;  2  &amp; 3 
\end{matrix}
\right] + 100 = 
\left[
\begin{matrix}
101 &amp;  102  &amp; 103 
\end{matrix}
\right] \]</div>
<p>So, that was broadcasting in Python. I hope that when you do the programming homework that broadcasting will allow you to not only make a code run faster, but also help you get what you want done with fewer lines of code.</p>
</section>
</section>
<section id="a-note-on-python-numpy-vectors">
<h4>A Note on Python/Numpy Vectors<a class="headerlink" href="#a-note-on-python-numpy-vectors" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/Yx8LWTEKaxg">Video</a></p>
<p><a class="reference external" href="https://youtu.be/TPRB5n9ckQs">Quick tour of Jupyter/iPython Notebooks</a></p>
<p>The ability of python to allow you to use broadcasting operations and more generally, the great flexibility of the python numpy program language is both a strength as well as a weakness of the programming language.</p>
<ul class="simple">
<li><p>The strength because they create expressivity of the language. A great flexibility of the language lets you get a lot done even with just a single line of code.</p></li>
<li><p>But there’s also weakness because with broadcasting and this great amount of flexibility, sometimes it’s possible you can introduce very subtle bugs or very strange looking bugs, if you’re not familiar with all of the intricacies of how broadcasting and how features like broadcasting work.</p>
<ul>
<li><p>For example, if you take a column vector and add it to a row vector, you would expect it to throw up a dimension mismatch or type error or something. But you might actually get back a matrix as a sum of a row vector and a column vector.</p></li>
</ul>
</li>
</ul>
<p>There is an internal logic to these strange effects of Python. But if you’re not familiar with Python, I’ve seen some students have very strange, very hard to find bugs. So in this section is share with you some couple tips and tricks that have been very useful for me to eliminate or simplify and eliminate all the strange looking bugs in my own code. And I hope that with these tips and tricks, you’ll also be able to much more easily write bug-free, python and numpy code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.03550454 0.62634242 0.01162295 0.47968223 0.952829  ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5,)
</pre></div>
</div>
</div>
</div>
<p>From the coding results, the shape of <code class="docutils literal notranslate"><span class="pre">a</span></code> is <span class="math notranslate nohighlight">\((5,\ )\)</span> structure. This is called a rank 1 array in Python and it’s neither a row vector nor a column vector. And this leads it to have some slightly non-intuitive effects.  For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.03550454 0.62634242 0.01162295 0.47968223 0.952829  ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.53167863641126
</pre></div>
</div>
</div>
</div>
<p>If I print <code class="docutils literal notranslate"><span class="pre">a</span></code> transpose, it ends up looking the same as <code class="docutils literal notranslate"><span class="pre">a</span></code>. So <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">a.T</span></code> end up looking the same. And if I print the inner product between <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">a.T</span></code>, you might think <code class="docutils literal notranslate"><span class="pre">a</span></code> times <code class="docutils literal notranslate"><span class="pre">a.T</span></code> is maybe the outer product should give you matrix maybe. But the result above shows that, you instead get back a number. So what I would recommend is that when you’re coding new networks, that you just not use data structures where the shape is <span class="math notranslate nohighlight">\((5,)\)</span> or <span class="math notranslate nohighlight">\((n,)\)</span> - a rank 1 array. Instead, if you set <code class="docutils literal notranslate"><span class="pre">a</span></code> to be (5,1) like below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.60765632]
 [0.1305282 ]
 [0.26909222]
 [0.6254817 ]
 [0.41021334]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.60765632 0.1305282  0.26909222 0.6254817  0.41021334]]
</pre></div>
</div>
</div>
</div>
<p>Then this commits a to be <span class="math notranslate nohighlight">\((5,1)\)</span> column vector. Now <code class="docutils literal notranslate"><span class="pre">a</span></code> transpose is a row vector. Notice one subtle difference. In this data structure, there are two square brackets when we print <code class="docutils literal notranslate"><span class="pre">a.T</span></code>. Whereas previously, there was one square bracket. So that’s the difference between this is really a 1 by 5 matrix versus one of these rank 1 arrays.</p>
<p>And if you print the product between <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">a.T</span></code>, then this gives you the outer product of a vector:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.36924621 0.07931628 0.16351559 0.38007791 0.24926873]
 [0.07931628 0.01703761 0.03512412 0.081643   0.05354441]
 [0.16351559 0.03512412 0.07241063 0.16831226 0.11038522]
 [0.38007791 0.081643   0.16831226 0.39122736 0.25658094]
 [0.24926873 0.05354441 0.11038522 0.25658094 0.16827498]]
</pre></div>
</div>
</div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The first command that we ran, just now. And this created a data structure with <code class="docutils literal notranslate"><span class="pre">a.shape</span></code> was this funny thing <span class="math notranslate nohighlight">\((5,)\)</span> so this is called a rank 1 array. And this is a very funny data structure. It doesn’t behave consistently as either a row vector nor a column vector, which makes some of its effects nonintuitive. So what I’m going to recommend is that when you’re doing your programing exercises, or in fact when you’re implementing logistic regression or neural networks that you just do not use these rank 1 arrays.</p>
<p>Instead, if every time you create an array, you commit to making it either a column vector, so this creates a <span class="math notranslate nohighlight">\((5,1)\)</span> vector, or commit to making it a row vector, then the behavior of your vectors may be easier to understand.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>   <span class="c1">#DO NOT USE</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>One more thing that I do a lot in my code is if I’m not entirely sure what’s the dimension of one of my vectors, I’ll often throw in an assertion statement like this,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>to make sure, in this case, that this is a <span class="math notranslate nohighlight">\((5,1)\)</span> vector. So this is a column vector. These assertions are really inexpensive to execute, and they also help to serve as documentation for your code. So don’t hesitate to throw in assertion statements like this whenever you feel like.</p>
<p>And then finally, if for some reason you do end up with a rank 1 array, You can reshape this.</p>
</section>
</section>
<section id="quiz">
<h3>Quiz<a class="headerlink" href="#quiz" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>In logistic regression given <span class="math notranslate nohighlight">\(x\)</span>, and parameters <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}, \ b \in \mathbb{R}\)</span>. How do we generate the output <span class="math notranslate nohighlight">\(\hat{y}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(\sigma(wx)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\text{tanh}(wx+b)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\sigma(wx + b)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(wx + b\)</span></p>
</li>
<li><p>In logistic regression given the input <span class="math notranslate nohighlight">\(x\)</span> and parameters <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}, \ b \in \mathbb{R}\)</span>. Which of the following best expresses what we want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to tell us?</p>
<p>A. <span class="math notranslate nohighlight">\(\sigma(wx)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\mathrm{P}(y=1 \mid x)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\mathrm{P}(y=\hat{y} \mid x)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\sigma(wx + b)\)</span></p>
</li>
<li><p>Which of these is the “Logistic Loss”?</p>
<p>A. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \lvert y^{(i)} - \hat{y}^{(i)} \rvert\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \text{max}(0, y^{(i)} - \hat{y}^{(i)})\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = - \Big( y^{(i)} \  \text{log}(\hat{y}^{(i)}) + (1-y^{(i)}) \ \text{log}(1-\hat{y}^{(i)}) \Big)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \lvert y^{(i)} - \hat{y}^{(i)} \rvert^{2}\)</span></p>
</li>
<li><p>Suppose that <span class="math notranslate nohighlight">\(\hat{y} = 0.5\)</span> and <span class="math notranslate nohighlight">\(y = 0\)</span>. What is the value of the “Logistic Loss”? Choose the best option.</p>
<p>A. <span class="math notranslate nohighlight">\(+ \infty\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(0.5\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(0.693\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y) = - \Big( y \  \text{log}(\hat{y}) + (1-y) \ \text{log}(1-\hat{y}) \Big)\)</span></p>
</li>
<li><p>Suppose <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a <span class="math notranslate nohighlight">\((8, 1)\)</span> array. Which of the following is a valid reshape?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">x.reshape(1,</span> <span class="pre">4,</span> <span class="pre">3)</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">x.reshape(2,</span> <span class="pre">2,</span> <span class="pre">2)</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">x.reshape(-1,</span> <span class="pre">3)</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">x.reshape(2,</span> <span class="pre">4,</span> <span class="pre">4)</span></code></p>
</li>
<li><p>Conside the Numpy array <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>: <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">np.array([[[1],[2]],[[3],[4]]])</span></code>. What is the shape of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((4,)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((1, 2, 2)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((2, 2, 1)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((2, 2)\)</span></p>
</li>
<li><p>Consider the following random arrays <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># a.shape = (3, 4)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># a.shape = (1, 4)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be the shape of <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. The computation cannot happen because it is not possible to broadcast more than one dimension.</p>
<p>B. c.shape = <span class="math notranslate nohighlight">\((3, 1)\)</span></p>
<p>C. c.shape = <span class="math notranslate nohighlight">\((1, 4)\)</span></p>
<p>D. c.shape = <span class="math notranslate nohighlight">\((3, 4)\)</span></p>
</li>
<li><p>Consider the following random arrays <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># a.shape = (2, 3)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># a.shape = (2, 1)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be the shape of <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. The computation cannot happen because the sizes do not match. It’s going to be “Error”!</p>
<p>B. c.shape = <span class="math notranslate nohighlight">\((2, 1)\)</span></p>
<p>C. c.shape = <span class="math notranslate nohighlight">\((2, 3)\)</span></p>
<p>D. c.shape = <span class="math notranslate nohighlight">\((3, 2)\)</span></p>
</li>
<li><p>Consider the following random arrays <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># a.shape = (4, 3)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># a.shape = (1, 3)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be the shape of <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. The computation cannot happen because the sizes do not match.</p>
<p>B. c.shape = <span class="math notranslate nohighlight">\((1, 3)\)</span></p>
<p>C. The computation cannot happen because it is not possible to broadcast more than one dimension.</p>
<p>D. c.shape = <span class="math notranslate nohighlight">\((4, 3)\)</span></p>
</li>
<li><p>Suppose you have <span class="math notranslate nohighlight">\(n_x\)</span> onput features per example. Recall that <span class="math notranslate nohighlight">\(\boldsymbol{X} = [x^{(1)} \  x^{(2)} \  \cdots \  x^{(m)}]\)</span>. What is the dimension of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((m, 1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((m, n_x)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((1, m)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((n_x, m)\)</span></p>
</li>
<li><p>Suppose our input batch consists of 8 grayscale images, each of dimension <span class="math notranslate nohighlight">\(8\times8\)</span>. We reshape these images into feature column vectors <span class="math notranslate nohighlight">\(x^{j}\)</span>. Remember that <span class="math notranslate nohighlight">\(\boldsymbol{X} = [x^{(1)} \  x^{(2)} \  \cdots \  x^{(8)}]\)</span>. What is the dimension of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((512, 1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((8, 64)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((64, 8)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((8, 8, 8)\)</span></p>
</li>
<li><p>Consider the following array:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
</pre></div>
</div>
<p>What is the result of <code class="docutils literal notranslate"><span class="pre">a*a</span></code>?</p>
<p>A. The computation cannot happen because the sizes do not match. It’s going to be “Error”!</p>
<p>B. <span class="math notranslate nohighlight">\(\begin{pmatrix} 4 &amp; 2 \\ 6 &amp;6\\ \end{pmatrix}\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\begin{pmatrix} 4 &amp; 1 \\ 1 &amp;9\\ \end{pmatrix}\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\begin{pmatrix} 5 &amp; 5 \\ 5 &amp;10\\ \end{pmatrix}\)</span></p>
</li>
<li><p>Consider the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</pre></div>
</div>
<p>How do you vectorize this?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">*</span> <span class="pre">b</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b.T</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">np.dot(a,</span> <span class="pre">b)</span></code></p>
</li>
<li><p>Consider the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</pre></div>
</div>
<p>How do you vectorize this?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">b.T</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">+</span> <span class="pre">b</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">+</span> <span class="pre">b.T</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code></p>
</li>
<li><p>Consider the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> 
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. It will lead to an error since you cannot use <code class="docutils literal notranslate"><span class="pre">*</span></code> to operate on these two matrices. You need to instead use <code class="docutils literal notranslate"><span class="pre">np.dot(a,</span> <span class="pre">b)</span></code>.</p>
<p>B. This will multiply a <span class="math notranslate nohighlight">\(3\times3\)</span> matrix a with a <span class="math notranslate nohighlight">\(3\times1\)</span> vector, thus resulting in a <span class="math notranslate nohighlight">\(3\times1\)</span> vector. That is, c.shape = <span class="math notranslate nohighlight">\((3, 1)\)</span>.</p>
<p>C. This will invoke broadcasting, so <code class="docutils literal notranslate"><span class="pre">b</span></code> is copied three times to become <span class="math notranslate nohighlight">\((3, 3)\)</span>, and <code class="docutils literal notranslate"><span class="pre">*</span></code> is an element-wise product so c.shape will be <span class="math notranslate nohighlight">\((3, 3)\)</span>.</p>
<p>D. This will invoke broadcasting, so <code class="docutils literal notranslate"><span class="pre">b</span></code> is copied three times to become <span class="math notranslate nohighlight">\((3, 3)\)</span>, and <code class="docutils literal notranslate"><span class="pre">*</span></code> invokes a matrix multiplication operation of two <span class="math notranslate nohighlight">\(3\times3\)</span> matrices so c.shape will be <span class="math notranslate nohighlight">\((3, 3)\)</span>.</p>
</li>
<li><p>Consider the code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> 
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">T</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Which of the following gives an equivalent output for <span class="math notranslate nohighlight">\(c\)</span>?</p>
</li>
</ol>
<p>A.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>B.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>C.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>D. The computation cannot happen because the sizes do not match. It’s going to be “Error”!</p>
<ol class="arabic" start="17">
<li><p>Consider the following computational graph:</p>
<figure class="align-default" id="e17">
<a class="reference internal image-reference" href="_images/2-e17.png"><img alt="_images/2-e17.png" src="_images/2-e17.png" style="height: 200px;" /></a>
</figure>
<p>What is the output of <span class="math notranslate nohighlight">\(J\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((a + c)(b - 1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(ab + bc + ac\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((c - 1)(a + c)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((a - 1)(b + c)\)</span></p>
</li>
<li><p>Consider the following computational graph:</p>
<figure class="align-default" id="e18">
<a class="reference internal image-reference" href="_images/2-e18.png"><img alt="_images/2-e18.png" src="_images/2-e18.png" style="height: 200px;" /></a>
</figure>
<p>What is the output of <span class="math notranslate nohighlight">\(J\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(J = a\times b + b \times c + a \times c \)</span></p>
<p>B. <span class="math notranslate nohighlight">\(J = (c - 1)(b + a)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(J = (a - 1)(b + c)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(J = (b - 1)(c + a)\)</span></p>
</li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Click here for answers!</p>
<ol class="arabic">
<li><p>C </br></p>
<p>B. Yes. in logisitc regression we use a linear function <span class="math notranslate nohighlight">\(wx + b\)</span> followed by the sigmoid function <span class="math notranslate nohighlight">\(\sigma\)</span>, to get an output <span class="math notranslate nohighlight">\(y\)</span>, referred to as <span class="math notranslate nohighlight">\(\hat{y}\)</span>, such that <span class="math notranslate nohighlight">\(0 &lt; \hat{y} &lt; 1\)</span>. </br></p>
</li>
<li><p>B </br></p>
<p>B. Yes. We want the output <span class="math notranslate nohighlight">\(\hat{y}\)</span> to tell us the probability that <span class="math notranslate nohighlight">\(y=1\)</span> given <span class="math notranslate nohighlight">\(x\)</span>. </br></p>
</li>
<li><p>C </br></p></li>
<li><p>C </br></p></li>
<li><p>B </br></p>
<p>B. Yes. This generates uses <span class="math notranslate nohighlight">\(2\times2\times2 = 8\)</span> entries. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. This array has two rows and in each row it has 2 arrays of <span class="math notranslate nohighlight">\(1\times1\)</span>. </br></p>
</li>
<li><p>D </br></p>
<p>D. Yes. Broadcasting is used, so row <code class="docutils literal notranslate"><span class="pre">b</span></code> is copied 3 times so it can be summed to each row of <code class="docutils literal notranslate"><span class="pre">a</span></code>. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. This is broadcasting.  <code class="docutils literal notranslate"><span class="pre">b</span></code> (column vector) is copied 3 times so that it can be summed to each column of <code class="docutils literal notranslate"><span class="pre">a</span></code>. </br></p>
</li>
<li><p>D </br></p>
<p>D. Yes. Broadcasting is invoked, so row <code class="docutils literal notranslate"><span class="pre">b</span></code> is multiplied element-wise each row of <code class="docutils literal notranslate"><span class="pre">a</span></code> to create <code class="docutils literal notranslate"><span class="pre">c</span></code>. </br></p>
</li>
<li><p>D </br></p></li>
<li><p>C </br></p>
<p>C. Yes. After converting the <span class="math notranslate nohighlight">\(8\times8\)</span> gray scale images to a column vector we get a vector of size 64, thus <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> has dimension <span class="math notranslate nohighlight">\((64, 8)\)</span>. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. Recall that * indicates element-wise multiplication. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. <code class="docutils literal notranslate"><span class="pre">b.T</span></code> gives a column vector with shape <span class="math notranslate nohighlight">\((1, 4)\)</span>. The result of <code class="docutils literal notranslate"><span class="pre">c</span></code> is equivalent to broadcasting <code class="docutils literal notranslate"><span class="pre">a*b.T</span></code>. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. <code class="docutils literal notranslate"><span class="pre">a[j][i]</span></code> being used fo <code class="docutils literal notranslate"><span class="pre">a[i][j]</span></code> indicates we are using <code class="docutils literal notranslate"><span class="pre">a.T</span></code> and the element in the row j is used in the column j thus we are using <code class="docutils literal notranslate"><span class="pre">b.T</span></code>. </br></p>
</li>
<li><p>C </br></p></li>
<li><p>C </br></p>
<p>C. Yes. Notice that to operate with <code class="docutils literal notranslate"><span class="pre">b.T</span></code> we need to use <code class="docutils literal notranslate"><span class="pre">b[j][i]</span></code>. </br></p>
</li>
<li><p>A </br></p></li>
<li><p>C </br></p></li>
</ol>
</div>
</section>
<div class="toctree-wrapper compound">
<span id="document-C2_Practical"></span><section class="tex2jax_ignore mathjax_ignore" id="pre-practical-python-basics-with-numpy-optional-assignment">
<h3>Pre-Practical: Python Basics with Numpy (optional assignment)<a class="headerlink" href="#pre-practical-python-basics-with-numpy-optional-assignment" title="Link to this heading">#</a></h3>
<p>Welcome to your first assignment. This exercise gives you a brief introduction to Python. Even if you’ve used Python before, this will help familiarize you with the functions we’ll need.</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>You will be using Python 3.</p></li>
<li><p>Avoid using for-loops and while-loops, unless you are explicitly told to do so.</p></li>
<li><p>After coding your function, run the cell right below it to check if your result is correct.</p></li>
</ul>
<p><strong>After this assignment you will:</strong></p>
<ul class="simple">
<li><p>Be able to use iPython Notebooks</p></li>
<li><p>Be able to use numpy functions and numpy matrix/vector operations</p></li>
<li><p>Understand the concept of “broadcasting”</p></li>
<li><p>Be able to vectorize code</p></li>
</ul>
<p>Let’s get started!</p>
<section id="important-note-on-submission-to-the-autograder">
<h4>Important Note on Submission to the AutoGrader<a class="headerlink" href="#important-note-on-submission-to-the-autograder" title="Link to this heading">#</a></h4>
<p>Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:</p>
<ol class="arabic simple">
<li><p>You have not added any <em>extra</em> <code class="docutils literal notranslate"><span class="pre">print</span></code> statement(s) in the assignment.</p></li>
<li><p>You have not added any <em>extra</em> code cell(s) in the assignment.</p></li>
<li><p>You have not changed any of the function parameters.</p></li>
<li><p>You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.</p></li>
<li><p>You are not changing the assignment code where it is not required, like creating <em>extra</em> variables.</p></li>
</ol>
<p>If you do any of the following, you will get something like, <code class="docutils literal notranslate"><span class="pre">Grader</span> <span class="pre">Error:</span> <span class="pre">Grader</span> <span class="pre">feedback</span> <span class="pre">not</span> <span class="pre">found</span></code> (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these <a class="reference external" href="https://www.coursera.org/learn/neural-networks-deep-learning/supplement/iLwon/h-ow-to-refresh-your-workspace">instructions</a>.</p>
</section>
<section id="about-ipython-notebooks">
<h4>About iPython Notebooks<a class="headerlink" href="#about-ipython-notebooks" title="Link to this heading">#</a></h4>
<p>iPython Notebooks are interactive coding environments embedded in a webpage. You will be using iPython notebooks in this class. You only need to write code between the # your code here comment. After writing your code, you can run the cell by either pressing “SHIFT”+”ENTER” or by clicking on “Run Cell” (denoted by a play symbol) in the upper bar of the notebook.</p>
<p>We will often specify “(≈ X lines of code)” in the comments to tell you about how much code you need to write. It is just a rough estimate, so don’t feel bad if your code is longer or shorter.</p>
<section id="exercise-1">
<h5>Exercise 1<a class="headerlink" href="#exercise-1" title="Link to this heading">#</a></h5>
<p>Set test to <code class="docutils literal notranslate"><span class="pre">&quot;Hello</span> <span class="pre">World&quot;</span></code> in the cell below to print “Hello World” and run the two cells below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># (≈ 1 line of code)</span>
<span class="c1"># test = </span>
<span class="c1"># YOUR CODE STARTS HERE</span>
<span class="n">test</span> <span class="o">=</span> <span class="s1">&#39;Hello World&#39;</span>

<span class="c1"># YOUR CODE ENDS HERE</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test: &quot;</span> <span class="o">+</span> <span class="n">test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>test: Hello World
</pre></div>
</div>
</div>
</div>
<p><strong>Expected output</strong>:</p>
<p>test: Hello World</p>
<p><strong>What you need to remember</strong>:</p>
<ul class="simple">
<li><p>Run your cells using SHIFT+ENTER (or “Run cell”)</p></li>
<li><p>Write code in the designated areas using Python 3 only</p></li>
<li><p>Do not modify the code outside of the designated areas</p></li>
</ul>
</section>
</section>
<section id="building-basic-functions-with-numpy">
<h4>1 - Building basic functions with numpy<a class="headerlink" href="#building-basic-functions-with-numpy" title="Link to this heading">#</a></h4>
<p>Numpy is the main package for scientific computing in Python. It is maintained by a large community (<a class="reference external" href="http://www.numpy.org">www.numpy.org</a>). In this exercise you will learn several key numpy functions such as <code class="docutils literal notranslate"><span class="pre">np.exp</span></code>, <code class="docutils literal notranslate"><span class="pre">np.log</span></code>, and <code class="docutils literal notranslate"><span class="pre">np.reshape</span></code>. You will need to know how to use these functions for future assignments.</p>
<section id="sigmoid-function-np-exp">
<h5>1.1 - sigmoid function, np.exp()<a class="headerlink" href="#sigmoid-function-np-exp" title="Link to this heading">#</a></h5>
<p>Before using <code class="docutils literal notranslate"><span class="pre">np.exp()</span></code>, you will use <code class="docutils literal notranslate"><span class="pre">math.exp()</span></code> to implement the sigmoid function. You will then see why <code class="docutils literal notranslate"><span class="pre">np.exp()</span></code> is preferable to <code class="docutils literal notranslate"><span class="pre">math.exp()</span></code>.</p>
</section>
<section id="exercise-2-basic-sigmoid">
<h5>Exercise 2 - basic_sigmoid<a class="headerlink" href="#exercise-2-basic-sigmoid" title="Link to this heading">#</a></h5>
<p>Build a function that returns the sigmoid of a real number x. Use <code class="docutils literal notranslate"><span class="pre">math.exp(x)</span></code> for the exponential function.</p>
<p><strong>Reminder</strong>:
<span class="math notranslate nohighlight">\(\text{sigmoid}(x) = \frac{1}{1+e^{-x}}\)</span> is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning.</p>
<figure class="align-default" id="pra-1">
<a class="reference internal image-reference" href="_images/Sigmoid.png"><img alt="_images/Sigmoid.png" src="_images/Sigmoid.png" style="width: 500px; height: 230px;" /></a>
</figure>
<p>To refer to a function belonging to a specific package you could call it using <code class="docutils literal notranslate"><span class="pre">package_name.function()</span></code>. Run the code below to see an example with <code class="docutils literal notranslate"><span class="pre">math.exp()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">from</span> <span class="nn">public_tests_c1w2</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># GRADED FUNCTION: basic_sigmoid</span>

<span class="k">def</span> <span class="nf">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute sigmoid of x.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    x -- A scalar</span>

<span class="sd">    Return:</span>
<span class="sd">    s -- sigmoid(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># (≈ 1 line of code)</span>
    <span class="c1"># s = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">s</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;basic_sigmoid(1) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">basic_sigmoid</span><span class="p">(</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">basic_sigmoid_test</span><span class="p">(</span><span class="n">basic_sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>basic_sigmoid(1) = 0.7310585786300049
 All tests passed.
</pre></div>
</div>
</div>
</div>
<p>Actually, we rarely use the “math” library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### One reason why we use &quot;numpy&quot; instead of &quot;math&quot; in Deep Learning ###</span>

<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="c1"># x becomes a python list object</span>
<span class="n">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># you will see this give an error when you run it, because x is a vector.</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">TypeError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">5</span><span class="p">],</span> <span class="n">line</span> <span class="mi">4</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1">### One reason why we use &quot;numpy&quot; instead of &quot;math&quot; in Deep Learning ###</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="c1"># x becomes a python list object</span>
<span class="ne">----&gt; </span><span class="mi">4</span> <span class="n">basic_sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nn">Cell In[3], line 19,</span> in <span class="ni">basic_sigmoid</span><span class="nt">(x)</span>
<span class="g g-Whitespace">      </span><span class="mi">7</span><span class="w"> </span><span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">      </span><span class="mi">8</span><span class="sd"> Compute sigmoid of x.</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span><span class="sd"> </span>
<span class="sd">   (...)</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span><span class="sd"> s -- sigmoid(x)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> <span class="c1"># (≈ 1 line of code)</span>
<span class="g g-Whitespace">     </span><span class="mi">17</span> <span class="c1"># s = </span>
<span class="g g-Whitespace">     </span><span class="mi">18</span> <span class="c1"># YOUR CODE STARTS HERE</span>
<span class="ne">---&gt; </span><span class="mi">19</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span> <span class="c1"># YOUR CODE ENDS HERE</span>
<span class="g g-Whitespace">     </span><span class="mi">23</span> <span class="k">return</span> <span class="n">s</span>

<span class="ne">TypeError</span>: bad operand type for unary -: &#39;list&#39;
</pre></div>
</div>
</div>
</div>
<p>In fact, if <span class="math notranslate nohighlight">\( x = (x_1, x_2, ..., x_n)\)</span> is a row vector then <code class="docutils literal notranslate"><span class="pre">np.exp(x)</span></code> will apply the exponential function to every element of x. The output will thus be: <code class="docutils literal notranslate"><span class="pre">np.exp(x)</span> <span class="pre">=</span> <span class="pre">(e^{x_1},</span> <span class="pre">e^{x_2},</span> <span class="pre">...,</span> <span class="pre">e^{x_n})</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># example of np.exp</span>
<span class="n">t_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">t_x</span><span class="p">))</span> <span class="c1"># result is (exp(1), exp(2), exp(3))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 2.71828183  7.3890561  20.08553692]
</pre></div>
</div>
</div>
</div>
<p>Furthermore, if x is a vector, then a Python operation such as <span class="math notranslate nohighlight">\(s = x + 3\)</span> or <span class="math notranslate nohighlight">\(s = \frac{1}{x}\)</span> will output s as a vector of the same size as x.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># example of vector operation</span>
<span class="n">t_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">t_x</span> <span class="o">+</span> <span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[4 5 6]
</pre></div>
</div>
</div>
</div>
<p>Any time you need more info on a numpy function, we encourage you to look at <a class="reference external" href="https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html">the official documentation</a>.</p>
<p>You can also create a new cell in the notebook and write <code class="docutils literal notranslate"><span class="pre">np.exp?</span></code> (for example) to get quick access to the documentation.</p>
</section>
<section id="exercise-3-sigmoid">
<h5>Exercise 3 - sigmoid<a class="headerlink" href="#exercise-3-sigmoid" title="Link to this heading">#</a></h5>
<p>Implement the sigmoid function using numpy.</p>
<p><strong>Instructions</strong>: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices…) are called numpy arrays. You don’t need to know more for now.</p>
<div class="math notranslate nohighlight">
\[\begin{split} \text{For } x \in \mathbb{R}^n \text{,     } \text{sigmoid}(x) = \text{sigmoid}\begin{pmatrix}
    x_1  \\
    x_2  \\
    ...  \\
    x_n  \\
\end{pmatrix} = \begin{pmatrix}
    \frac{1}{1+e^{-x_1}}  \\
    \frac{1}{1+e^{-x_2}}  \\
    ...  \\
    \frac{1}{1+e^{-x_n}}  \\
\end{pmatrix}\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: sigmoid</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of x</span>

<span class="sd">    Arguments:</span>
<span class="sd">    x -- A scalar or numpy array of any size</span>

<span class="sd">    Return:</span>
<span class="sd">    s -- sigmoid(x)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># (≈ 1 line of code)</span>
    <span class="c1"># s = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">s</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;sigmoid(t_x) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">t_x</span><span class="p">)))</span>

<span class="n">sigmoid_test</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sigmoid(t_x) = [0.73105858 0.88079708 0.95257413]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="sigmoid-gradient">
<h5>1.2 - Sigmoid Gradient<a class="headerlink" href="#sigmoid-gradient" title="Link to this heading">#</a></h5>
<p>As you’ve seen in lecture, you will need to compute gradients to optimize loss functions using backpropagation. Let’s code your first gradient function.</p>
</section>
<section id="exercise-4-sigmoid-derivative">
<h5>Exercise 4 - sigmoid_derivative<a class="headerlink" href="#exercise-4-sigmoid-derivative" title="Link to this heading">#</a></h5>
<p>Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input x. The formula is:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}\_\text{derivative}(x) = \sigma'(x) = \sigma(x) (1 - \sigma(x))\tag{2}\]</div>
<p>You often code this function in two steps:</p>
<ol class="arabic simple">
<li><p>Set s to be the sigmoid of x. You might find your sigmoid(x) function useful.</p></li>
<li><p>Compute <span class="math notranslate nohighlight">\(\sigma'(x) = s(1-s)\)</span></p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: sigmoid_derivative</span>

<span class="k">def</span> <span class="nf">sigmoid_derivative</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x.</span>
<span class="sd">    You can store the output of the sigmoid function into variables and then use it to calculate the gradient.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    x -- A scalar or numpy array</span>

<span class="sd">    Return:</span>
<span class="sd">    ds -- Your computed gradient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># s = </span>
    <span class="c1"># ds = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
    <span class="n">ds</span> <span class="o">=</span> <span class="n">s</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">s</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">ds</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid_derivative(t_x) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid_derivative</span><span class="p">(</span><span class="n">t_x</span><span class="p">)))</span>

<span class="n">sigmoid_derivative_test</span><span class="p">(</span><span class="n">sigmoid_derivative</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sigmoid_derivative(t_x) = [0.19661193 0.10499359 0.04517666]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="reshaping-arrays">
<h5>1.3 - Reshaping arrays<a class="headerlink" href="#reshaping-arrays" title="Link to this heading">#</a></h5>
<p>Two common numpy functions used in deep learning are <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html">np.shape</a> and <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html">np.reshape()</a>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">X.shape</span></code> is used to get the shape (dimension) of a matrix/vector X.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">X.reshape(...)</span></code> is used to reshape X into some other dimension.</p></li>
</ul>
<p>For example, in computer science, an image is represented by a 3D array of shape <span class="math notranslate nohighlight">\((\text{length}, \text{height}, \text{depth} = 3)\)</span>. However, when you read an image as the input of an algorithm you convert it to a vector of shape <span class="math notranslate nohighlight">\((\text{length} \times \text{height} \times 3, 1)\)</span>. In other words, you “unroll”, or reshape, the 3D array into a 1D vector.</p>
<figure class="align-default" id="practice-1">
<a class="reference internal image-reference" href="_images/image2vector_kiank.png"><img alt="_images/image2vector_kiank.png" src="_images/image2vector_kiank.png" style="width: 500px; height: 300px;" /></a>
</figure>
</section>
<section id="exercise-5-image2vector">
<h5>Exercise 5 - image2vector<a class="headerlink" href="#exercise-5-image2vector" title="Link to this heading">#</a></h5>
<p>Implement <code class="docutils literal notranslate"><span class="pre">image2vector()</span></code> that takes an input of shape (length, height, 3) and returns a vector of shape (length*height*3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span> <span class="c1"># v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Please don’t hardcode the dimensions of image as a constant. Instead look up the quantities you need with <code class="docutils literal notranslate"><span class="pre">image.shape[0]</span></code>, etc.</p></li>
<li><p>You can use v = v.reshape(-1, 1). Just make sure you understand why it works.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION:image2vector</span>

<span class="k">def</span> <span class="nf">image2vector</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Argument:</span>
<span class="sd">    image -- a numpy array of shape (length, height, depth)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    v -- a vector of shape (length*height*depth, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># (≈ 1 line of code)</span>
    <span class="c1"># v =</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">v</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is a 3 by 3 by 2 array, typically images will be (num_px_x, num_px_y,3) where 3 represents the RGB values</span>
<span class="n">t_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[[</span> <span class="mf">0.67826139</span><span class="p">,</span>  <span class="mf">0.29380381</span><span class="p">],</span>
                     <span class="p">[</span> <span class="mf">0.90714982</span><span class="p">,</span>  <span class="mf">0.52835647</span><span class="p">],</span>
                     <span class="p">[</span> <span class="mf">0.4215251</span> <span class="p">,</span>  <span class="mf">0.45017551</span><span class="p">]],</span>

                   <span class="p">[[</span> <span class="mf">0.92814219</span><span class="p">,</span>  <span class="mf">0.96677647</span><span class="p">],</span>
                    <span class="p">[</span> <span class="mf">0.85304703</span><span class="p">,</span>  <span class="mf">0.52351845</span><span class="p">],</span>
                    <span class="p">[</span> <span class="mf">0.19981397</span><span class="p">,</span>  <span class="mf">0.27417313</span><span class="p">]],</span>

                   <span class="p">[[</span> <span class="mf">0.60659855</span><span class="p">,</span>  <span class="mf">0.00533165</span><span class="p">],</span>
                    <span class="p">[</span> <span class="mf">0.10820313</span><span class="p">,</span>  <span class="mf">0.49978937</span><span class="p">],</span>
                    <span class="p">[</span> <span class="mf">0.34144279</span><span class="p">,</span>  <span class="mf">0.94630077</span><span class="p">]]])</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;image2vector(image) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">image2vector</span><span class="p">(</span><span class="n">t_image</span><span class="p">)))</span>

<span class="n">image2vector_test</span><span class="p">(</span><span class="n">image2vector</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>image2vector(image) = [[0.67826139]
 [0.29380381]
 [0.90714982]
 [0.52835647]
 [0.4215251 ]
 [0.45017551]
 [0.92814219]
 [0.96677647]
 [0.85304703]
 [0.52351845]
 [0.19981397]
 [0.27417313]
 [0.60659855]
 [0.00533165]
 [0.10820313]
 [0.49978937]
 [0.34144279]
 [0.94630077]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="normalizing-rows">
<h5>1.4 - Normalizing rows<a class="headerlink" href="#normalizing-rows" title="Link to this heading">#</a></h5>
<p>Another common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to <span class="math notranslate nohighlight">\( \frac{x}{\| x\|} \)</span> (dividing each row vector of x by its norm).</p>
<p>For example, if</p>
<div class="math notranslate nohighlight">
\[\begin{split}x = \begin{bmatrix}
        0 &amp; 3 &amp; 4 \\
        2 &amp; 6 &amp; 4 \\
\end{bmatrix}\end{split}\]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\| x\| = \text{np.linalg.norm(x, axis=1, keepdims=True)} = \begin{bmatrix}
    5 \\
    \sqrt{56} \\
\end{bmatrix}\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split} x\_normalized = \frac{x}{\| x\|} = \begin{bmatrix}
    0 &amp; \frac{3}{5} &amp; \frac{4}{5} \\
    \frac{2}{\sqrt{56}} &amp; \frac{6}{\sqrt{56}} &amp; \frac{4}{\sqrt{56}} \\
\end{bmatrix}\end{split}\]</div>
<p>Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you’re going to learn about it in part 5.</p>
<p>With <code class="docutils literal notranslate"><span class="pre">keepdims=True</span></code> the result will broadcast correctly against the original x.</p>
<p><code class="docutils literal notranslate"><span class="pre">axis=1</span></code> means you are going to get the norm in a row-wise manner. If you need the norm in a column-wise way, you would need to set <code class="docutils literal notranslate"><span class="pre">axis=0</span></code>.</p>
<p>numpy.linalg.norm has another parameter <code class="docutils literal notranslate"><span class="pre">ord</span></code> where we specify the type of normalization to be done (in the exercise below you’ll do 2-norm). To get familiar with the types of normalization you can visit <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html">numpy.linalg.norm</a></p>
</section>
<section id="exercise-6-normalize-rows">
<h5>Exercise 6 - normalize_rows<a class="headerlink" href="#exercise-6-normalize-rows" title="Link to this heading">#</a></h5>
<p>Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: normalize_rows</span>

<span class="k">def</span> <span class="nf">normalize_rows</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement a function that normalizes each row of the matrix x (to have unit length).</span>
<span class="sd">    </span>
<span class="sd">    Argument:</span>
<span class="sd">    x -- A numpy matrix of shape (n, m)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    x -- The normalized (by row) numpy matrix. You are allowed to modify x.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)</span>
    <span class="c1"># x_norm =</span>
    <span class="c1"># Divide x by its norm.</span>
    <span class="c1"># x =</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">x_norm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">ord</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">/=</span> <span class="n">x_norm</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="k">return</span> <span class="n">x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;normalizeRows(x) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">normalize_rows</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="n">normalizeRows_test</span><span class="p">(</span><span class="n">normalize_rows</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>normalizeRows(x) = [[0.         0.6        0.8       ]
 [0.13736056 0.82416338 0.54944226]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
<p><strong>Note</strong>:
In normalize_rows(), you can try to print the shapes of x_norm and x, and then rerun the assessment. You’ll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting and we’ll talk about it now!</p>
</section>
<section id="exercise-7-softmax">
<h5>Exercise 7 - softmax<a class="headerlink" href="#exercise-7-softmax" title="Link to this heading">#</a></h5>
<p>Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization.</p>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{for } x \in \mathbb{R}^{1\times n} \text{,     }\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
 \text{softmax}(x) &amp;= \text{softmax}\left(\begin{bmatrix}
    x_1  &amp;&amp;
    x_2 &amp;&amp;
    ...  &amp;&amp;
    x_n  
\end{bmatrix}\right) \\&amp;= \begin{bmatrix}
    \frac{e^{x_1}}{\sum_{j}e^{x_j}}  &amp;&amp;
    \frac{e^{x_2}}{\sum_{j}e^{x_j}}  &amp;&amp;
    ...  &amp;&amp;
    \frac{e^{x_n}}{\sum_{j}e^{x_j}} 
\end{bmatrix} 
\end{align*}\end{split}\]</div>
<ul class="simple">
<li><p>for a matrix  <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{m \times n}\)</span>,  <span class="math notranslate nohighlight">\(x_{ij}\)</span> maps to the element in the <span class="math notranslate nohighlight">\(i^{th}\)</span> row and <span class="math notranslate nohighlight">\(j^{th}\)</span> column of <span class="math notranslate nohighlight">\(x\)</span>, thus we have:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
\text{softmax}(x) &amp;= \text{softmax}\begin{bmatrix}
            x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots  &amp; x_{1n} \\
            x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots  &amp; x_{2n} \\
            \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
            x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots  &amp; x_{mn}
            \end{bmatrix} \\ \\&amp;= 
 \begin{bmatrix}
    \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots  &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\
    \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots  &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots  &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}
\end{bmatrix} \\ \\ &amp;= \begin{pmatrix}
    \text{softmax}\text{(first row of x)}  \\
    \text{softmax}\text{(second row of x)} \\
    \vdots  \\
    \text{softmax}\text{(last row of x)} \\
\end{pmatrix} 
\end{align*}\end{split}\]</div>
<p><strong>Notes:</strong>
Note that later in the course, you’ll see “m” used to represent the “number of training examples”, and each training example is in its own column of the matrix. Also, each feature will be in its own row (each row has data for the same feature).<br />
Softmax should be performed for all features of each training example, so softmax would be performed on the columns (once we switch to that representation later in this course).</p>
<p>However, in this coding practice, we’re just focusing on getting familiar with Python, so we’re using the common math notation <span class="math notranslate nohighlight">\(m \times n\)</span><br />
where <span class="math notranslate nohighlight">\(m\)</span> is the number of rows and <span class="math notranslate nohighlight">\(n\)</span> is the number of columns.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: softmax</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculates the softmax for each row of the input x.</span>

<span class="sd">    Your code should work for a row vector and also for matrices of shape (m,n).</span>

<span class="sd">    Argument:</span>
<span class="sd">    x -- A numpy matrix of shape (m,n)</span>

<span class="sd">    Returns:</span>
<span class="sd">    s -- A numpy matrix equal to the softmax of x, of shape (m,n)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">#(≈ 3 lines of code)</span>
    <span class="c1"># Apply exp() element-wise to x. Use np.exp(...).</span>
    <span class="c1"># x_exp = ...</span>

    <span class="c1"># Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True).</span>
    <span class="c1"># x_sum = ...</span>
    
    <span class="c1"># Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting.</span>
    <span class="c1"># s = ...</span>
    
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">x_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="n">x_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x_exp</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    
    <span class="n">s</span> <span class="o">=</span> <span class="n">x_exp</span> <span class="o">/</span> <span class="n">x_sum</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">s</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span> <span class="p">,</span><span class="mi">0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;softmax(x) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">softmax</span><span class="p">(</span><span class="n">t_x</span><span class="p">)))</span>

<span class="n">softmax_test</span><span class="p">(</span><span class="n">softmax</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>softmax(x) = [[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04
  1.21052389e-04]
 [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04
  8.01252314e-04]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
<section id="notes">
<h6>Notes<a class="headerlink" href="#notes" title="Link to this heading">#</a></h6>
<ul class="simple">
<li><p>If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). <strong>x_exp/x_sum</strong> works due to python broadcasting.</p></li>
</ul>
<p>Congratulations! You now have a pretty good understanding of python numpy and have implemented a few useful functions that you will be using in deep learning.</p>
<p><strong>What you need to remember:</strong></p>
<ul class="simple">
<li><p>np.exp(x) works for any np.array x and applies the exponential function to every coordinate</p></li>
<li><p>the sigmoid function and its gradient</p></li>
<li><p>image2vector is commonly used in deep learning</p></li>
<li><p>np.reshape is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs.</p></li>
<li><p>numpy has efficient built-in functions</p></li>
<li><p>broadcasting is extremely useful</p></li>
</ul>
</section>
</section>
</section>
<section id="vectorization">
<h4>2 - Vectorization<a class="headerlink" href="#vectorization" title="Link to this heading">#</a></h4>
<p>In deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is  computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">x1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1">### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">dot</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dot = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dot</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="c1">### CLASSIC OUTER PRODUCT IMPLEMENTATION ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">outer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)))</span> <span class="c1"># we create a len(x1)*len(x2) matrix with only zeros</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)):</span>
        <span class="n">outer</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;outer = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">outer</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="c1">### CLASSIC ELEMENTWISE IMPLEMENTATION ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
    <span class="n">mul</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;elementwise multiplication = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mul</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="c1">### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">))</span> <span class="c1"># Random 3*len(x1) numpy array</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">gdot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)):</span>
        <span class="n">gdot</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">x1</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;gdot = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gdot</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dot = 278
 ----- Computation time = 0.1120000000005561ms
outer = [[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]
 [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]
 [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]
 [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]
 ----- Computation time = 0.2629999999994581ms
elementwise multiplication = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]
 ----- Computation time = 0.11399999999994748ms
gdot = [13.84179255 20.37769035 27.11990522]
 ----- Computation time = 0.204000000000093ms
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">x2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1">### VECTORIZED DOT PRODUCT OF VECTORS ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dot = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dot</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="c1">### VECTORIZED OUTER PRODUCT ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">outer</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;outer = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">outer</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="c1">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">mul</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="n">x2</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;elementwise multiplication = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mul</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="c1">### VECTORIZED GENERAL DOT PRODUCT ###</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="n">dot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">x1</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">process_time</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;gdot = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dot</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> ----- Computation time = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span> <span class="o">*</span> <span class="p">(</span><span class="n">toc</span> <span class="o">-</span> <span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dot = 278
 ----- Computation time = 0.5890000000006168ms
outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]
 [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]
 [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]
 [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]
 ----- Computation time = 0.5790000000001072ms
elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]
 ----- Computation time = 0.1020000000000465ms
gdot = [13.84179255 20.37769035 27.11990522]
 ----- Computation time = 0.40900000000032577ms
</pre></div>
</div>
</div>
</div>
<p>As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger.</p>
<p><strong>Note</strong> that <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code class="docutils literal notranslate"><span class="pre">np.multiply()</span></code> and the <code class="docutils literal notranslate"><span class="pre">*</span></code> operator (which is equivalent to  <code class="docutils literal notranslate"><span class="pre">.*</span></code> in Matlab/Octave), which performs an element-wise multiplication.</p>
<section id="implement-the-l1-and-l2-loss-functions">
<h5>2.1 Implement the L1 and L2 loss functions<a class="headerlink" href="#implement-the-l1-and-l2-loss-functions" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-8-l1">
<h5>Exercise 8 - L1<a class="headerlink" href="#exercise-8-l1" title="Link to this heading">#</a></h5>
<p>Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful.</p>
<p><strong>Reminder</strong>:</p>
<ul class="simple">
<li><p>The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions (<span class="math notranslate nohighlight">\( \hat{y} \)</span>) are from the true values (<span class="math notranslate nohighlight">\(y\)</span>). In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost.</p></li>
<li><p>L1 loss is defined as:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{align*} &amp; L_1(\hat{y}, y) = \sum_{i=0}^{m-1}|y^{(i)} - \hat{y}^{(i)}| \end{align*}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: L1</span>

<span class="k">def</span> <span class="nf">L1</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    yhat -- vector of size m (predicted labels)</span>
<span class="sd">    y -- vector of size m (true labels)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    loss -- the value of the L1 loss function defined above</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># loss = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">yhat</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L1</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)))</span>

<span class="n">L1_test</span><span class="p">(</span><span class="n">L1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L1 = 1.1
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-9-l2">
<h5>Exercise 9 - L2<a class="headerlink" href="#exercise-9-l2" title="Link to this heading">#</a></h5>
<p>Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if <span class="math notranslate nohighlight">\(x = [x_1, x_2, ..., x_n]\)</span>, then <code class="docutils literal notranslate"><span class="pre">np.dot(x,x)</span></code> = <span class="math notranslate nohighlight">\(\sum_{j=1}^n x_j^{2}\)</span>.</p>
<ul class="simple">
<li><p>L2 loss is defined as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{align*} &amp; L_2(\hat{y},y) = \sum_{i=0}^{m-1}(y^{(i)} - \hat{y}^{(i)})^2 \end{align*}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: L2</span>

<span class="k">def</span> <span class="nf">L2</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    yhat -- vector of size m (predicted labels)</span>
<span class="sd">    y -- vector of size m (true labels)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    loss -- the value of the L2 loss function defined above</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># loss = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="o">-</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="o">-</span><span class="n">yhat</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">yhat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.9</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L2</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">y</span><span class="p">)))</span>

<span class="n">L2_test</span><span class="p">(</span><span class="n">L2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L2 = 0.43
 All tests passed.
</pre></div>
</div>
</div>
</div>
<p>Congratulations on completing this assignment. We hope that this little warm-up exercise helps you in the future assignments, which will be more exciting and interesting!</p>
<p><strong>What to remember:</strong></p>
<ul class="simple">
<li><p>Vectorization is very important in deep learning. It provides computational efficiency and clarity.</p></li>
<li><p>You have reviewed the L1 and L2 loss.</p></li>
<li><p>You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc…</p></li>
</ul>
</section>
</section>
</section>
<span id="document-C2_Practical_Test"></span><section class="tex2jax_ignore mathjax_ignore" id="practicel-1-logistic-regression-with-a-neural-network-mindset">
<h3>Practicel 1: Logistic Regression with a Neural Network mindset<a class="headerlink" href="#practicel-1-logistic-regression-with-a-neural-network-mindset" title="Link to this heading">#</a></h3>
<p>Welcome to your first (required) programming assignment! You will build a logistic regression classifier to recognize  cats. This assignment will step you through how to do this with a Neural Network mindset, and will also hone your intuitions about deep learning.</p>
<p><strong>Instructions:</strong></p>
<ul class="simple">
<li><p>Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">np.dot(X,Y)</span></code> to calculate dot products.</p></li>
</ul>
<p><strong>You will learn to:</strong></p>
<ul class="simple">
<li><p>Build the general architecture of a learning algorithm, including:</p>
<ul>
<li><p>Initializing parameters</p></li>
<li><p>Calculating the cost function and its gradient</p></li>
<li><p>Using an optimization algorithm (gradient descent)</p></li>
</ul>
</li>
<li><p>Gather all three functions above into a main model function, in the right order.</p></li>
</ul>
<section id="important-note-on-submission-to-the-autograder">
<h4>Important Note on Submission to the AutoGrader<a class="headerlink" href="#important-note-on-submission-to-the-autograder" title="Link to this heading">#</a></h4>
<p>Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:</p>
<ol class="arabic simple">
<li><p>You have not added any <em>extra</em> <code class="docutils literal notranslate"><span class="pre">print</span></code> statement(s) in the assignment.</p></li>
<li><p>You have not added any <em>extra</em> code cell(s) in the assignment.</p></li>
<li><p>You have not changed any of the function parameters.</p></li>
<li><p>You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.</p></li>
<li><p>You are not changing the assignment code where it is not required, like creating <em>extra</em> variables.</p></li>
</ol>
<p>If you do any of the following, you will get something like, <code class="docutils literal notranslate"><span class="pre">Grader</span> <span class="pre">Error:</span> <span class="pre">Grader</span> <span class="pre">feedback</span> <span class="pre">not</span> <span class="pre">found</span></code> (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these <a class="reference external" href="https://www.coursera.org/learn/neural-networks-deep-learning/supplement/iLwon/h-ow-to-refresh-your-workspace">instructions</a>.</p>
</section>
<section id="packages">
<h4>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h4>
<p>First, let’s run the cell below to import all the packages that you will need during this assignment.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://numpy.org/doc/1.20/">numpy</a> is the fundamental package for scientific computing with Python.</p></li>
<li><p><a class="reference external" href="http://www.h5py.org">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</p></li>
<li><p><a class="reference external" href="http://matplotlib.org">matplotlib</a> is a famous library to plot graphs in Python.</p></li>
<li><p><a class="reference external" href="https://pillow.readthedocs.io/en/stable/">PIL</a> and <a class="reference external" href="https://www.scipy.org/">scipy</a> are used here to test your model with your own picture at the end.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">ndimage</span>
<span class="kn">from</span> <span class="nn">lr_utils</span> <span class="kn">import</span> <span class="n">load_dataset</span>
<span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
</pre></div>
</div>
</div>
</div>
</section>
<section id="overview-of-the-problem-set">
<h4>2 - Overview of the Problem set<a class="headerlink" href="#overview-of-the-problem-set" title="Link to this heading">#</a></h4>
<p><strong>Problem Statement</strong>: You are given a dataset (“data.h5”) containing:
- a training set of m_train images labeled as cat (y=1) or non-cat (y=0)
- a test set of m_test images labeled as cat or non-cat
- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px).</p>
<p>You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat.</p>
<p>Let’s get more familiar with the dataset. Load the data by running the following code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Loading the data (cat/non-cat)</span>
<span class="n">train_set_x_orig</span><span class="p">,</span> <span class="n">train_set_y</span><span class="p">,</span> <span class="n">test_set_x_orig</span><span class="p">,</span> <span class="n">test_set_y</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing).</p>
<p>Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the <code class="docutils literal notranslate"><span class="pre">index</span></code> value and re-run to see other images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of a picture</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">25</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_set_x_orig</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;y = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_set_y</span><span class="p">[:,</span> <span class="n">index</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;, it&#39;s a &#39;&quot;</span> <span class="o">+</span> <span class="n">classes</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">train_set_y</span><span class="p">[:,</span> <span class="n">index</span><span class="p">])]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">+</span>  <span class="s2">&quot;&#39; picture.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y = [1], it&#39;s a &#39;cat&#39; picture.
</pre></div>
</div>
<img alt="_images/d8bb211c74cf154e5618e2f55c34771e161a8b4ec1c2888010528a46a3d5c63b.png" src="_images/d8bb211c74cf154e5618e2f55c34771e161a8b4ec1c2888010528a46a3d5c63b.png" />
</div>
</div>
<p>Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs.</p>
<section id="exercise-1">
<h5>Exercise 1<a class="headerlink" href="#exercise-1" title="Link to this heading">#</a></h5>
<p>Find the values for:
- m_train (number of training examples)
- m_test (number of test examples)
- num_px (= height = width of a training image)
Remember that <code class="docutils literal notranslate"><span class="pre">train_set_x_orig</span></code> is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access <code class="docutils literal notranslate"><span class="pre">m_train</span></code> by writing <code class="docutils literal notranslate"><span class="pre">train_set_x_orig.shape[0]</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#(≈ 3 lines of code)</span>
<span class="c1"># m_train = </span>
<span class="c1"># m_test = </span>
<span class="c1"># num_px = </span>
<span class="c1"># YOUR CODE STARTS HERE</span>

<span class="n">m_train</span> <span class="o">=</span> <span class="n">train_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">m_test</span> <span class="o">=</span> <span class="n">test_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_px</span> <span class="o">=</span> <span class="n">train_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># YOUR CODE ENDS HERE</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Number of training examples: m_train = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">m_train</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Number of testing examples: m_test = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">m_test</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Height/Width of each image: num_px = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_px</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Each image is of size: (&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_px</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_px</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, 3)&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_set_x shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_set_y shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_set_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_set_x shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_set_y shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_set_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of training examples: m_train = 209
Number of testing examples: m_test = 50
Height/Width of each image: num_px = 64
Each image is of size: (64, 64, 3)
train_set_x shape: (209, 64, 64, 3)
train_set_y shape: (1, 209)
test_set_x shape: (50, 64, 64, 3)
test_set_y shape: (1, 50)
</pre></div>
</div>
</div>
</div>
<p><strong>Expected Output for m_train, m_test and num_px</strong>:</p>
<table style="width:15%">
  <tr>
    <td> m_train </td>
    <td> 209 </td> 
  </tr>
  <tr>
    <td>m_test</td>
    <td> 50 </td> 
  </tr>
  <tr>
    <td>num_px</td>
    <td> 64 </td> 
  </tr>
</table>
<p>For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px <span class="math notranslate nohighlight">\(*\)</span> num_px <span class="math notranslate nohighlight">\(*\)</span> 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.</p>
</section>
<section id="exercise-2">
<h5>Exercise 2<a class="headerlink" href="#exercise-2" title="Link to this heading">#</a></h5>
<p>Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px <span class="math notranslate nohighlight">\(*\)</span> num_px <span class="math notranslate nohighlight">\(*\)</span> 3, 1).</p>
<p>A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b<span class="math notranslate nohighlight">\(*\)</span>c<span class="math notranslate nohighlight">\(*\)</span>d, a) is to use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_flatten</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>      <span class="c1"># X.T is the transpose of X</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the training and test examples</span>
<span class="c1">#(≈ 2 lines of code)</span>
<span class="c1"># train_set_x_flatten = ...</span>
<span class="c1"># test_set_x_flatten = ...</span>
<span class="c1"># YOUR CODE STARTS HERE</span>

<span class="n">train_set_x_flatten</span> <span class="o">=</span> <span class="n">train_set_x_orig</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">train_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">test_set_x_flatten</span> <span class="o">=</span> <span class="n">test_set_x_orig</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_set_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># YOUR CODE ENDS HERE</span>

<span class="c1"># Check that the first 10 pixels of the second image are in the correct place</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">alltrue</span><span class="p">(</span><span class="n">train_set_x_flatten</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="mi">196</span><span class="p">,</span> <span class="mi">192</span><span class="p">,</span> <span class="mi">190</span><span class="p">,</span> <span class="mi">193</span><span class="p">,</span> <span class="mi">186</span><span class="p">,</span> <span class="mi">182</span><span class="p">,</span> <span class="mi">188</span><span class="p">,</span> <span class="mi">179</span><span class="p">,</span> <span class="mi">174</span><span class="p">,</span> <span class="mi">213</span><span class="p">]),</span> <span class="s2">&quot;Wrong solution. Use (X.shape[0], -1).T.&quot;</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">alltrue</span><span class="p">(</span><span class="n">test_set_x_flatten</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="p">[</span><span class="mi">115</span><span class="p">,</span> <span class="mi">110</span><span class="p">,</span> <span class="mi">111</span><span class="p">,</span> <span class="mi">137</span><span class="p">,</span> <span class="mi">129</span><span class="p">,</span> <span class="mi">129</span><span class="p">,</span> <span class="mi">155</span><span class="p">,</span> <span class="mi">146</span><span class="p">,</span> <span class="mi">145</span><span class="p">,</span> <span class="mi">159</span><span class="p">]),</span> <span class="s2">&quot;Wrong solution. Use (X.shape[0], -1).T.&quot;</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_set_x_flatten shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_set_x_flatten</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_set_y shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_set_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_set_x_flatten shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_set_x_flatten</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_set_y shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_set_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train_set_x_flatten shape: (12288, 209)
train_set_y shape: (1, 209)
test_set_x_flatten shape: (12288, 50)
test_set_y shape: (1, 50)
</pre></div>
</div>
</div>
</div>
<p><strong>Expected Output</strong>:</p>
<table style="width:35%">
  <tr>
    <td>train_set_x_flatten shape</td>
    <td> (12288, 209)</td> 
  </tr>
  <tr>
    <td>train_set_y shape</td>
    <td>(1, 209)</td> 
  </tr>
  <tr>
    <td>test_set_x_flatten shape</td>
    <td>(12288, 50)</td> 
  </tr>
  <tr>
    <td>test_set_y shape</td>
    <td>(1, 50)</td> 
  </tr>
</table><p>To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.</p>
<p>One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).</p>
<!-- During the training of your model, you're going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don't explode. You will see that more in detail later in the lectures. !--> 
<p>Let’s standardize our dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_set_x</span> <span class="o">=</span> <span class="n">train_set_x_flatten</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">test_set_x</span> <span class="o">=</span> <span class="n">test_set_x_flatten</span> <span class="o">/</span> <span class="mf">255.</span>
</pre></div>
</div>
</div>
</div>
<p><strong>What you need to remember:</strong></p>
<p>Common steps for pre-processing a new dataset are:</p>
<ul class="simple">
<li><p>Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …)</p></li>
<li><p>Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)</p></li>
<li><p>“Standardize” the data</p></li>
</ul>
</section>
</section>
<section id="general-architecture-of-the-learning-algorithm">
<h4>3 - General Architecture of the learning algorithm<a class="headerlink" href="#general-architecture-of-the-learning-algorithm" title="Link to this heading">#</a></h4>
<p>It’s time to design a simple algorithm to distinguish cat images from non-cat images.</p>
<p>You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why <strong>Logistic Regression is actually a very simple Neural Network!</strong></p>
<figure class="align-default" id="test-1">
<a class="reference internal image-reference" href="_images/LogReg_kiank.png"><img alt="_images/LogReg_kiank.png" src="_images/LogReg_kiank.png" style="width: 520px; height: 400px;" /></a>
</figure>
<p><strong>Mathematical expression of the algorithm</strong>:</p>
<p>For one example <span class="math notranslate nohighlight">\(x^{(i)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[z^{(i)} = w^T x^{(i)} + b \tag{1}\]</div>
<div class="math notranslate nohighlight">
\[\hat{y}^{(i)} = a^{(i)} = \text{sigmoid}(z^{(i)})\tag{2}\]</div>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \log(a^{(i)}) - (1-y^{(i)} )  \log(1-a^{(i)})\tag{3}\]</div>
<p>The cost is then computed by summing over all training examples:</p>
<div class="math notranslate nohighlight">
\[ J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{6}\]</div>
<p><strong>Key steps</strong>:
In this exercise, you will carry out the following steps:
- Initialize the parameters of the model
- Learn the parameters for the model by minimizing the cost<br />
- Use the learned parameters to make predictions (on the test set)
- Analyse the results and conclude</p>
</section>
<section id="building-the-parts-of-our-algorithm">
<h4>4 - Building the parts of our algorithm<a class="headerlink" href="#building-the-parts-of-our-algorithm" title="Link to this heading">#</a></h4>
<p>The main steps for building a Neural Network are:</p>
<ol class="arabic simple">
<li><p>Define the model structure (such as number of input features)</p></li>
<li><p>Initialize the model’s parameters</p></li>
<li><p>Loop:</p>
<ul class="simple">
<li><p>Calculate current loss (forward propagation)</p></li>
<li><p>Calculate current gradient (backward propagation)</p></li>
<li><p>Update parameters (gradient descent)</p></li>
</ul>
</li>
</ol>
<p>You often build 1-3 separately and integrate them into one function we call <code class="docutils literal notranslate"><span class="pre">model()</span></code>.</p>
<section id="helper-functions">
<h5>4.1 - Helper functions<a class="headerlink" href="#helper-functions" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-3-sigmoid">
<h5>Exercise 3 - sigmoid<a class="headerlink" href="#exercise-3-sigmoid" title="Link to this heading">#</a></h5>
<p>Using your code from “Python Basics”, implement <code class="docutils literal notranslate"><span class="pre">sigmoid()</span></code>. As you’ve seen in the figure above, you need to compute <span class="math notranslate nohighlight">\(\text{sigmoid}(z) = \frac{1}{1 + e^{-z}}\)</span> for <span class="math notranslate nohighlight">\(z = w^T x + b\)</span> to make predictions. Use np.exp().</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: sigmoid</span>

<span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the sigmoid of z</span>

<span class="sd">    Arguments:</span>
<span class="sd">    z -- A scalar or numpy array of any size.</span>

<span class="sd">    Return:</span>
<span class="sd">    s -- sigmoid(z)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># s = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">s</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">s</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;sigmoid([0, 2]) = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">]))))</span>

<span class="n">sigmoid_test</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>sigmoid([0, 2]) = [0.5        0.88079708]
All tests passed!
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.62245933 0.5        0.88079708]
</pre></div>
</div>
</div>
</div>
</section>
<section id="initializing-parameters">
<h5>4.2 - Initializing parameters<a class="headerlink" href="#initializing-parameters" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-4-initialize-with-zeros">
<h5>Exercise 4 - initialize_with_zeros<a class="headerlink" href="#exercise-4-initialize-with-zeros" title="Link to this heading">#</a></h5>
<p>Implement parameter initialization in the cell below. You have to initialize w as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_with_zeros</span>

<span class="k">def</span> <span class="nf">initialize_with_zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.</span>
<span class="sd">    </span>
<span class="sd">    Argument:</span>
<span class="sd">    dim -- size of the w vector we want (or number of parameters in this case)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    w -- initialized vector of shape (dim, 1)</span>
<span class="sd">    b -- initialized scalar (corresponds to the bias) of type float</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># (≈ 2 lines of code)</span>
    <span class="c1"># w = ...</span>
    <span class="c1"># b = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">dim</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">b</span> <span class="o">=</span> <span class="mf">0.0</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">initialize_with_zeros</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="nb">float</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;w = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">w</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;b = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>

<span class="n">initialize_with_zeros_test_1</span><span class="p">(</span><span class="n">initialize_with_zeros</span><span class="p">)</span>
<span class="n">initialize_with_zeros_test_2</span><span class="p">(</span><span class="n">initialize_with_zeros</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [[0.]
 [0.]]
b = 0.0
First test passed!
Second test passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="forward-and-backward-propagation">
<h5>4.3 - Forward and Backward propagation<a class="headerlink" href="#forward-and-backward-propagation" title="Link to this heading">#</a></h5>
<p>Now that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters.</p>
</section>
<section id="exercise-5-propagate">
<h5>Exercise 5 - propagate<a class="headerlink" href="#exercise-5-propagate" title="Link to this heading">#</a></h5>
<p>Implement a function <code class="docutils literal notranslate"><span class="pre">propagate()</span></code> that computes the cost function and its gradient.</p>
<p><strong>Hints</strong>:</p>
<p>Forward Propagation:</p>
<ul class="simple">
<li><p>You get X</p></li>
<li><p>You compute <span class="math notranslate nohighlight">\(A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})\)</span></p></li>
<li><p>You calculate the cost function: <span class="math notranslate nohighlight">\(J = -\dfrac{1}{m}\sum_{i=1}^{m}(y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)}))\)</span></p></li>
</ul>
<p>Here are the two formulas you will be using:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial J}{\partial w} = \dfrac{1}{m}X(A-Y)^T\tag{7}\]</div>
<div class="math notranslate nohighlight">
\[ \frac{\partial J}{\partial b} = \dfrac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: propagate</span>

<span class="k">def</span> <span class="nf">propagate</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the cost function and its gradient for the propagation explained above</span>

<span class="sd">    Arguments:</span>
<span class="sd">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span>
<span class="sd">    b -- bias, a scalar</span>
<span class="sd">    X -- data of size (num_px * num_px * 3, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)</span>

<span class="sd">    Return:</span>
<span class="sd">    grads -- dictionary containing the gradients of the weights and bias</span>
<span class="sd">            (dw -- gradient of the loss with respect to w, thus same shape as w)</span>
<span class="sd">            (db -- gradient of the loss with respect to b, thus same shape as b)</span>
<span class="sd">    cost -- negative log-likelihood cost for logistic regression</span>
<span class="sd">    </span>
<span class="sd">    Tips:</span>
<span class="sd">    - Write your code step by step for the propagation. np.log(), np.dot()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># FORWARD PROPAGATION (FROM X TO COST)</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># compute activation</span>
    <span class="c1"># A = ...</span>
    <span class="c1"># compute cost by using np.dot to perform multiplication. </span>
    <span class="c1"># And don&#39;t use loops for the sum.</span>
    <span class="c1"># cost = ...                                </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Y</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="c1"># BACKWARD PROPAGATION (TO FIND GRAD)</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># dw = ...</span>
    <span class="c1"># db = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>

    
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dw&quot;</span><span class="p">:</span> <span class="n">dw</span><span class="p">,</span>
             <span class="s2">&quot;db&quot;</span><span class="p">:</span> <span class="n">db</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">grads</span><span class="p">,</span> <span class="n">cost</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">]])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="n">grads</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">propagate</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

<span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dw&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span>
<span class="k">assert</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dw&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span><span class="p">])</span> <span class="o">==</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span>


<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dw = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dw&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;db = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;cost = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>

<span class="n">propagate_test</span><span class="p">(</span><span class="n">propagate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dw = [[ 0.25071532]
 [-0.06604096]]
db = -0.1250040450043965
cost = 0.15900537707692405
All tests passed!
</pre></div>
</div>
</div>
</div>
<p><strong>Expected output</strong></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dw</span> <span class="o">=</span> <span class="p">[[</span> <span class="mf">0.25071532</span><span class="p">]</span>
 <span class="p">[</span><span class="o">-</span><span class="mf">0.06604096</span><span class="p">]]</span>
<span class="n">db</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.1250040450043965</span>
<span class="n">cost</span> <span class="o">=</span> <span class="mf">0.15900537707692405</span>
</pre></div>
</div>
</section>
<section id="optimization">
<h5>4.4 - Optimization<a class="headerlink" href="#optimization" title="Link to this heading">#</a></h5>
<ul class="simple">
<li><p>You have initialized your parameters.</p></li>
<li><p>You are also able to compute a cost function and its gradient.</p></li>
<li><p>Now, you want to update the parameters using gradient descent.</p></li>
</ul>
</section>
<section id="exercise-6-optimize">
<h5>Exercise 6 - optimize<a class="headerlink" href="#exercise-6-optimize" title="Link to this heading">#</a></h5>
<p>Write down the optimization function. The goal is to learn <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> by minimizing the cost function <span class="math notranslate nohighlight">\(J\)</span>. For a parameter <span class="math notranslate nohighlight">\(\theta\)</span>, the update rule is <span class="math notranslate nohighlight">\( \theta = \theta - \alpha \text{ } d\theta\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: optimize</span>

<span class="k">def</span> <span class="nf">optimize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.009</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function optimizes w and b by running a gradient descent algorithm</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span>
<span class="sd">    b -- bias, a scalar</span>
<span class="sd">    X -- data of shape (num_px * num_px * 3, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)</span>
<span class="sd">    num_iterations -- number of iterations of the optimization loop</span>
<span class="sd">    learning_rate -- learning rate of the gradient descent update rule</span>
<span class="sd">    print_cost -- True to print the loss every 100 steps</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    params -- dictionary containing the weights w and bias b</span>
<span class="sd">    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function</span>
<span class="sd">    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.</span>
<span class="sd">    </span>
<span class="sd">    Tips:</span>
<span class="sd">    You basically need to write down two steps and iterate through them:</span>
<span class="sd">        1) Calculate the cost and the gradient for the current parameters. Use propagate().</span>
<span class="sd">        2) Update the parameters using gradient descent rule for w and b.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">w</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>
        <span class="c1"># (≈ 1 lines of code)</span>
        <span class="c1"># Cost and gradient calculation </span>
        <span class="c1"># grads, cost = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">grads</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">propagate</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Retrieve derivatives from grads</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dw&quot;</span><span class="p">]</span>
        <span class="n">db</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span><span class="p">]</span>
        
        <span class="c1"># update rule (≈ 2 lines of code)</span>
        <span class="c1"># w = ...</span>
        <span class="c1"># b = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dw</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Record the costs</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        
            <span class="c1"># Print the cost every 100 training iterations</span>
            <span class="k">if</span> <span class="n">print_cost</span><span class="p">:</span>
                <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">%i</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
    
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;w&quot;</span><span class="p">:</span> <span class="n">w</span><span class="p">,</span>
              <span class="s2">&quot;b&quot;</span><span class="p">:</span> <span class="n">b</span><span class="p">}</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dw&quot;</span><span class="p">:</span> <span class="n">dw</span><span class="p">,</span>
             <span class="s2">&quot;db&quot;</span><span class="p">:</span> <span class="n">db</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">costs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.009</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;w = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;b = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dw = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dw&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;db = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Costs = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">costs</span><span class="p">))</span>

<span class="n">optimize_test</span><span class="p">(</span><span class="n">optimize</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [[0.80956046]
 [2.0508202 ]]
b = 1.5948713189708588
dw = [[ 0.17860505]
 [-0.04840656]]
db = -0.08888460336847771
Costs = [array(0.15900538)]
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="exercise-7-predict">
<h5>Exercise 7 - predict<a class="headerlink" href="#exercise-7-predict" title="Link to this heading">#</a></h5>
<p>The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> function. There are two steps to computing predictions:</p>
<ol class="arabic simple">
<li><p>Calculate <span class="math notranslate nohighlight">\(\hat{Y} = A = \sigma(w^T X + b)\)</span></p></li>
<li><p>Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector <code class="docutils literal notranslate"><span class="pre">Y_prediction</span></code>. If you wish, you can use an <code class="docutils literal notranslate"><span class="pre">if</span></code>/<code class="docutils literal notranslate"><span class="pre">else</span></code> statement in a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop (though there is also a way to vectorize this).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: predict</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    w -- weights, a numpy array of size (num_px * num_px * 3, 1)</span>
<span class="sd">    b -- bias, a scalar</span>
<span class="sd">    X -- data of size (num_px * num_px * 3, number of examples)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X</span>
<span class="sd">    &#39;&#39;&#39;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Y_prediction</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Compute vector &quot;A&quot; predicting the probabilities of a cat being present in the picture</span>
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># A = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">A</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        
        <span class="c1"># Convert probabilities A[0,i] to actual predictions p[0,i]</span>
        <span class="c1">#(≈ 4 lines of code)</span>
        <span class="c1"># if A[0, i] &gt; ____ :</span>
        <span class="c1">#     Y_prediction[0,i] = </span>
        <span class="c1"># else:</span>
        <span class="c1">#     Y_prediction[0,i] = </span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="k">if</span> <span class="n">A</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">Y_prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">Y_prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">Y_prediction</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1124579</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.23106775</span><span class="p">]])</span>
<span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.3</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.2</span><span class="p">],[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;predictions = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">predict</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X</span><span class="p">)))</span>

<span class="n">predict_test</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>predictions = [[1. 1. 0.]]
All tests passed!
</pre></div>
</div>
</div>
</div>
<p><strong>What to remember:</strong></p>
<p>You’ve implemented several functions that:</p>
<ul class="simple">
<li><p>Initialize (w,b)</p></li>
<li><p>Optimize the loss iteratively to learn parameters (w,b):</p>
<ul>
<li><p>Computing the cost and its gradient</p></li>
<li><p>Updating the parameters using gradient descent</p></li>
</ul>
</li>
<li><p>Use the learned (w,b) to predict the labels for a given set of examples</p></li>
</ul>
</section>
</section>
<section id="merge-all-functions-into-a-model">
<h4>5 - Merge all functions into a model<a class="headerlink" href="#merge-all-functions-into-a-model" title="Link to this heading">#</a></h4>
<p>You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.</p>
<section id="exercise-8-model">
<h5>Exercise 8 - model<a class="headerlink" href="#exercise-8-model" title="Link to this heading">#</a></h5>
<p>Implement the model function. Use the following notation:
- Y_prediction_test for your predictions on the test set
- Y_prediction_train for your predictions on the train set
- parameters, grads, costs for the outputs of optimize()</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: model</span>

<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Builds the logistic regression model by calling the function you&#39;ve implemented previously</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)</span>
<span class="sd">    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)</span>
<span class="sd">    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)</span>
<span class="sd">    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)</span>
<span class="sd">    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters</span>
<span class="sd">    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()</span>
<span class="sd">    print_cost -- Set to True to print the cost every 100 iterations</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    d -- dictionary containing information about the model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># (≈ 1 line of code)   </span>
    <span class="c1"># initialize parameters with zeros</span>
    <span class="c1"># and use the &quot;shape&quot; function to get the first dimension of X_train</span>
    <span class="c1"># w, b = ...</span>
    
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># Gradient descent </span>
    <span class="c1"># params, grads, costs = ...</span>
    
    <span class="c1"># Retrieve parameters w and b from dictionary &quot;params&quot;</span>
    <span class="c1"># w = ...</span>
    <span class="c1"># b = ...</span>
    
    <span class="c1"># Predict test/train set examples (≈ 2 lines of code)</span>
    <span class="c1"># Y_prediction_test = ...</span>
    <span class="c1"># Y_prediction_train = ...</span>
    
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">initialize_with_zeros</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">optimize</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="n">w</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">]</span>
    
    <span class="n">Y_prediction_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
    <span class="n">Y_prediction_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="c1"># Print train/test Errors</span>
    <span class="k">if</span> <span class="n">print_cost</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;train accuracy: </span><span class="si">{}</span><span class="s2"> %&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Y_prediction_train</span> <span class="o">-</span> <span class="n">Y_train</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;test accuracy: </span><span class="si">{}</span><span class="s2"> %&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">Y_prediction_test</span> <span class="o">-</span> <span class="n">Y_test</span><span class="p">))</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>

    
    <span class="n">d</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;costs&quot;</span><span class="p">:</span> <span class="n">costs</span><span class="p">,</span>
         <span class="s2">&quot;Y_prediction_test&quot;</span><span class="p">:</span> <span class="n">Y_prediction_test</span><span class="p">,</span> 
         <span class="s2">&quot;Y_prediction_train&quot;</span> <span class="p">:</span> <span class="n">Y_prediction_train</span><span class="p">,</span> 
         <span class="s2">&quot;w&quot;</span> <span class="p">:</span> <span class="n">w</span><span class="p">,</span> 
         <span class="s2">&quot;b&quot;</span> <span class="p">:</span> <span class="n">b</span><span class="p">,</span>
         <span class="s2">&quot;learning_rate&quot;</span> <span class="p">:</span> <span class="n">learning_rate</span><span class="p">,</span>
         <span class="s2">&quot;num_iterations&quot;</span><span class="p">:</span> <span class="n">num_iterations</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">d</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">public_tests</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">model_test</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>All tests passed!
</pre></div>
</div>
</div>
</div>
<p>If you pass all the tests, run the following cell to train your model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logistic_regression_model</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span><span class="p">,</span> <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.005</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train accuracy: 99.04306220095694 %
test accuracy: 70.0 %
</pre></div>
</div>
</div>
</div>
<p><strong>Comment</strong>: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test accuracy is 70%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week!</p>
<p>Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the <code class="docutils literal notranslate"><span class="pre">index</span></code> variable) you can look at predictions on pictures of the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of a picture that was wrongly classified.</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">test_set_x</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">num_px</span><span class="p">,</span> <span class="n">num_px</span><span class="p">,</span> <span class="mi">3</span><span class="p">)))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;y = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_set_y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">index</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;, you predicted that it is a </span><span class="se">\&quot;</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">classes</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">logistic_regression_model</span><span class="p">[</span><span class="s1">&#39;Y_prediction_test&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span><span class="n">index</span><span class="p">])]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">+</span>  <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2"> picture.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y = 1, you predicted that it is a &quot;cat&quot; picture.
</pre></div>
</div>
<img alt="_images/d687d8fe49a9882d2d7264f7e545eecc0fce71d58a6a53c7b32a29f042e3811b.png" src="_images/d687d8fe49a9882d2d7264f7e545eecc0fce71d58a6a53c7b32a29f042e3811b.png" />
</div>
</div>
<p>Let’s also plot the cost function and the gradients.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot learning curve (with costs)</span>
<span class="n">costs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">logistic_regression_model</span><span class="p">[</span><span class="s1">&#39;costs&#39;</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (per hundreds)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning rate =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">logistic_regression_model</span><span class="p">[</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6dac305439b3e81fe3999bfdd246e924799957582fa93016b54eb6d580844c5a.png" src="_images/6dac305439b3e81fe3999bfdd246e924799957582fa93016b54eb6d580844c5a.png" />
</div>
</div>
<p><strong>Interpretation</strong>:
You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting.</p>
</section>
</section>
<section id="further-analysis-optional-ungraded-exercise">
<h4>6 - Further analysis (optional/ungraded exercise)<a class="headerlink" href="#further-analysis-optional-ungraded-exercise" title="Link to this heading">#</a></h4>
<p>Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<section id="choice-of-learning-rate">
<h5>Choice of learning rate<a class="headerlink" href="#choice-of-learning-rate" title="Link to this heading">#</a></h5>
<p><strong>Reminder</strong>:
In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>  determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate.</p>
<p>Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the <code class="docutils literal notranslate"><span class="pre">learning_rates</span></code> variable to contain, and see what happens.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learning_rates</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.0001</span><span class="p">]</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Training a model with learning rate: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
    <span class="n">models</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)]</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_set_x</span><span class="p">,</span> <span class="n">train_set_y</span><span class="p">,</span> <span class="n">test_set_x</span><span class="p">,</span> <span class="n">test_set_y</span><span class="p">,</span> <span class="n">num_iterations</span><span class="o">=</span><span class="mi">1500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="o">+</span> <span class="s2">&quot;-------------------------------------------------------&quot;</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">lr</span> <span class="ow">in</span> <span class="n">learning_rates</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)][</span><span class="s2">&quot;costs&quot;</span><span class="p">]),</span> <span class="n">label</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">lr</span><span class="p">)][</span><span class="s2">&quot;learning_rate&quot;</span><span class="p">]))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (hundreds)&#39;</span><span class="p">)</span>

<span class="n">legend</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper center&#39;</span><span class="p">,</span> <span class="n">shadow</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">frame</span> <span class="o">=</span> <span class="n">legend</span><span class="o">.</span><span class="n">get_frame</span><span class="p">()</span>
<span class="n">frame</span><span class="o">.</span><span class="n">set_facecolor</span><span class="p">(</span><span class="s1">&#39;0.90&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training a model with learning rate: 0.01
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------

Training a model with learning rate: 0.001
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------

Training a model with learning rate: 0.0001
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-------------------------------------------------------
</pre></div>
</div>
<img alt="_images/012f8ff036bf7576b4249ab09197ca6dd659e6e51c3ed50b612d39f5442c5682.png" src="_images/012f8ff036bf7576b4249ab09197ca6dd659e6e51c3ed50b612d39f5442c5682.png" />
</div>
</div>
<p><strong>Interpretation</strong>:</p>
<ul class="simple">
<li><p>Different learning rates give different costs and thus different predictions results.</p></li>
<li><p>If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost).</p></li>
<li><p>A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.</p></li>
<li><p>In deep learning, we usually recommend that you:</p>
<ul>
<li><p>Choose the learning rate that better minimizes the cost function.</p></li>
<li><p>If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="test-with-your-own-image-optional-ungraded-exercise">
<h4>7 - Test with your own image (optional/ungraded exercise)<a class="headerlink" href="#test-with-your-own-image-optional-ungraded-exercise" title="Link to this heading">#</a></h4>
<p>Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that:</p>
<p>Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># change this to the name of your image file</span>
<span class="n">my_image</span> <span class="o">=</span> <span class="s2">&quot;my_cat.jpg&quot;</span>   

<span class="c1"># We preprocess the image to fit your algorithm.</span>
<span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;images/&quot;</span> <span class="o">+</span> <span class="n">my_image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">num_px</span><span class="p">,</span> <span class="n">num_px</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_px</span> <span class="o">*</span> <span class="n">num_px</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">my_predicted_image</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">logistic_regression_model</span><span class="p">[</span><span class="s2">&quot;w&quot;</span><span class="p">],</span> <span class="n">logistic_regression_model</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">],</span> <span class="n">image</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;y = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">my_predicted_image</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;, your algorithm predicts a </span><span class="se">\&quot;</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">classes</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">my_predicted_image</span><span class="p">)),]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">+</span>  <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2"> picture.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y = 1.0, your algorithm predicts a &quot;cat&quot; picture.
</pre></div>
</div>
<img alt="_images/7d397141c0ab61d9adc957bfbd2dd00ef3eba546ba5acaa6f1326daa51e90565.png" src="_images/7d397141c0ab61d9adc957bfbd2dd00ef3eba546ba5acaa6f1326daa51e90565.png" />
</div>
</div>
<p><strong>What to remember from this assignment:</strong></p>
<ol class="arabic simple">
<li><p>Preprocessing the dataset is important.</p></li>
<li><p>You implemented each function separately: <code class="docutils literal notranslate"><span class="pre">initialize()</span></code>, <code class="docutils literal notranslate"><span class="pre">propagate()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimize()</span></code>. Then you built a <code class="docutils literal notranslate"><span class="pre">model()</span></code>.</p></li>
<li><p>Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!</p></li>
</ol>
<p>Finally, if you’d like, we invite you to try different things on this Notebook. Make sure you submit before trying anything. Once you submit, things you can play with include:
- Play with the learning rate and the number of iterations
- Try different initialization methods and compare the results
- Test other preprocessings (center the data, or divide each row by its standard deviation)</p>
<p>Bibliography:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/">http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/</a></p></li>
<li><p><a class="reference external" href="https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c">https://stats.stackexchange.com/questions/211436/why-do-we-normalize-images-by-subtracting-the-datasets-image-mean-and-not-the-c</a></p></li>
</ul>
</section>
</section>
</div>
</section>
<span id="document-C3"></span><section class="tex2jax_ignore mathjax_ignore" id="shallow-neural-networks">
<span id="shallownn"></span><h2>3 Shallow Neural Networks<a class="headerlink" href="#shallow-neural-networks" title="Link to this heading">#</a></h2>
<p>Build a neural network with one hidden layer, using forward propagation and backpropagation.</p>
<p><strong>Learning Objectives</strong></p>
<ul class="simple">
<li><p>Describe hidden units and hidden layers.</p></li>
<li><p>Use units with a non-linear activation function, such as tanh.</p></li>
<li><p>Implement forward and backward propagation.</p></li>
<li><p>Apply random initialization to your neural network.</p></li>
<li><p>Increase fluency in Deep Learning notations and Neural Network Representations.</p></li>
<li><p>Implement a 2-class classification neural network with a single hidden layer.</p></li>
<li><p>Compute the cross entropy loss.</p></li>
</ul>
<hr class="docutils" />
<section id="neural-networks-overview">
<h3>Neural Networks Overview<a class="headerlink" href="#neural-networks-overview" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/f53vwY_wNKY">Video</a></p>
<p>Let’s give a quick overview of how you implement a neural network. Last chapter, we had learned about logistic regression, and we saw how this model corresponds to the following computation draft.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/3-1.png"><img alt="_images/3-1.png" src="_images/3-1.png" style="height: 280px;" /></a>
</figure>
<p>Whereas previously, the node corresponds to two steps to calculations. The first is compute the <span class="math notranslate nohighlight">\(z\)</span>-value, second is it computes the <span class="math notranslate nohighlight">\(a\)</span> value. In the neural network, the stack of nodes will correspond to a <span class="math notranslate nohighlight">\(z\)</span>-like calculation like this, as well as, an <span class="math notranslate nohighlight">\(a\)</span>-like calculation like that. Then, the second node will correspond to another <span class="math notranslate nohighlight">\(z\)</span> and another <span class="math notranslate nohighlight">\(a\)</span> like calculation.</p>
<p>New notation that we’ll introduce is that we’ll use superscript square bracket one (<span class="math notranslate nohighlight">\([1]\)</span>) to refer to quantities associated with the first stack of nodes, it’s called a layer. Then later, we’ll use superscript square bracket two (<span class="math notranslate nohighlight">\([2]\)</span>) to refer to quantities associated with the second node. That’s called another layer of the neural network.</p>
<p>The superscript square brackets <span class="math notranslate nohighlight">\([\ ]\)</span>, like we have here, are not to be confused with the superscript round brackets <span class="math notranslate nohighlight">\((\ )\)</span> which we use to refer to individual training examples.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(x\)</span> superscript round bracket <span class="math notranslate nohighlight">\(i\)</span> (<span class="math notranslate nohighlight">\(x^i\)</span>) refer to the <span class="math notranslate nohighlight">\(i\)</span>-th training example</p></li>
<li><p>superscript square bracket one and two refer to these different layers; layer one and layer two in this neural network.</p></li>
</ul>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/3-2.png"><img alt="_images/3-2.png" src="_images/3-2.png" style="height: 380px;" /></a>
</figure>
<p>This gives you a quick overview of what a neural network looks like. It’s basically taken logistic regression and repeating it twice.</p>
</section>
<section id="neural-network-representation">
<h3>Neural Network Representation<a class="headerlink" href="#neural-network-representation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/Gk_xaB2Hg7U">Video</a></p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/3-3.png"><img alt="_images/3-3.png" src="_images/3-3.png" style="height: 450px;" /></a>
</figure>
<p>In this example, <span class="math notranslate nohighlight">\(a^{[1]}\)</span> is a four dimensional vector, in <code class="docutils literal notranslate"><span class="pre">Python</span></code> it is the <span class="math notranslate nohighlight">\(4 \times 1\)</span> matrix, or a <span class="math notranslate nohighlight">\(4\)</span> column vector, which shows in the picture. And it’s four dimensional, because in this case we have four nodes, or four units, or four hidden units in this hidden layer.</p>
<p>The hidden layer will have associated with it parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(W^{[1]}, b^{[1]}\)</span>, superscripts square bracket 1 to indicate that these are parameters associated with layer one with the hidden layer.</p></li>
<li><p>We’ll see later that <span class="math notranslate nohighlight">\(W^{[1]}\)</span> will be a <span class="math notranslate nohighlight">\(4 \times 3\)</span> matrix and <span class="math notranslate nohighlight">\(b^{[1]}\)</span> will be a <span class="math notranslate nohighlight">\(4 \times 1\)</span> vector in this example.</p></li>
<li><p>Where the first coordinate four comes from the fact that we have four nodes of our hidden units and a layer, and three comes from the fact that we have three input features.</p></li>
<li><p>In some of the output layers has associated with it also, parameters <span class="math notranslate nohighlight">\(W^{[2]}\)</span> and <span class="math notranslate nohighlight">\(b^{[2]}\)</span>. And it turns out the dimensions of these are <span class="math notranslate nohighlight">\(1 \times 4\)</span> and <span class="math notranslate nohighlight">\(1 \times 1\)</span>. And this <span class="math notranslate nohighlight">\(1 \times 4\)</span> is because the hidden layer has four hidden units, the output layer has just one unit.</p></li>
</ul>
<p>We’ll talk about the dimensions of these matrices in next section.</p>
<div class="hint admonition">
<p class="admonition-title">What the word <strong>hidden</strong> means?</p>
<p>In a neural network that you train with supervised learning, the training set contains values of the inputs <span class="math notranslate nohighlight">\(x\)</span> as well as the target outputs <span class="math notranslate nohighlight">\(y\)</span>. So the term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is, you don’t see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden layer; just because you don’t see it in the training set.</p>
</div>
</section>
<section id="computing-a-neural-network-s-output">
<h3>Computing a Neural Network’s Output<a class="headerlink" href="#computing-a-neural-network-s-output" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/cptdfvi5wK0">Video</a></p>
<p>In the last section, you learned what a single hidden layer neural network looks like. In this section, let’s go through the details of exactly how this neural network computes these outputs. What you see is that it is like logistic regression, but repeated a lot of times.</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/3-4.png"><img alt="_images/3-4.png" src="_images/3-4.png" style="height: 360px;" /></a>
</figure>
<p>Now, we’ve said before that logistic regression, the circle in logistic regression, really represents two steps of computation rows. You compute <span class="math notranslate nohighlight">\(z\)</span> first and then compute the activation as a sigmoid function of <span class="math notranslate nohighlight">\(z\)</span>. So, a neural network just does this a lot more times, like shows in the left. Let’s start by focusing on just one of the nodes in the hidden layer. Let’s look at the first node in the hidden layer.</p>
<p>I’ve grayed out the other nodes for now. So, similar to logistic regression on the left, this nodes in the hidden layer does two steps of computation.</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="_images/3-5.jpg"><img alt="_images/3-5.jpg" src="_images/3-5.jpg" style="height: 380px;" /></a>
</figure>
<p>The first step and think of as the left half of this node, it computes <span class="math notranslate nohighlight">\(z_1^{[1]} = w_1^{[1]T} x + b_1^{[1]}\)</span>, and the notation we’ll use is, these are all quantities associated with the first hidden layer. So, that’s why we have a bunch of square brackets there. This is the first node in the hidden layer. So, that’s why we have the subscript one over there. So first, it does that, and then the second step, is it computes <span class="math notranslate nohighlight">\(a_1^{[1]} = \sigma (z_1^{[1]} )\)</span>, like so. So, for both <span class="math notranslate nohighlight">\(z\)</span> and <span class="math notranslate nohighlight">\(a\)</span>, the notational convention is that the <span class="math notranslate nohighlight">\([l]\)</span> here in superscript square brackets, refers to the layer number, and the <span class="math notranslate nohighlight">\([i]\)</span> subscript here, refers to the nodes in that layer. So, the node we’ll be looking at is <strong>layer one</strong>, that is a hidden layer <strong>node one</strong>.</p>
<p>That little circle, that first node in the neural network, represents carrying out these two steps of computation. Now, let’s look at the second node in the neural network, or the second node in the hidden layer of the neural network. Similar to the logistic regression unit on the left, this little circle represents two steps of computation. The first step is it computes <span class="math notranslate nohighlight">\(z\)</span>. This is still layer one, but now as a second node <span class="math notranslate nohighlight">\(z_2^{[1]} = w_2^{[1]T} x + b_2^{[1]}\)</span>. And then <span class="math notranslate nohighlight">\(a_2^{[1]} = \sigma (z_2^{[1]} )\)</span>.</p>
<p>So, we’ve talked through the first two hidden units in a neural network, having units three and four also represents some computations. So now, if you then go through and write out the corresponding equations for the third and fourth hidden units, you get the following. Now, if you’re actually implementing a neural network, doing this with a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, seems really inefficient. So, what we’re going to do, is take these four equations and vectorize. So, we’re going to start by showing how to compute <span class="math notranslate nohighlight">\(z\)</span> as a vector, it turns out you could do it as follows.</p>
<ul class="simple">
<li><p>stack <span class="math notranslate nohighlight">\(w_i^{[1]T}\)</span> into a matrix. By stacking those four <span class="math notranslate nohighlight">\(w\)</span> vectors together, you end up with a matrix. So, another way to think of this is that we have four logistic regression units there, and each of the logistic regression units, has a corresponding parameter vector, <span class="math notranslate nohighlight">\(w\)</span>. By stacking those four vectors together, you end up with this <span class="math notranslate nohighlight">\(4 \times 3\)</span> matrix. This matrix here which we obtained by stacking the lowercase <span class="math notranslate nohighlight">\(w_1^{[1]}\)</span> through <span class="math notranslate nohighlight">\(w_4^{[1]}\)</span>, we’re going to call this matrix capital <span class="math notranslate nohighlight">\(W^{[1]}\)</span>.</p></li>
<li><p>Computing vector <span class="math notranslate nohighlight">\(z^{[1]}\)</span>, which is taken by stacking up these individuals of <span class="math notranslate nohighlight">\(z\)</span>’s into a column vector. When we’re vectorizing, one of the rules of thumb that might help you navigate this, is that while we have different nodes in the layer, we’ll stack them vertically. So, that’s why we have <span class="math notranslate nohighlight">\(z_1^{[1]}\)</span> through <span class="math notranslate nohighlight">\(z_4^{[1]}\)</span>, those corresponded to four different nodes in the hidden layer, and so we stacked these four numbers vertically to form the vector <span class="math notranslate nohighlight">\(z^{[1]}\)</span>.</p></li>
<li><p>Computing vector <span class="math notranslate nohighlight">\(a^{[1]}\)</span>. So, prior won’t surprise you to see that we’re going to define <span class="math notranslate nohighlight">\(a^{[1]}\)</span>, as just stacking together, those activation values, <span class="math notranslate nohighlight">\(a_1^{[1]}\)</span> through <span class="math notranslate nohighlight">\(a_4^{[1]}\)</span>. So, just take these four values and stack them together in a vector called <span class="math notranslate nohighlight">\(a^{[1]}\)</span>. This is going to be a sigmoid of <span class="math notranslate nohighlight">\(z^{[1]}\)</span>, where this now has been implementation of the sigmoid function that takes in the four elements of <span class="math notranslate nohighlight">\(z\)</span>, and applies the sigmoid function element-wise to it.</p></li>
</ul>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="_images/3-6.png"><img alt="_images/3-6.png" src="_images/3-6.png" style="height: 460px;" /></a>
</figure>
<p>To sumarrise the first layer of the neural network given an input <span class="math notranslate nohighlight">\(x\)</span>:</p>
<figure class="align-default" id="id7">
<a class="reference internal image-reference" href="_images/3-7.png"><img alt="_images/3-7.png" src="_images/3-7.png" style="height: 300px;" /></a>
</figure>
<p>Remember, that we said <span class="math notranslate nohighlight">\(x\)</span> is equal to <span class="math notranslate nohighlight">\(a^{[0]}\)</span>. Just say <span class="math notranslate nohighlight">\(\hat{y}\)</span> is also equal to <span class="math notranslate nohighlight">\(a^{[2]}\)</span>. If you want, you can actually take the <span class="math notranslate nohighlight">\(x\)</span> in the first equation and replace it with <span class="math notranslate nohighlight">\(a^{[0]}\)</span>, since <span class="math notranslate nohighlight">\(a^{[0]}\)</span> is if you want as an alias for the vector of input features, <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>For logistic regression, to implement the output or to implement prediction, you compute <span class="math notranslate nohighlight">\(\mathbf{z} = \mathbf{w}^T \mathbf{x} + \mathbf{b}\)</span>, and <span class="math notranslate nohighlight">\(\hat{y} = a = \sigma{(z)}\)</span>. When you have a neural network with one hidden layer, what you need to implement, is to computer this output is just the four equations shows in the figure on the left.</p>
<p>You can think of this as a vectorized implementation of computing the output of first four logistic regression units in the hidden layer:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{z}^{[1]} &amp;= \mathbf{W}^{[1]} \mathbf{a}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{a}^{[1]} &amp;= \sigma({\mathbf{z}^{[1]}})
\end{aligned}
\end{split}\]</div>
<p>and then the logistic regression in the output layer which is what this does:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{z}^{[2]} &amp;= \mathbf{W}^{[2]} \mathbf{a}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{a}^{[2]} &amp;= \sigma({\mathbf{z}^{[2]}})
\end{aligned}
\end{split}\]</div>
</section>
<section id="vectorizing-across-multiple-examples">
<h3>Vectorizing Across Multiple Examples<a class="headerlink" href="#vectorizing-across-multiple-examples" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/WSu85RCigi0">Video</a></p>
<p>In the last section, you have learned how to compute the prediction on a neural network, given a single training example. In this section, you will learn how to vectorize across multiple training examples. And the outcome will be quite similar to what you learned for logistic regression. Whereby stacking up different training examples in different columns of the matrix, you’d be able to take the equations you had from the previous section. And with very little modification, change them to make the neural network compute the outputs on all the examples on pretty much all at the same time. So let’s see the details on how to do that.</p>
<p>The four equations we have from the previous section of how you compute <span class="math notranslate nohighlight">\(\mathbf{z}^{[1]}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{a}^{[1]}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{z}^{[2]}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{a}^{[2]}\)</span>. And they tell you how, given an input feature back to <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, you can use them to generate <span class="math notranslate nohighlight">\(\mathbf{a}^{[2]} = \hat{\mathbf{y}}\)</span> hat for a single training example.</p>
<div class="math notranslate nohighlight">
\[\mathbf{x} \longrightarrow \mathbf{a}^{[2]} = \hat{\mathbf{y}}\]</div>
<p>Now if you have <span class="math notranslate nohighlight">\(m\)</span> training examples, you need to repeat this process for:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{x}^{(1)} &amp;\longrightarrow \mathbf{a}^{[2](1)} = \hat{\mathbf{y}}^{(1)} \\
\mathbf{x}^{(2)} &amp;\longrightarrow \mathbf{a}^{[2](2)} = \hat{\mathbf{y}}^{(2)}  \\
\vdots \\
\mathbf{x}^{(m)} &amp;\longrightarrow \mathbf{a}^{[2](m)} = \hat{\mathbf{y}}^{(m)}
\end{aligned}
\end{split}\]</div>
<p>So the notation <span class="math notranslate nohighlight">\(\mathbf{a}^{[2](i)}\)</span>:</p>
<ul class="simple">
<li><p>the round bracket <span class="math notranslate nohighlight">\(i\)</span>, <span class="math notranslate nohighlight">\((i)\)</span>, refers to training example <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p>the square bracket 2, <span class="math notranslate nohighlight">\([2]\)</span>, refers to layer 2.</p></li>
</ul>
<p>And so to suggest that if you have an unvectorized implementation and want to compute the predictions of all your training examples, you need to do a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop from <code class="docutils literal notranslate"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">to</span> <span class="pre">m</span></code>. Then basically implement these four equations. What we like to do is vectorize this whole computation, so as to get rid of this <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
<figure class="align-default" id="id8">
<a class="reference internal image-reference" href="_images/3-8.png"><img alt="_images/3-8.png" src="_images/3-8.png" style="height: 380px;" /></a>
</figure>
<p>They are also obtained by taking these vectors and stacking them horizontally. And taking these vectors and stacking them horizontally, in order to get <span class="math notranslate nohighlight">\(\mathbf{Z}^{[2]}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{A}^{[2]}\)</span>. One of the property of this notation that might help you to think about it is that this matrixes say <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span>, and <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>, horizontally we’re going to index across training examples. So that’s why the horizontal index corresponds to different training example, when you sweep from left to right you’re scanning through the training cells. And vertically this vertical index corresponds to different nodes in the neural network.</p>
<ul class="simple">
<li><p>horizontally the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> goes over different training examples.</p></li>
<li><p>vertically the different indices in the matrix <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> corresponds to different hidden units. As you scan down this is your indexing to the hidden units number.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\)</span> where horizontally corresponds to different training examples. And vertically it corresponds to different input features which are really different than those of the input layer of the neural network.</p></li>
</ul>
</section>
<section id="explanation-for-vectorized-implementation">
<h3>Explanation for Vectorized Implementation<a class="headerlink" href="#explanation-for-vectorized-implementation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/PzpiZKWtyRc">Video</a></p>
<figure class="align-default" id="id9">
<a class="reference internal image-reference" href="_images/3-9.png"><img alt="_images/3-9.png" src="_images/3-9.png" style="height: 380px;" /></a>
</figure>
<p>I hope this gives a justification for why we had previously <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]} \mathbf{x}^{(i)} = \mathbf{z}^{[1](i)} \)</span> when we’re looking at single training example at the time. When you took the different training examples and stacked them up in different columns, then the corresponding result is that you end up with the <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>’s also stacked at the columns.</p>
<p>So in this section, I’ve only justified that <span class="math notranslate nohighlight">\(\mathbf{Z}^{[1]} = \mathbf{W}^{[1]} \mathbf{X} + \mathbf{b}^{[1]}\)</span> is a correct vectorization of the first step of the four steps we have in the previous section, but it turns out that a similar analysis allows you to show that the other steps also work on using a very similar logic where if you stack the inputs in columns then after the equation, you get the corresponding outputs also stacked up in columns.</p>
<figure class="align-default" id="id10">
<a class="reference internal image-reference" href="_images/3-10.png"><img alt="_images/3-10.png" src="_images/3-10.png" style="height: 380px;" /></a>
</figure>
<div class="hint admonition">
<p class="admonition-title">Reminder</p>
<p>Because <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is also equal to <span class="math notranslate nohighlight">\(\mathbf{A}^{[0]}\)</span>, remember that the input feature vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> was equal to <span class="math notranslate nohighlight">\(\mathbf{a}^{[0]}\)</span>, so <span class="math notranslate nohighlight">\(\mathbf{x}^{(i)}\)</span> equals \mathbf{a}^{<a class="reference internal" href="#i"><span class="xref myst">0</span></a>}. Then there’s actually a certain symmetry to these equations where this first equation can also be written  <span class="math notranslate nohighlight">\(\mathbf{Z}^{[1]} = \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]}\)</span>.</p>
<p>And so, you see that the first pair of equations and the second pair of equations actually look very similar but just of all of the indices advance by one. So this kind of shows that the different layers of a neural network are roughly doing the same thing or just doing the same computation over and over. And here we have two-layer neural network where we go to a much deeper neural network in next chapter. You see that even deeper neural networks are basically taking these two steps and just doing them even more times than you’re seeing here. So that’s how you can vectorize your neural network across multiple training examples.</p>
</div>
</section>
<section id="activation-functions">
<h3>Activation Functions<a class="headerlink" href="#activation-functions" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/pFa8lh3Rqmo">Video</a></p>
<ol class="arabic simple">
<li><p><strong>sigmoid function</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[g(z) = \dfrac{1}{1+e^{-z}}\]</div>
<ul class="simple">
<li><p>the sigmoid function goes between zero and one.</p></li>
<li><p>I would say never use this except for the output layer if you’re doing binary classification or maybe almost never use this. And the reason I almost never use this is because the tanh is pretty much strictly superior.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>tanh function</strong> (tangent function / hyperbolic tangent function)</p></li>
</ol>
<div class="math notranslate nohighlight">
\[g(z) = \dfrac{e^z - e^{-z}}{e^z + e^{-z}}\]</div>
<ul class="simple">
<li><p>the tanh function goes between <span class="math notranslate nohighlight">\((-1, 1)\)</span>.</p></li>
<li><p>This almost always works better than the sigmoid function because with values between plus one and minus one, the mean of the activations that come out of your hidden layer are closer to having a zero mean. And so just as sometimes when you train a learning algorithm, you might center the data and your data have zero mean using a <strong>tanh</strong> instead of a sigmoid function. Kind of has the effect of centering your data so that the mean of your data is close to 0 rather than 0.5. And this actually makes learning for the next layer a little bit easier.</p></li>
<li><p>The one <em><strong>exception</strong></em> is for the <strong>output layer</strong> because if <span class="math notranslate nohighlight">\(y\)</span> is either zero or one (<span class="math notranslate nohighlight">\(y \in (0,1)\)</span>), then it makes sense for <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be a number that you want to output that’s between zero and one rather than between -1 and 1. So the one exception where I would use the sigmoid activation function is when you’re using binary classification. In which case you might use the sigmoid activation function for the upper layer.</p></li>
<li><p>One of the downsides of both the sigmoid function and the tanh function is that if <span class="math notranslate nohighlight">\(z\)</span> is either very large or very small, then the gradient of the derivative of the slope of this function becomes very small. So if <span class="math notranslate nohighlight">\(z\)</span> is very large or <span class="math notranslate nohighlight">\(z\)</span> is very small, the slope of the function either ends up being close to zero and so this can slow down gradient descent. So one other choice that is very popular in machine learning is what’s called the rectified linear unit.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>ReLU function</strong> (Rectified Linear unit)</p></li>
</ol>
<div class="math notranslate nohighlight">
\[g(z) = \text{max}(0, z)\]</div>
<ul class="simple">
<li><p>So the derivative is one so long as <span class="math notranslate nohighlight">\(z\)</span> is positive and derivative or the slope is zero when <span class="math notranslate nohighlight">\(z\)</span> is negative. If you’re implementing this, technically the derivative when <span class="math notranslate nohighlight">\(z\)</span> is exactly zero is not well defined. But when you implement this in the computer, the odds that you get exactly <span class="math notranslate nohighlight">\(z\)</span> equals <span class="math notranslate nohighlight">\(0.000000000000000\)</span> is very small. So you don’t need to worry about it. In practice, you could pretend a derivative when <span class="math notranslate nohighlight">\(z\)</span> is equal to zero, you can pretend is either one or zero. And you can work just fine. So the fact is not differentiable.</p></li>
<li><p>One disadvantage of the ReLU is that the derivative is equal to zero when <span class="math notranslate nohighlight">\(z\)</span> is negative. In practice this works just fine. But there is another version of the value called the Leaky ReLU.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p><strong>Leaky ReLU function</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[g(z) = \text{max}(0.01z, z)\]</div>
<ul class="simple">
<li><p>instead of it being zero when <span class="math notranslate nohighlight">\(z\)</span> is negative, it just takes a slight slope.</p></li>
<li><p>And you might say, why is that constant <span class="math notranslate nohighlight">\(0.01\)</span>? Well, you can also make that an another parameter of the learning algorithm. And some people say that works even better, but how they see people do that. So, if you feel like trying it in your application, please feel free to do so. And you can just see how it works and how well it works, and stick with it if it gives you a good result.</p></li>
<li><p>This usually works better than the ReLU activation function. Although, it’s just not used as much in practice. Either one should be fine. Although, if you had to pick one, I usually just use the ReLU.</p></li>
<li><p>The advantage of both the ReLU and the Leaky ReLU is that for a lot of the space of <span class="math notranslate nohighlight">\(z\)</span>, the derivative of the activation function, the slope of the activation function is very different from zero. And so in practice, using the value activation function, your neural network will often learn much faster than when using the tanh or the sigmoid activation function. And the main reason is that there is less of this effect of the slope of the function going to zero, which slows down learning. And I know that for half of the range of <span class="math notranslate nohighlight">\(z\)</span> in ReLU, the slope for value is zero. But in practice, enough of your hidden units will have <span class="math notranslate nohighlight">\(z\)</span> greater than zero. So learning can still be quite fast for most training examples.</p></li>
</ul>
<figure class="align-default" id="id11">
<a class="reference internal image-reference" href="_images/3-11.png"><img alt="_images/3-11.png" src="_images/3-11.png" style="height: 360px;" /></a>
</figure>
<ul class="simple">
<li><p>The activation functions can be different for different layers. And sometimes to denote that the activation functions are different for different layers, we might use these square brackets superscripts as well to indicate that <span class="math notranslate nohighlight">\(g^{[1]}(z)\)</span> may be different than <span class="math notranslate nohighlight">\(g^{[2]}(z)\)</span>.</p></li>
</ul>
<div class="hint admonition">
<p class="admonition-title"><em><strong>Rules of thumb for choosing activation functions</strong></em></p>
<p>If your output is zero one value, if you’re using binary classification, then the sigmoid activation function is very natural choice for the output layer. And then for all other units, the ReLU or the rectified linear unit is increasingly the default choice of activation function. So if you’re not sure what to use for your hidden layer, I would just use the ReLU activation function, is what you see most people using these days. Although sometimes people also use the tanh activation function.</p>
<p>One of the things we will see in deep learning is that you often have a lot of different choices in how you build your neural network. Ranging from a number of hidden units to the choices activation function, to how you initialize the ways. A lot of choices like that. And it turns out that it is sometimes difficult to get good guidelines for exactly what will work best for your problem. So throughout these courses, I’ll keep on giving you a sense of what I see in the industry in terms of what’s more or less popular. But for your application, with your applications, idiosyncrasies is actually very difficult to know in advance exactly what will work best. So common piece of advice would be, if you’re not sure which one of these activation functions work best, try them all. And evaluate on a holdout validation set or a development set. And see which one works better and then go of that. And I think that by testing these different choices for your application, you would be better at future proofing your neural network architecture against the idiosyncracies problems. As well as evolutions of the algorithms rather than, if I were to tell you always use a value activation and don’t use anything else. That just may or may not apply for whatever problem you end up working on.</p>
</div>
</section>
<section id="why-do-you-need-non-linear-activation-functions">
<h3>Why do you need Non-Linear Activation Functions?<a class="headerlink" href="#why-do-you-need-non-linear-activation-functions" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/cJmupX30PCs">Video</a></p>
</section>
<section id="derivatives-of-activation-functions">
<h3>Derivatives of Activation Functions<a class="headerlink" href="#derivatives-of-activation-functions" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/WLgjYfNifGk">Video</a></p>
<ol class="arabic simple">
<li><p><strong>sigmoid function</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[a = g(z) = \dfrac{1}{1+e^{-z}}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
g'(z) 
&amp;= \dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \\
&amp;= \dfrac{-e^{-z}}{(1+e^{-z})^2} \\
&amp;= \dfrac{1}{1+e^{-z}} \cdot \dfrac{-e^{-z}}{1+e^{-z}} \\
&amp;= \dfrac{1}{1+e^{-z}} \cdot \Big( 1 - \dfrac{1}{1+e^{-z}} \Big) \\
&amp;= g(z) \Big(1-g(z) \Big) \\
&amp;= a(1-a)
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(z = 10\)</span>, <span class="math notranslate nohighlight">\(g(z) \approx 1\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 1\times(1-1) \approx 0\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(z = -10\)</span>, <span class="math notranslate nohighlight">\(g(z) \approx 0\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 0\times(1-0) \approx 0\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(z = 0\)</span>, <span class="math notranslate nohighlight">\(g(z) = \dfrac{1}{2}\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) = \dfrac{1}{2}\times(1-\dfrac{1}{2}) = \dfrac{1}{4}\)</span></p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>tanh function</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[a = g(z) = \dfrac{e^z - e^{-z}}{e^z + e^{-z}}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
g'(z) 
&amp;= \dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \\
&amp;= \dfrac{(e^z + e^{-z})^2 - (e^z - e^{-z})^2}{(e^z + e^{-z})^2} \\
&amp;= 1- \dfrac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2} \\
&amp;= 1- \Big(g(z) \Big)^2 \\
&amp;= 1-a^2
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(z = 10\)</span>, <span class="math notranslate nohighlight">\(g(z) \approx 1\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 0\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(z = -10\)</span>, <span class="math notranslate nohighlight">\(g(z) \approx -1\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 0\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(z = 0\)</span>, <span class="math notranslate nohighlight">\(g(z) = 0\)</span>, <span class="math notranslate nohighlight">\(\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) = 1\)</span></p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>ReLU function</strong> and <strong>Leaky ReLU function</strong></p></li>
</ol>
<div class="math notranslate nohighlight">
\[g(z) = \text{max}(0, z)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
g'(z) =
\begin{cases}
0,\quad \mbox{if} z &lt; 0\\
1,\quad \mbox{if} z \geq 0
\end{cases}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[g(z) = \text{max}(0.01z, z)\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
g'(z) =
\begin{cases}
0.01,\quad &amp;\mbox{if} z &lt; 0\\
1,\quad &amp;\mbox{if} z \geq 0
\end{cases}
\end{split}\]</div>
</section>
<section id="gradient-descent-for-neural-networks">
<h3>Gradient Descent for Neural Networks<a class="headerlink" href="#gradient-descent-for-neural-networks" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/CT21tIhFP-E">Video</a></p>
<p>Parameters: <span class="math notranslate nohighlight">\(\mathbf{W}^{[1]}, \ \mathbf{b}^{[1]}, \ \mathbf{W}^{[2]}, \ \mathbf{b}^{[2]}\)</span></p>
<p>Dimensions: <span class="math notranslate nohighlight">\(n_x = n^{[0]}\)</span>, <span class="math notranslate nohighlight">\(n^{[1]}\)</span>, <span class="math notranslate nohighlight">\(n^{[2]}\)</span>;</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{[1]}\)</span>: <span class="math notranslate nohighlight">\((n^{[1]}, n^{[0]})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}^{[1]}\)</span>: <span class="math notranslate nohighlight">\((n^{[1]}, 1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}^{[2]}\)</span>: <span class="math notranslate nohighlight">\((n^{[2]}, n^{[1]})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}^{[2]}\)</span>: <span class="math notranslate nohighlight">\((n^{[2]}, 1)\)</span></p></li>
</ul>
<p>Cost function: <span class="math notranslate nohighlight">\(\boldsymbol{J}(\mathbf{W}^{[1]}, \ \mathbf{b}^{[1]}, \ \mathbf{W}^{[2]}, \ \mathbf{b}^{[2]}) = \dfrac{1}{m} \sum_{i=1}^{m}  \mathcal{L}(\hat{y}, y) \)</span></p>
<p><strong>Forward propogation</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{[1]} &amp;= \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{A}^{[1]} &amp;= g^{[1]}({\mathbf{Z}^{[1]}}) \\
\mathbf{Z}^{[2]} &amp;= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{A}^{[2]} &amp;= g^{[2]}({\mathbf{Z}^{[2]}}) = \sigma({\mathbf{Z}^{[2]}})
\end{aligned}
\end{split}\]</div>
<p><strong>Backward propogation</strong>:</p>
<figure class="align-default" id="id12">
<a class="reference internal image-reference" href="_images/3-12.png"><img alt="_images/3-12.png" src="_images/3-12.png" style="height: 300px;" /></a>
</figure>
</section>
<section id="backpropagation-intuition-optional">
<h3>Backpropagation Intuition (Optional)<a class="headerlink" href="#backpropagation-intuition-optional" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/MXLJQ0qIRxM">Video</a></p>
</section>
<section id="random-initialization">
<h3>Random Initialization<a class="headerlink" href="#random-initialization" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/BSAWBJRTjuM">Video</a></p>
<p>When you change your neural network, it’s important to initialize the weights randomly. For logistic regression, it was okay to initialize the weights to zero. But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent, it won’t work.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="hint admonition">
<p class="admonition-title">Where did the constant <span class="math notranslate nohighlight">\(0.01\)</span> comes from and why is it? Why not put the number <span class="math notranslate nohighlight">\(100\)</span> or <span class="math notranslate nohighlight">\(1000\)</span>?</p>
<p>If <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is too large, you’re more likely to end up even at the very start of training, with very large values of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning.</p>
<ul class="simple">
<li><p>Turns out that we usually prefer to initialize the weights to very small random values. Because if you are using a tanh or sigmoid activation function, or the other sigmoid, even just at the output layer. If the weights are too large, then when you compute the activation values, remember that <span class="math notranslate nohighlight">\(\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{a}^{[0]} + \mathbf{b}^{[1]}\)</span>. And then <span class="math notranslate nohighlight">\(\mathbf{a}^{[1]} \)</span> is the activation function applied to <span class="math notranslate nohighlight">\(\mathbf{z}^{[1]}\)</span>. So if <span class="math notranslate nohighlight">\(\mathbf{W}\)</span> is very big, <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> will be very, or at least some values of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> will be either very large or very small. And so in that case, you’re more likely to end up at these <strong>flat parts</strong> of the tanh function or the sigmoid function, where the slope or the gradient is very small. Meaning that gradient descent will be very slow. So learning was very slow.</p></li>
</ul>
<p>If you don’t have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue. But if you’re doing binary classification, and your output unit is a sigmoid function, then you just don’t want the initial parameters to be too large. So that’s why multiplying by 0.01 would be something reasonable to try, or any other small number.</p>
<figure class="align-default" id="id13">
<a class="reference internal image-reference" href="_images/3-13.png"><img alt="_images/3-13.png" src="_images/3-13.png" style="height: 210px;" /></a>
</figure>
</div>
<p>When you’re training a neural network with just one hidden layer, it is a relatively shallow neural network, without too many hidden layers. Set it to 0.01 will probably work okay. But when you’re training a very very deep neural network, then you might want to pick a different constant than 0.01. And in next chapter material, we’ll talk a little bit about how and when you might want to choose a different constant than 0.01. But either way, it will usually end up being a relatively small number.</p>
</section>
<section id="quiz">
<h3>Quiz<a class="headerlink" href="#quiz" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>Which of the following are true? (Check all that apply.)</p>
<p>A. <span class="math notranslate nohighlight">\(W^{[1]}\)</span> is a matrix with rows equal to the transpose of the parameter vectors of the first layer.</p>
<p>B. <span class="math notranslate nohighlight">\(W^{[1]}\)</span> is a matrix with rows equal to the parameter vectors of the first layer.</p>
<p>C. <span class="math notranslate nohighlight">\(W_1\)</span> is a matrix with rows equal to the parameter vectors of the first layer.</p>
<p>D. <span class="math notranslate nohighlight">\(w^{[4]}_3\)</span> is the column vector of parameters of the fourth layer and third neuron.</p>
<p>E. <span class="math notranslate nohighlight">\(w^{[4]}_3\)</span> is the column vector of parameters of the third layer and fourth neuron.</p>
<p>F. <span class="math notranslate nohighlight">\(w^{[4]}_3\)</span> is the row vector of parameters of the fourth layer and third neuron.</p>
</li>
<li><p><strong>True/False</strong> The sigmoid function is only mentioned as an activation function for historical reasons. The tanh is always preferred without exceptions in all the layers of a Neural Network.   __________</p></li>
<li><p>Which of these is a correct vectorized implementation of forward propagation for layer <span class="math notranslate nohighlight">\(l\)</span>, where <span class="math notranslate nohighlight">\(1 \le l \le L\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(\begin{aligned}
Z^{[l]} &amp;= W^{[l-1]}A^{[l]} + b^{[l-1]} \\
A^{[l]} &amp;= g^{[l]}(Z^{[l]})
\end{aligned}\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\begin{aligned}
Z^{[l]} &amp;= W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} &amp;= g^{[l]}(Z^{[l]})
\end{aligned}\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\begin{aligned}
Z^{[l]} &amp;= W^{[l]}A^{[l]} + b^{[l]} \\
A^{[l+1]} &amp;= g^{[l]}(Z^{[l]})
\end{aligned}\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\begin{aligned}
Z^{[l]} &amp;= W^{[l]}A^{[l]} + b^{[l]} \\
A^{[l+1]} &amp;= g^{[l+1]}(Z^{[l]})
\end{aligned}\)</span></p>
</li>
<li><p><strong>True/False</strong> The use of the ReLU activation function is becoming more rare because the ReLU function has no derivative for <span class="math notranslate nohighlight">\(c = 0\)</span>.  __________</p></li>
<li><p>Consider the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span><span class="mi">1</span> <span class="p">)</span> 
</pre></div>
</div>
<p>What will be the <span class="math notranslate nohighlight">\(y.shape\)</span>?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">1)</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">)</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">5)</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">(5,</span> <span class="pre">)</span></code></p>
</li>
<li><p>Suppose you have built a neural network with one hidden layer and tanh as activation function for the hidden layers. Which of the following is a best option to initialize the weights?</p>
<p>A. Initialize all weights to a single number chosen randomly.</p>
<p>B. Initialize all weights to 0.</p>
<p>C. Initialize the weights to large random numbers.</p>
<p>D. Initialize the weights to small random numbers.</p>
</li>
<li><p><strong>True/False</strong> A single output and single layer neural network that uses the sigmoid function as activation is equivalent to the logistic regression.   __________</p></li>
<li><p>You have built a network using the tanh activation for all the hidden units. You initialize the weights to relatively large values, using <code class="docutils literal notranslate"><span class="pre">np.random.randn(..,</span> <span class="pre">..)*1000</span></code>. What will happen?</p>
<p>A. This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.</p>
<p>B. This will cause the inputs of the tanh to also be very large, causing the units to be “highly activated” and thus speed up learning compared to if the weights had to start from small values.</p>
<p>C. So long as you initialize the weights randomly gradient descent is not affected by whether the weights are large or small.</p>
<p>D. This will cause the inputs of the tanh to also be very large, thus causing gradients to also become large. You therefore have to set <span class="math notranslate nohighlight">\(\alpha\)</span> to a very small value to prevent divergence; this will slow down learning.</p>
</li>
<li><p>Consider the following 1 hidden layer neural network:</p>
<figure class="align-default" id="q9">
<a class="reference internal image-reference" href="_images/3-q9.png"><img alt="_images/3-q9.png" src="_images/3-q9.png" style="height: 200px;" /></a>
</figure>
<p>Which of the following statements are True? (Check all that apply).</p>
<p>A. <span class="math notranslate nohighlight">\(b^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((4,1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(b^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((2,1)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(b^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((4,1)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(W^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((2,4)\)</span></p>
<p>E. <span class="math notranslate nohighlight">\(W^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((1,4)\)</span></p>
<p>F. <span class="math notranslate nohighlight">\(W^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((4,2)\)</span></p>
<p>G. <span class="math notranslate nohighlight">\(b^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((1,1)\)</span></p>
<p>H. <span class="math notranslate nohighlight">\(W^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((4,1)\)</span></p>
</li>
<li><p>Consider the following 1 hidden layer neural network:</p>
<figure class="align-default" id="q10">
<a class="reference internal image-reference" href="_images/3-q10.png"><img alt="_images/3-q10.png" src="_images/3-q10.png" style="height: 460px;" /></a>
</figure>
<p>What are the dimensions of <span class="math notranslate nohighlight">\(Z^{[1]}\)</span>  and <span class="math notranslate nohighlight">\(A^{[1]}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(Z^{[1]}\)</span> and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> are <span class="math notranslate nohighlight">\((2,m)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(Z^{[1]}\)</span> and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> are <span class="math notranslate nohighlight">\((4,1)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(Z^{[1]}\)</span> and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> are <span class="math notranslate nohighlight">\((4,m)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(Z^{[1]}\)</span> and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> are <span class="math notranslate nohighlight">\((2,1)\)</span></p>
</li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Click here for answers!</p>
<ol class="arabic">
<li><p>AD </br></p>
<p>A. We construct <span class="math notranslate nohighlight">\(W^{[1]}\)</span> stacking the parameter vectors <span class="math notranslate nohighlight">\(w^{[1]_j}\)</span> of all the neurons of the first layer. </br></p>
<p>D. The vector <span class="math notranslate nohighlight">\(w^{[i]_j}\)</span> is the column vector of parameters of the i-th layer and j-th neuron of that layer. </br></p>
</li>
<li><p>False </br></p>
<p>Although the tanh almost always works better than the sigmoid function when used in hidden layers, thus is always proffered as activation function, the exception is for the output layer in classification problems. </br></p>
</li>
<li><p>B </br></p></li>
<li><p>False </br></p>
<p>Although the ReLU function has no derivative at <span class="math notranslate nohighlight">\(c = 0\)</span> this rarely causes any problems in practice. Moreover it has become the default activation function in many cases, as explained in the lectures.  </br></p>
</li>
<li><p>B </br></p>
<p>B. Yes. By using <code class="docutils literal notranslate"><span class="pre">axis=1</span></code> the sum is computed over each row of the array, thus the resulting array is a column vector with 4 entries. Since the option <code class="docutils literal notranslate"><span class="pre">keepdims</span></code> was not used the array doesn’t keep the second dimension. </br></p>
</li>
<li><p>D </br></p>
<p>D. The use of random numbers helps to “break the symmetry” between all the neurons allowing them to compute different functions. When using small random numbers the values <span class="math notranslate nohighlight">\(z^{[k]}\)</span> will be close to zero thus the activation values will have a larger gradient speeding up the training process. </br></p>
</li>
<li><p>True </br></p>
<p>The logistic regression model can be expressed by <span class="math notranslate nohighlight">\(\hat{y} = \sigma(Wx+b)\)</span>. This isthe same as <span class="math notranslate nohighlight">\(a^{[1]}=\sigma(W^{[1]}X+b)\)</span>. </br></p>
</li>
<li><p>A </br></p>
<p>A. tanh becomes flat for large values; this leads its gradient to be close to zero. This slows down the optimization algorithm. </br></p>
</li>
<li><p>CEFG </br></p></li>
<li><p>A </br></p>
<p>A. Yes. The <span class="math notranslate nohighlight">\(Z^{[1]}\)</span> and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> are calculated over a batch of training examples. The number of columns in <span class="math notranslate nohighlight">\(Z^{[1]}\)</span>  and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> is equal to the number of examples in the batch, <span class="math notranslate nohighlight">\(m\)</span>. And the number of rows in <span class="math notranslate nohighlight">\(Z^{[1]}\)</span>  and <span class="math notranslate nohighlight">\(A^{[1]}\)</span> is equal to the number of neurons in the first layer. </br></p>
</li>
</ol>
</div>
</section>
<div class="toctree-wrapper compound">
<span id="document-C3_Practical_Test"></span><section class="tex2jax_ignore mathjax_ignore" id="practical-2-planar-data-classification-with-one-hidden-layer">
<h3>Practical 2: Planar data classification with one hidden layer<a class="headerlink" href="#practical-2-planar-data-classification-with-one-hidden-layer" title="Link to this heading">#</a></h3>
<p>Welcome to your week 3 programming assignment! It’s time to build your first neural network, which will have one hidden layer. Now, you’ll notice a big difference between this model and the one you implemented previously using logistic regression.</p>
<p>By the end of this assignment, you’ll be able to:</p>
<ul class="simple">
<li><p>Implement a 2-class classification neural network with a single hidden layer</p></li>
<li><p>Use units with a non-linear activation function, such as tanh</p></li>
<li><p>Compute the cross entropy loss</p></li>
<li><p>Implement forward and backward propagation</p></li>
</ul>
<section id="important-note-on-submission-to-the-autograder">
<h4>Important Note on Submission to the AutoGrader<a class="headerlink" href="#important-note-on-submission-to-the-autograder" title="Link to this heading">#</a></h4>
<p>Before submitting your assignment to the AutoGrader, please make sure you are not doing the following:</p>
<ol class="arabic simple">
<li><p>You have not added any <em>extra</em> <code class="docutils literal notranslate"><span class="pre">print</span></code> statement(s) in the assignment.</p></li>
<li><p>You have not added any <em>extra</em> code cell(s) in the assignment.</p></li>
<li><p>You have not changed any of the function parameters.</p></li>
<li><p>You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.</p></li>
<li><p>You are not changing the assignment code where it is not required, like creating <em>extra</em> variables.</p></li>
</ol>
<p>If you do any of the following, you will get something like, <code class="docutils literal notranslate"><span class="pre">Grader</span> <span class="pre">Error:</span> <span class="pre">Grader</span> <span class="pre">feedback</span> <span class="pre">not</span> <span class="pre">found</span></code> (or similarly unexpected) error upon submitting your assignment. Before asking for help/debugging the errors in your assignment, check for these first. If this is the case, and you don’t remember the changes you have made, you can get a fresh copy of the assignment by following these <a class="reference external" href="https://www.coursera.org/learn/neural-networks-deep-learning/supplement/iLwon/h-ow-to-refresh-your-workspace">instructions</a>.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="packages">
<h3>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h3>
<p>First import all the packages that you will need during this assignment.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.numpy.org/">numpy</a> is the fundamental package for scientific computing with Python.</p></li>
<li><p><a class="reference external" href="http://scikit-learn.org/stable/">sklearn</a> provides simple and efficient tools for data mining and data analysis.</p></li>
<li><p><a class="reference external" href="http://matplotlib.org">matplotlib</a> is a library for plotting graphs in Python.</p></li>
<li><p>testCases provides some test examples to assess the correctness of your functions</p></li>
<li><p>planar_utils provide various useful functions used in this assignment</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Package imports</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">testCases_v2_c1w3</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">public_tests_c1w3</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span>
<span class="kn">from</span> <span class="nn">planar_utils_c1w3</span> <span class="kn">import</span> <span class="n">plot_decision_boundary</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">load_planar_dataset</span><span class="p">,</span> <span class="n">load_extra_datasets</span>

<span class="o">%</span><span class="k">matplotlib</span> inline

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="load-the-dataset">
<h3>2 - Load the Dataset<a class="headerlink" href="#load-the-dataset" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">load_planar_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. In other words, we want the classifier to define regions as either red or blue.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the data:</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7ce848841ec753d2a37852d93a82cd50688e675dd1f51ec1ff6943c563c545a5.png" src="_images/7ce848841ec753d2a37852d93a82cd50688e675dd1f51ec1ff6943c563c545a5.png" />
</div>
</div>
<p>You have:</p>
<ul class="simple">
<li><p>a numpy-array (matrix) <code class="docutils literal notranslate"><span class="pre">X</span></code> that contains your features <code class="docutils literal notranslate"><span class="pre">(x1,</span> <span class="pre">x2)</span></code></p></li>
<li><p>a numpy-array (vector) <code class="docutils literal notranslate"><span class="pre">Y</span></code> that contains your labels <code class="docutils literal notranslate"><span class="pre">(red:0,</span> <span class="pre">blue:1)</span></code>.</p></li>
</ul>
<p>First, get a better sense of what your data is like.</p>
<section id="exercise-1">
<h4>Exercise 1<a class="headerlink" href="#exercise-1" title="Link to this heading">#</a></h4>
<p>How many training examples do you have? In addition, what is the <code class="docutils literal notranslate"><span class="pre">shape</span></code> of the variables <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">Y</span></code>?</p>
<p><strong>Hint</strong>: How do you get the shape of a numpy array? <a class="reference external" href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html">(help)</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># (≈ 3 lines of code)</span>
<span class="c1"># shape_X = ...</span>
<span class="c1"># shape_Y = ...</span>
<span class="c1"># training set size</span>
<span class="c1"># m = ...</span>
<span class="c1"># YOUR CODE STARTS HERE</span>

<span class="n">shape_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">shape_Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">X</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># YOUR CODE ENDS HERE</span>

<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of X is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape_X</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;The shape of Y is: &#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">shape_Y</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;I have m = </span><span class="si">%d</span><span class="s1"> training examples!&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">m</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The shape of X is: (2, 400)
The shape of Y is: (1, 400)
I have m = 400 training examples!
</pre></div>
</div>
</div>
</div>
</section>
<section id="simple-logistic-regression">
<h4>3 - Simple Logistic Regression<a class="headerlink" href="#simple-logistic-regression" title="Link to this heading">#</a></h4>
<p>Before building a full neural network, let’s check how logistic regression performs on this problem. You can use sklearn’s built-in functions for this. Run the code below to train a logistic regression classifier on the dataset.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the logistic regression classifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegressionCV</span><span class="p">();</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>You can now plot the decision boundary of these models! Run the code below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the decision boundary for logistic regression</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic Regression&quot;</span><span class="p">)</span>

<span class="c1"># Print accuracy</span>
<span class="n">LR_predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Accuracy of logistic regression: </span><span class="si">%d</span><span class="s1"> &#39;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">LR_predictions</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span><span class="mi">1</span><span class="o">-</span><span class="n">LR_predictions</span><span class="p">))</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span> <span class="o">+</span>
       <span class="s1">&#39;% &#39;</span> <span class="o">+</span> <span class="s2">&quot;(percentage of correctly labelled datapoints)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints)
</pre></div>
</div>
<img alt="_images/7555e38bfec3058ac37800d3ab99da0428dc13156fa830d98f089b719cdb9220.png" src="_images/7555e38bfec3058ac37800d3ab99da0428dc13156fa830d98f089b719cdb9220.png" />
</div>
</div>
<p><strong>Interpretation</strong>: The dataset is not linearly separable, so logistic regression doesn’t perform well. Hopefully a neural network will do better. Let’s try this now!</p>
</section>
<section id="neural-network-model">
<h4>4 - Neural Network model<a class="headerlink" href="#neural-network-model" title="Link to this heading">#</a></h4>
<p>Logistic regression didn’t work well on the flower dataset. Next, you’re going to train a Neural Network with a single hidden layer and see how that handles the same problem.</p>
<p><strong>The model</strong>:</p>
<figure class="align-default" id="test-1">
<a class="reference internal image-reference" href="_images/classification_kiank.png"><img alt="_images/classification_kiank.png" src="_images/classification_kiank.png" style="width: 600px; height: 360px;" /></a>
</figure>
<p><strong>Mathematically</strong>:</p>
<p>For one example <span class="math notranslate nohighlight">\(x^{(i)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\tag{1}\]</div>
<div class="math notranslate nohighlight">
\[a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}\]</div>
<div class="math notranslate nohighlight">
\[z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\tag{3}\]</div>
<div class="math notranslate nohighlight">
\[\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}y^{(i)}_{\text{prediction}} = 
\begin{cases} 
1 &amp; \mbox{if } a^{[2](i)} &gt; 0.5 \\ 
0 &amp; \mbox{otherwise } 
\end{cases}\end{split}\]</div>
<p>Given the predictions on all the examples, you can also compute the cost <span class="math notranslate nohighlight">\(J\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right)  \large  \right) \small \tag{6}\]</div>
<p><strong>Reminder</strong>: The general methodology to build a Neural Network is to:
1. Define the neural network structure ( # of input units,  # of hidden units, etc).
2. Initialize the model’s parameters
3. Loop:
- Implement forward propagation
- Compute loss
- Implement backward propagation to get the gradients
- Update parameters (gradient descent)</p>
<p>In practice, you’ll often build helper functions to compute steps 1-3, then merge them into one function called <code class="docutils literal notranslate"><span class="pre">nn_model()</span></code>. Once you’ve built <code class="docutils literal notranslate"><span class="pre">nn_model()</span></code> and learned the right parameters, you can make predictions on new data.</p>
<section id="defining-the-neural-network-structure">
<h5>4.1 - Defining the neural network structure<a class="headerlink" href="#defining-the-neural-network-structure" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-2-layer-sizes">
<h5>Exercise 2 - layer_sizes<a class="headerlink" href="#exercise-2-layer-sizes" title="Link to this heading">#</a></h5>
<p>Define three variables:</p>
<ul class="simple">
<li><p>n_x: the size of the input layer</p></li>
<li><p>n_h: the size of the hidden layer (<strong>set this to 4, as <code class="docutils literal notranslate"><span class="pre">n_h</span> <span class="pre">=</span> <span class="pre">4</span></code>, but only for this Exercise 2</strong>)</p></li>
<li><p>n_y: the size of the output layer</p></li>
</ul>
<p><strong>Hint</strong>: Use shapes of X and Y to find n_x and n_y. Also, hard code the hidden layer size to be 4.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: layer_sizes</span>

<span class="k">def</span> <span class="nf">layer_sizes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input dataset of shape (input size, number of examples)</span>
<span class="sd">    Y -- labels of shape (output size, number of examples)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    n_x -- the size of the input layer</span>
<span class="sd">    n_h -- the size of the hidden layer</span>
<span class="sd">    n_y -- the size of the output layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1">#(≈ 3 lines of code)</span>
    <span class="c1"># n_x = ... </span>
    <span class="c1"># n_h = ...</span>
    <span class="c1"># n_y = ... </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">n_x</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_h</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">n_y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span> <span class="o">=</span> <span class="n">layer_sizes_test_case</span><span class="p">()</span>
<span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The size of the input layer is: n_x = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The size of the hidden layer is: n_h = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_h</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The size of the output layer is: n_y = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n_y</span><span class="p">))</span>

<span class="n">layer_sizes_test</span><span class="p">(</span><span class="n">layer_sizes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The size of the input layer is: n_x = 5
The size of the hidden layer is: n_h = 4
The size of the output layer is: n_y = 2
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="initialize-the-model-s-parameters">
<h5>4.2 - Initialize the model’s parameters<a class="headerlink" href="#initialize-the-model-s-parameters" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-3-initialize-parameters">
<h5>Exercise 3 -  initialize_parameters<a class="headerlink" href="#exercise-3-initialize-parameters" title="Link to this heading">#</a></h5>
<p>Implement the function <code class="docutils literal notranslate"><span class="pre">initialize_parameters()</span></code>.</p>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p>Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed.</p></li>
<li><p>You will initialize the weights matrices with random values.</p>
<ul>
<li><p>Use: <code class="docutils literal notranslate"><span class="pre">np.random.randn(a,b)</span> <span class="pre">*</span> <span class="pre">0.01</span></code> to randomly initialize a matrix of shape (a,b).</p></li>
</ul>
</li>
<li><p>You will initialize the bias vectors as zeros.</p>
<ul>
<li><p>Use: <code class="docutils literal notranslate"><span class="pre">np.zeros((a,b))</span></code> to initialize a matrix of shape (a,b) with zeros.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters</span>

<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Argument:</span>
<span class="sd">    n_x -- size of the input layer</span>
<span class="sd">    n_h -- size of the hidden layer</span>
<span class="sd">    n_y -- size of the output layer</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    params -- python dictionary containing your parameters:</span>
<span class="sd">                    W1 -- weight matrix of shape (n_h, n_x)</span>
<span class="sd">                    b1 -- bias vector of shape (n_h, 1)</span>
<span class="sd">                    W2 -- weight matrix of shape (n_y, n_h)</span>
<span class="sd">                    b2 -- bias vector of shape (n_y, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>    
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># W1 = ...</span>
    <span class="c1"># b1 = ...</span>
    <span class="c1"># W2 = ...</span>
    <span class="c1"># b2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_h</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;W1&quot;</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s2">&quot;b1&quot;</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s2">&quot;W2&quot;</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s2">&quot;b2&quot;</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span> <span class="o">=</span> <span class="n">initialize_parameters_test_case</span><span class="p">()</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="n">initialize_parameters_test</span><span class="p">(</span><span class="n">initialize_parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[-0.00416758 -0.00056267]
 [-0.02136196  0.01640271]
 [-0.01793436 -0.00841747]
 [ 0.00502881 -0.01245288]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01057952 -0.00909008  0.00551454  0.02292208]]
b2 = [[0.]]
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-loop">
<h5>4.3 - The Loop<a class="headerlink" href="#the-loop" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-4-forward-propagation">
<h5>Exercise 4 - forward_propagation<a class="headerlink" href="#exercise-4-forward-propagation" title="Link to this heading">#</a></h5>
<p>Implement <code class="docutils literal notranslate"><span class="pre">forward_propagation()</span></code> using the following equations:</p>
<div class="math notranslate nohighlight">
\[Z^{[1]} =  W^{[1]} X + b^{[1]}\tag{7}\]</div>
<div class="math notranslate nohighlight">
\[A^{[1]} = \tanh(Z^{[1]})\tag{8}\]</div>
<div class="math notranslate nohighlight">
\[Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\tag{9}\]</div>
<div class="math notranslate nohighlight">
\[\hat{Y} = A^{[2]} = \sigma(Z^{[2]})\tag{10}\]</div>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p>Check the mathematical representation of your classifier in the figure above.</p></li>
<li><p>Use the function <code class="docutils literal notranslate"><span class="pre">sigmoid()</span></code>. It’s built into (imported) this notebook.</p></li>
<li><p>Use the function <code class="docutils literal notranslate"><span class="pre">np.tanh()</span></code>. It’s part of the numpy library.</p></li>
<li><p>Implement using these steps:</p>
<ol class="arabic simple">
<li><p>Retrieve each parameter from the dictionary “parameters” (which is the output of <code class="docutils literal notranslate"><span class="pre">initialize_parameters()</span></code> by using <code class="docutils literal notranslate"><span class="pre">parameters[&quot;..&quot;]</span></code>.</p></li>
<li><p>Implement Forward Propagation. Compute <span class="math notranslate nohighlight">\(Z^{[1]}, A^{[1]}, Z^{[2]}\)</span> and <span class="math notranslate nohighlight">\(A^{[2]}\)</span> (the vector of all your predictions on all the examples in the training set).</p></li>
</ol>
</li>
<li><p>Values needed in the backpropagation are stored in “cache”. The cache will be given as an input to the backpropagation function.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION:forward_propagation</span>

<span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Argument:</span>
<span class="sd">    X -- input data of size (n_x, m)</span>
<span class="sd">    parameters -- python dictionary containing your parameters (output of initialization function)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    A2 -- The sigmoid output of the second activation</span>
<span class="sd">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Retrieve each parameter from the dictionary &quot;parameters&quot;</span>
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># W1 = ...</span>
    <span class="c1"># b1 = ...</span>
    <span class="c1"># W2 = ...</span>
    <span class="c1"># b2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Implement Forward Propagation to calculate A2 (probabilities)</span>
    <span class="c1"># (≈ 4 lines of code)</span>
    <span class="c1"># Z1 = ...</span>
    <span class="c1"># A1 = ...</span>
    <span class="c1"># Z2 = ...</span>
    <span class="c1"># A2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">assert</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Z1&quot;</span><span class="p">:</span> <span class="n">Z1</span><span class="p">,</span>
             <span class="s2">&quot;A1&quot;</span><span class="p">:</span> <span class="n">A1</span><span class="p">,</span>
             <span class="s2">&quot;Z2&quot;</span><span class="p">:</span> <span class="n">Z2</span><span class="p">,</span>
             <span class="s2">&quot;A2&quot;</span><span class="p">:</span> <span class="n">A2</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_X</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">forward_propagation_test_case</span><span class="p">()</span>
<span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">A2</span><span class="p">))</span>

<span class="n">forward_propagation_test</span><span class="p">(</span><span class="n">forward_propagation</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A2 = [[0.21292656 0.21274673 0.21295976]]
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="compute-the-cost">
<h5>4.4 - Compute the Cost<a class="headerlink" href="#compute-the-cost" title="Link to this heading">#</a></h5>
<p>Now that you’ve computed <span class="math notranslate nohighlight">\(A^{[2]}\)</span> (in the Python variable “<code class="docutils literal notranslate"><span class="pre">A2</span></code>”), which contains <span class="math notranslate nohighlight">\(a^{[2](i)}\)</span> for all examples, you can compute the cost function as follows:</p>
<div class="math notranslate nohighlight">
\[J = - \frac{1}{m} \sum\limits_{i = 1}^{m} \bigg( y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \bigg) \tag{11}\]</div>
</section>
<section id="exercise-5-compute-cost">
<h5>Exercise 5 - compute_cost<a class="headerlink" href="#exercise-5-compute-cost" title="Link to this heading">#</a></h5>
<p>Implement <code class="docutils literal notranslate"><span class="pre">compute_cost()</span></code> to compute the value of the cost <span class="math notranslate nohighlight">\(J\)</span>.</p>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p>There are many ways to implement the cross-entropy loss. This is one way to implement one part of the equation without for loops:
<span class="math notranslate nohighlight">\(- \sum\limits_{i=1}^{m}  y^{(i)}\log(a^{[2](i)})\)</span>:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logprobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A2</span><span class="p">),</span><span class="n">Y</span><span class="p">)</span>
<span class="n">cost</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>          
</pre></div>
</div>
<ul class="simple">
<li><p>Use that to build the whole expression of the cost function.</p></li>
</ul>
<p><strong>Notes</strong>:</p>
<ul class="simple">
<li><p>You can use either <code class="docutils literal notranslate"><span class="pre">np.multiply()</span></code> and then <code class="docutils literal notranslate"><span class="pre">np.sum()</span></code> or directly <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code>).</p></li>
<li><p>If you use <code class="docutils literal notranslate"><span class="pre">np.multiply</span></code> followed by <code class="docutils literal notranslate"><span class="pre">np.sum</span></code> the end result will be a type <code class="docutils literal notranslate"><span class="pre">float</span></code>, whereas if you use <code class="docutils literal notranslate"><span class="pre">np.dot</span></code>, the result will be a 2D numpy array.</p></li>
<li><p>You can use <code class="docutils literal notranslate"><span class="pre">np.squeeze()</span></code> to remove redundant dimensions (in the case of single float, this will be reduced to a zero-dimension array).</p></li>
<li><p>You can also cast the array as a type <code class="docutils literal notranslate"><span class="pre">float</span></code> using <code class="docutils literal notranslate"><span class="pre">float()</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: compute_cost</span>

<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cross-entropy cost given in equation (13)</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)</span>
<span class="sd">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span>

<span class="sd">    Returns:</span>
<span class="sd">    cost -- cross-entropy cost given equation (13)</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of examples</span>

    <span class="c1"># Compute the cross-entropy cost</span>
    <span class="c1"># (≈ 2 lines of code)</span>
    <span class="c1"># logprobs = ...</span>
    <span class="c1"># cost = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">logprobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">A2</span><span class="p">),</span><span class="n">Y</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">A2</span><span class="p">),</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">logprobs</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="n">cost</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>  <span class="c1"># makes sure cost is the dimension we expect. </span>
                                    <span class="c1"># E.g., turns [[17]] into 17 </span>
    
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A2</span><span class="p">,</span> <span class="n">t_Y</span> <span class="o">=</span> <span class="n">compute_cost_test_case</span><span class="p">()</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cost = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">)))</span>

<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cost = 0.6930587610394646
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="implement-backpropagation">
<h5>4.5 - Implement Backpropagation<a class="headerlink" href="#implement-backpropagation" title="Link to this heading">#</a></h5>
<p>Using the cache computed during forward propagation, you can now implement backward propagation.</p>
</section>
<section id="exercise-6-backward-propagation">
<h5>Exercise 6 -  backward_propagation<a class="headerlink" href="#exercise-6-backward-propagation" title="Link to this heading">#</a></h5>
<p>Implement the function <code class="docutils literal notranslate"><span class="pre">backward_propagation()</span></code>.</p>
<p><strong>Instructions</strong>:
Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation.</p>
<figure class="align-default" id="test-2">
<a class="reference internal image-reference" href="_images/grad_summary.png"><img alt="_images/grad_summary.png" src="_images/grad_summary.png" style="width: 680px; height: 360px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Backpropagation. Use the six equations on the right</span><a class="headerlink" href="#test-2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<!--
$\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } = \frac{1}{m} (a^{[2](i)} - y^{(i)})$

$\frac{\partial \mathcal{J} }{ \partial W_2 } = \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } a^{[1] (i) T} $

$\frac{\partial \mathcal{J} }{ \partial b_2 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)}}}$

$\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} } =  W_2^T \frac{\partial \mathcal{J} }{ \partial z_{2}^{(i)} } * ( 1 - a^{[1] (i) 2}) $

$\frac{\partial \mathcal{J} }{ \partial W_1 } = \frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)} }  X^T $

$\frac{\partial \mathcal{J} _i }{ \partial b_1 } = \sum_i{\frac{\partial \mathcal{J} }{ \partial z_{1}^{(i)}}}$

- Note that $*$ denotes elementwise multiplication.
- The notation you will use is common in deep learning coding:
    - dW1 = $\frac{\partial \mathcal{J} }{ \partial W_1 }$
    - db1 = $\frac{\partial \mathcal{J} }{ \partial b_1 }$
    - dW2 = $\frac{\partial \mathcal{J} }{ \partial W_2 }$
    - db2 = $\frac{\partial \mathcal{J} }{ \partial b_2 }$
    
!-->
<ul class="simple">
<li><p>Tips:</p>
<ul>
<li><p>To compute dZ1 you’ll need to compute <span class="math notranslate nohighlight">\(g^{[1]'}(Z^{[1]})\)</span>. Since <span class="math notranslate nohighlight">\(g^{[1]}(.)\)</span> is the tanh activation function, if <span class="math notranslate nohighlight">\(a = g^{[1]}(z)\)</span> then <span class="math notranslate nohighlight">\(g^{[1]'}(z) = 1-a^2\)</span>. So you can compute
<span class="math notranslate nohighlight">\(g^{[1]'}(Z^{[1]})\)</span> using <code class="docutils literal notranslate"><span class="pre">(1</span> <span class="pre">-</span> <span class="pre">np.power(A1,</span> <span class="pre">2))</span></code>.</p></li>
</ul>
</li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: backward_propagation</span>

<span class="k">def</span> <span class="nf">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward propagation using the instructions above.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    parameters -- python dictionary containing our parameters </span>
<span class="sd">    cache -- a dictionary containing &quot;Z1&quot;, &quot;A1&quot;, &quot;Z2&quot; and &quot;A2&quot;.</span>
<span class="sd">    X -- input data of shape (2, number of examples)</span>
<span class="sd">    Y -- &quot;true&quot; labels vector of shape (1, number of examples)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    grads -- python dictionary containing your gradients with respect to different parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># First, retrieve W1 and W2 from the dictionary &quot;parameters&quot;.</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># W1 = ...</span>
    <span class="c1"># W2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
        
    <span class="c1"># Retrieve also A1 and A2 from dictionary &quot;cache&quot;.</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># A1 = ...</span>
    <span class="c1"># A2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">A1</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;A1&quot;</span><span class="p">]</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s2">&quot;A2&quot;</span><span class="p">]</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Backward propagation: calculate dW1, db1, dW2, db2. </span>
    <span class="c1">#(≈ 6 lines of code, corresponding to 6 equations on slide above)</span>
    <span class="c1"># dZ2 = ...</span>
    <span class="c1"># dW2 = ...</span>
    <span class="c1"># db2 = ...</span>
    <span class="c1"># dZ1 = ...</span>
    <span class="c1"># dW1 = ...</span>
    <span class="c1"># db1 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">dZ2</span> <span class="o">=</span> <span class="n">A2</span><span class="o">-</span><span class="n">Y</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dW1&quot;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span>
             <span class="s2">&quot;db1&quot;</span><span class="p">:</span> <span class="n">db1</span><span class="p">,</span>
             <span class="s2">&quot;dW2&quot;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span>
             <span class="s2">&quot;db2&quot;</span><span class="p">:</span> <span class="n">db2</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span> <span class="o">=</span> <span class="n">backward_propagation_test_case</span><span class="p">()</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dW1 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;db1 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dW2 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW2&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;db2 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db2&quot;</span><span class="p">]))</span>

<span class="n">backward_propagation_test</span><span class="p">(</span><span class="n">backward_propagation</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dW1 = [[ 0.00301023 -0.00747267]
 [ 0.00257968 -0.00641288]
 [-0.00156892  0.003893  ]
 [-0.00652037  0.01618243]]
db1 = [[ 0.00176201]
 [ 0.00150995]
 [-0.00091736]
 [-0.00381422]]
dW2 = [[ 0.00078841  0.01765429 -0.00084166 -0.01022527]]
db2 = [[-0.16655712]]
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="update-parameters">
<h5>4.6 - Update Parameters<a class="headerlink" href="#update-parameters" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-7-update-parameters">
<h5>Exercise 7 - update_parameters<a class="headerlink" href="#exercise-7-update-parameters" title="Link to this heading">#</a></h5>
<p>Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).</p>
<p><strong>General gradient descent rule</strong>: <span class="math notranslate nohighlight">\(\theta = \theta - \alpha \frac{\partial J }{ \partial \theta }\)</span> where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate and <span class="math notranslate nohighlight">\(\theta\)</span> represents a parameter.</p>
<figure class="align-default" id="test-3">
<a class="reference internal image-reference" href="_images/sgd.gif"><img alt="_images/sgd.gif" src="_images/sgd.gif" style="width: 360px; height: 360px;" /></a>
</figure>
<figure class="align-default" id="test-4">
<a class="reference internal image-reference" href="_images/sgd_bad.gif"><img alt="_images/sgd_bad.gif" src="_images/sgd_bad.gif" style="width: 360px; height: 360px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.</span><a class="headerlink" href="#test-4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Hint</strong></p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">copy.deepcopy(...)</span></code> when copying lists or dictionaries that are passed as parameters to functions. It avoids input parameters being modified within the function. In some scenarios, this could be inefficient, but it is required for grading purposes.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: update_parameters</span>

<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Updates parameters using the gradient descent update rule given above</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    parameters -- python dictionary containing your parameters </span>
<span class="sd">    grads -- python dictionary containing your gradients </span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your updated parameters </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Retrieve a copy of each parameter from the dictionary &quot;parameters&quot;. Use copy.deepcopy(...) for W1 and W2</span>
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># W1 = ...</span>
    <span class="c1"># b1 = ...</span>
    <span class="c1"># W2 = ...</span>
    <span class="c1"># b2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">])</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">])</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>

    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Retrieve each gradient from the dictionary &quot;grads&quot;</span>
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># dW1 = ...</span>
    <span class="c1"># db1 = ...</span>
    <span class="c1"># dW2 = ...</span>
    <span class="c1"># db2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">dW1</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW1&quot;</span><span class="p">]</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db1&quot;</span><span class="p">]</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW2&quot;</span><span class="p">]</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db2&quot;</span><span class="p">]</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Update rule for each parameter</span>
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># W1 = ...</span>
    <span class="c1"># b1 = ...</span>
    <span class="c1"># W2 = ...</span>
    <span class="c1"># b2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">W1</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW1</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">b1</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db1</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">W2</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dW2</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">b2</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">db2</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;W1&quot;</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s2">&quot;b1&quot;</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s2">&quot;W2&quot;</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s2">&quot;b2&quot;</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">update_parameters_test_case</span><span class="p">()</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="n">update_parameters_test</span><span class="p">(</span><span class="n">update_parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[-0.00643025  0.01936718]
 [-0.02410458  0.03978052]
 [-0.01653973 -0.02096177]
 [ 0.01046864 -0.05990141]]
b1 = [[-1.02420756e-06]
 [ 1.27373948e-05]
 [ 8.32996807e-07]
 [-3.20136836e-06]]
W2 = [[-0.01041081 -0.04463285  0.01758031  0.04747113]]
b2 = [[0.00010457]]
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="integration">
<h5>4.7 - Integration<a class="headerlink" href="#integration" title="Link to this heading">#</a></h5>
<p>Integrate your functions in <code class="docutils literal notranslate"><span class="pre">nn_model()</span></code></p>
</section>
<section id="exercise-8-nn-model">
<h5>Exercise 8 - nn_model<a class="headerlink" href="#exercise-8-nn-model" title="Link to this heading">#</a></h5>
<p>Build your neural network model in <code class="docutils literal notranslate"><span class="pre">nn_model()</span></code>.</p>
<p><strong>Instructions</strong>: The neural network model has to use the previous functions in the right order.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: nn_model</span>

<span class="k">def</span> <span class="nf">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- dataset of shape (2, number of examples)</span>
<span class="sd">    Y -- labels of shape (1, number of examples)</span>
<span class="sd">    n_h -- size of the hidden layer</span>
<span class="sd">    num_iterations -- Number of iterations in gradient descent loop</span>
<span class="sd">    print_cost -- if True, print the cost every 1000 iterations</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- parameters learnt by the model. They can then be used to predict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">n_x</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_y</span> <span class="o">=</span> <span class="n">layer_sizes</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span>
    
    <span class="c1"># Initialize parameters</span>
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># parameters = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Loop (gradient descent)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>
         
        <span class="c1">#(≈ 4 lines of code)</span>
        <span class="c1"># Forward propagation. Inputs: &quot;X, parameters&quot;. Outputs: &quot;A2, cache&quot;.</span>
        <span class="c1"># A2, cache = ...</span>
        
        <span class="c1"># Cost function. Inputs: &quot;A2, Y&quot;. Outputs: &quot;cost&quot;.</span>
        <span class="c1"># cost = ...</span>
 
        <span class="c1"># Backpropagation. Inputs: &quot;parameters, cache, X, Y&quot;. Outputs: &quot;grads&quot;.</span>
        <span class="c1"># grads = ...</span>
 
        <span class="c1"># Gradient descent parameter update. Inputs: &quot;parameters, grads&quot;. Outputs: &quot;parameters&quot;.</span>
        <span class="c1"># parameters = ...</span>
        
        <span class="c1"># YOUR CODE STARTS HERE</span>
        
        <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1.2</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Print the cost every 1000 iterations</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">%i</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nn_model_test</span><span class="p">(</span><span class="n">nn_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.693086
Cost after iteration 1000: 0.000220
Cost after iteration 2000: 0.000108
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 3000: 0.000072
Cost after iteration 4000: 0.000054
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 5000: 0.000043
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 6000: 0.000036
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 7000: 0.000030
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 8000: 0.000027
Cost after iteration 9000: 0.000024
W1 = [[ 0.71392202  1.31281102]
 [-0.76411243 -1.41967065]
 [-0.75040545 -1.38857337]
 [ 0.56495575  1.04857776]]
b1 = [[-0.0073536 ]
 [ 0.01534663]
 [ 0.01262938]
 [ 0.00218135]]
W2 = [[ 2.82545815 -3.3063945  -3.16116615  1.8549574 ]]
b2 = [[0.00393452]]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="test-the-model">
<h4>5 - Test the Model<a class="headerlink" href="#test-the-model" title="Link to this heading">#</a></h4>
<section id="predict">
<h5>5.1 - Predict<a class="headerlink" href="#predict" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-9-predict">
<h5>Exercise 9 - predict<a class="headerlink" href="#exercise-9-predict" title="Link to this heading">#</a></h5>
<p>Predict with your model by building <code class="docutils literal notranslate"><span class="pre">predict()</span></code>.
Use forward propagation to predict results.</p>
<p><strong>Reminder</strong>: predictions = <span class="math notranslate nohighlight">\(y_{prediction} = \mathbb 1 \text{{activation &gt; 0.5}} = \begin{cases}
      1 &amp; \text{if}\ activation &gt; 0.5 \\
      0 &amp; \text{otherwise}
    \end{cases}\)</span></p>
<p>As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: <code class="docutils literal notranslate"><span class="pre">X_new</span> <span class="pre">=</span> <span class="pre">(X</span> <span class="pre">&gt;</span> <span class="pre">threshold)</span></code></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: predict</span>

<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Using the learned parameters, predicts a class for each example in X</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    parameters -- python dictionary containing your parameters </span>
<span class="sd">    X -- input data of size (n_x, m)</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    predictions -- vector of predictions of our model (red: 0 / blue: 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># A2, cache = ...</span>
    <span class="c1"># predictions = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">A2</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="p">(</span><span class="n">A2</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>

    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">predictions</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">t_X</span> <span class="o">=</span> <span class="n">predict_test_case</span><span class="p">()</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">t_X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictions: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">predictions</span><span class="p">))</span>

<span class="n">predict_test</span><span class="p">(</span><span class="n">predict</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Predictions: [[ True False  True]]
All tests passed!
</pre></div>
</div>
</div>
</div>
</section>
<section id="test-the-model-on-the-planar-dataset">
<h5>5.2 - Test the Model on the Planar Dataset<a class="headerlink" href="#test-the-model-on-the-planar-dataset" title="Link to this heading">#</a></h5>
<p>It’s time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of <span class="math notranslate nohighlight">\(n_h\)</span> hidden units!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build a model with a n_h-dimensional hidden layer</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot the decision boundary</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Decision Boundary for hidden layer size &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.693162
Cost after iteration 1000: 0.258625
Cost after iteration 2000: 0.239334
Cost after iteration 3000: 0.230802
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 4000: 0.225528
Cost after iteration 5000: 0.221845
Cost after iteration 6000: 0.219094
Cost after iteration 7000: 0.220612
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 8000: 0.219396
Cost after iteration 9000: 0.218481
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Decision Boundary for hidden layer size 4&#39;)
</pre></div>
</div>
<img alt="_images/bf147159ca8de05dbde90336298735b48db74a92ac3a8fc747aa275c67677917.png" src="_images/bf147159ca8de05dbde90336298735b48db74a92ac3a8fc747aa275c67677917.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print accuracy</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Accuracy: </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="nb">float</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;%&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 90%
</pre></div>
</div>
</div>
</div>
<p>Accuracy is really high compared to Logistic Regression. The model has learned the patterns of the flower’s petals! Unlike logistic regression, neural networks are able to learn even highly non-linear decision boundaries.</p>
</section>
<section id="congrats-on-finishing-reviewing-this-programming-assignment-material">
<h5>Congrats on finishing reviewing this Programming Assignment material!<a class="headerlink" href="#congrats-on-finishing-reviewing-this-programming-assignment-material" title="Link to this heading">#</a></h5>
<p>Here’s a quick recap of all you just accomplished:</p>
<ul class="simple">
<li><p>Built a complete 2-class classification neural network with a hidden layer</p></li>
<li><p>Made good use of a non-linear unit</p></li>
<li><p>Computed the cross entropy loss</p></li>
<li><p>Implemented forward and backward propagation</p></li>
<li><p>Seen the impact of varying the hidden layer size, including overfitting.</p></li>
</ul>
<p>You’ve created a neural network that can learn patterns! Excellent work. Below, there are some exercises to try out some other hidden layer sizes, and other datasets.</p>
</section>
</section>
<section id="exercise-tuning-hidden-layer-size-optional-ungraded-exercise">
<h4>6 - Exercise: Tuning hidden layer size (optional/ungraded exercise)<a class="headerlink" href="#exercise-tuning-hidden-layer-size-optional-ungraded-exercise" title="Link to this heading">#</a></h4>
<p>Run the following code(it may take 1-2 minutes). Then, observe different behaviors of the model for various hidden layer sizes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># This may take about 2 minutes to run</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>
<span class="n">hidden_layer_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>

<span class="c1"># you can try with different hidden layer sizes</span>
<span class="c1"># but make sure before you submit the assignment it is set as &quot;hidden_layer_sizes = [1, 2, 3, 4, 5]&quot;</span>
<span class="c1"># hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50]</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">n_h</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Hidden Layer of size </span><span class="si">%d</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">n_h</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">nn_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">)</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span><span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">predictions</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">Y</span><span class="o">.</span><span class="n">size</span><span class="p">)</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span>
    <span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Accuracy for </span><span class="si">{}</span><span class="s2"> hidden units: </span><span class="si">{}</span><span class="s2"> %&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy for 1 hidden units: 67.5 %
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy for 2 hidden units: 67.25 %
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy for 3 hidden units: 90.75 %
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy for 4 hidden units: 90.5 %
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy for 5 hidden units: 91.25 %
</pre></div>
</div>
<img alt="_images/9fe10a0cf1df0a49cbcb4ed9bc0d9df2c1abe8058f5dfaf1376da064ffe8a253.png" src="_images/9fe10a0cf1df0a49cbcb4ed9bc0d9df2c1abe8058f5dfaf1376da064ffe8a253.png" />
</div>
</div>
<p><strong>Interpretation</strong>:</p>
<ul class="simple">
<li><p>The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data.</p></li>
<li><p>The best hidden layer size seems to be around <code class="docutils literal notranslate"><span class="pre">n_h</span> <span class="pre">=</span> <span class="pre">5</span></code>. Indeed, a value around here seems to  fits the data well without also incurring noticeable overfitting.</p></li>
<li><p>Later, you’ll become familiar with regularization, which lets you use very large models (such as <code class="docutils literal notranslate"><span class="pre">n_h</span> <span class="pre">=</span> <span class="pre">50</span></code>) without much overfitting.</p></li>
</ul>
<p><strong>Note</strong>: Remember to submit the assignment by clicking the blue “Submit Assignment” button at the upper-right.</p>
<p><strong>Some optional/ungraded questions that you can explore if you wish</strong>:</p>
<ul class="simple">
<li><p>What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?</p></li>
<li><p>Play with the learning_rate. What happens?</p></li>
<li><p>What if we change the dataset? (See part 7 below!)</p></li>
</ul>
</section>
<section id="exercise-performance-on-other-datasets">
<h4>7 - Exercise: Performance on other datasets<a class="headerlink" href="#exercise-performance-on-other-datasets" title="Link to this heading">#</a></h4>
<p>You need rerun the whole notebook (minus the dataset part) for each of the following datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Datasets</span>
<span class="n">noisy_circles</span><span class="p">,</span> <span class="n">noisy_moons</span><span class="p">,</span> <span class="n">blobs</span><span class="p">,</span> <span class="n">gaussian_quantiles</span><span class="p">,</span> <span class="n">no_structure</span> <span class="o">=</span> <span class="n">load_extra_datasets</span><span class="p">()</span>

<span class="n">datasets</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;noisy_circles&quot;</span><span class="p">:</span> <span class="n">noisy_circles</span><span class="p">,</span>
            <span class="s2">&quot;noisy_moons&quot;</span><span class="p">:</span> <span class="n">noisy_moons</span><span class="p">,</span>
            <span class="s2">&quot;blobs&quot;</span><span class="p">:</span> <span class="n">blobs</span><span class="p">,</span>
            <span class="s2">&quot;gaussian_quantiles&quot;</span><span class="p">:</span> <span class="n">gaussian_quantiles</span><span class="p">}</span>

<span class="c1">### START CODE HERE ### (choose your dataset)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;noisy_moons&quot;</span>
<span class="c1">### END CODE HERE ###</span>

<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">datasets</span><span class="p">[</span><span class="n">dataset</span><span class="p">]</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="c1"># make blobs binary</span>
<span class="k">if</span> <span class="n">dataset</span> <span class="o">==</span> <span class="s2">&quot;blobs&quot;</span><span class="p">:</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">%</span><span class="k">2</span>

<span class="c1"># Visualize the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">Spectral</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c07627c5bf4de4ee364545ee181b5c0b801a68cc86bcc5b84a79fee5c6112da7.png" src="_images/c07627c5bf4de4ee364545ee181b5c0b801a68cc86bcc5b84a79fee5c6112da7.png" />
</div>
</div>
<p><strong>References</strong>:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://scs.ryerson.ca/~aharley/neural-networks/">http://scs.ryerson.ca/~aharley/neural-networks/</a></p></li>
<li><p><a class="reference external" href="http://cs231n.github.io/neural-networks-case-study/">http://cs231n.github.io/neural-networks-case-study/</a></p></li>
</ul>
</section>
</section>
</div>
</section>
<span id="document-C4"></span><section class="tex2jax_ignore mathjax_ignore" id="deep-neural-networks">
<span id="dnn"></span><h2>4 Deep Neural Networks<a class="headerlink" href="#deep-neural-networks" title="Link to this heading">#</a></h2>
<p>Analyze the key computations underlying deep learning, then use them to build and train deep neural networks for computer vision tasks.</p>
<p><strong>Learning Objectives</strong></p>
<ul class="simple">
<li><p>Describe the successive block structure of a deep neural network.</p></li>
<li><p>Build a deep L-layer neural network.</p></li>
<li><p>Analyze matrix and vector dimensions to check neural network implementations.</p></li>
<li><p>Use a cache to pass information from forward to back propagation.</p></li>
<li><p>Explain the role of hyperparameters in deep learning.</p></li>
<li><p>Build a 2-layer neural network.</p></li>
</ul>
<hr class="docutils" />
<section id="deep-l-layer-neural-network">
<h3>Deep L-layer Neural Network<a class="headerlink" href="#deep-l-layer-neural-network" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/xkNf7Nqn9VI">Video</a></p>
<p>Logistic regression is a very “shallow” model, whereas the model, with five hidden layers for example, is a much deeper model, and shallow versus depth is a matter of degree. So neural network of a single hidden layer, this would be a 2 layer neural network. Remember when we count layers in a neural network, we don’t count the input layer, we just count the hidden layers as was the output layer. So, a 2 layer neural network is still quite shallow, but not as shallow as logistic regression. Technically logistic regression is a one layer neural network, but over the last several years the AI, on the machine learning community, has realized that there are functions that very deep neural networks can learn that shallower models are often unable to.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/4-1.png"><img alt="_images/4-1.png" src="_images/4-1.png" style="height: 400px;" /></a>
</figure>
<p>Although for any given problem, it might be hard to predict in advance exactly how deep in your network you would want. So it would be reasonable to try logistic regression, try one and then two hidden layers, and view the number of hidden layers as another hyper parameter that you could try a variety of values of, and evaluate on all that across validation data, or on your development set.</p>
<p>Let’s now go through the notation we used to describe deep neural networks. Here’s is a four layer neural network with three hidden layers:</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/4-2.png"><img alt="_images/4-2.png" src="_images/4-2.png" style="height: 300px;" /></a>
</figure>
<p>Notation:</p>
<ul class="simple">
<li><p>Number of layers (L): L = 4</p></li>
<li><p>Number of unites in layer <span class="math notranslate nohighlight">\(l\)</span>: <span class="math notranslate nohighlight">\(n^{[l]}\)</span>.</p>
<ul>
<li><p>In this example: <span class="math notranslate nohighlight">\(n^{[1]} = 5, \quad n^{[2]} = 5, \quad n^{[3]} = 3, \quad n^{[4]} = n^{[L]}= 1, \quad n^{[0]} = n_x = 3\)</span></p></li>
</ul>
</li>
<li><p>Activation in layer <span class="math notranslate nohighlight">\(l\)</span>: <span class="math notranslate nohighlight">\(a^{[l]} = g^{[l]}(\mathbf{z}^{[l]})\)</span>.</p></li>
<li><p>Weights for <span class="math notranslate nohighlight">\(\mathbf{z}^{[l]}\)</span>: <span class="math notranslate nohighlight">\(\mathbf{w}^{[l]}, \quad b^{[l]}\)</span>.</p></li>
</ul>
</section>
<section id="forward-propagation-in-a-deep-network">
<h3>Forward Propagation in a Deep Network<a class="headerlink" href="#forward-propagation-in-a-deep-network" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/JT5mKK93lII">Video</a></p>
<figure class="align-default" id="copy">
<a class="reference internal image-reference" href="_images/4-2.png"><img alt="_images/4-2.png" src="_images/4-2.png" style="height: 300px;" /></a>
</figure>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{[l]} &amp;= \mathbf{W}^{[l]} \mathbf{A}^{[l-1]} + \mathbf{b}^{[l]} \\
\mathbf{A}^{[l]} &amp;= g^{[l]}({\mathbf{Z}^{[l]}}) \\
\end{aligned}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{[1]} &amp;= \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{A}^{[1]} &amp;= g^{[1]}({\mathbf{Z}^{[1]}}) \\
\mathbf{Z}^{[2]} &amp;= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{A}^{[2]} &amp;= g^{[2]}({\mathbf{Z}^{[2]}})  \\
\mathbf{Z}^{[3]} &amp;= \mathbf{W}^{[3]} \mathbf{A}^{[2]} + \mathbf{b}^{[3]} \\
\mathbf{A}^{[3]} &amp;= g^{[3]}({\mathbf{Z}^{[3]}})  \\
\mathbf{Z}^{[4]} &amp;= \mathbf{W}^{[4]} \mathbf{A}^{[3]} + \mathbf{b}^{[4]} \\
\mathbf{A}^{[4]} &amp;= g^{[4]}({\mathbf{Z}^{[4]}}) 
\end{aligned}
\end{split}\]</div>
<p>If you look at this implementation of vectorization, it looks like that there is going to be a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop here, by using <code class="docutils literal notranslate"><span class="pre">for</span> <span class="pre">l=1...L</span></code>.</p>
<p>I know that when implementing neural networks, we usually want to get rid of explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loops. But this is one place where I don’t think there’s any way to implement this without an explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. So when implementing forward propagation, it is perfectly okay to have a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop to compute the activations for layer one, then layer two, then layer three, then layer four. I don’t think there is any way to do this without a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop that goes from one to capital L, from one through the total number of layers in the neural network. So, in this place, it’s perfectly okay to have an explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
<p>If the pieces we’ve seen so far looks a little bit familiar to you, that’s because what we’re seeing is taking a piece very similar to what you’ve seen in the neural network with a single hidden layer and just repeating that more times.</p>
<p>Now, it turns out that we implement a deep neural network, one of the ways to increase your odds of having a bug-free implementation is to think very systematic and carefully about the matrix dimensions you’re working with. So, when I’m trying to debug my own code, I’ll often pull a piece of paper, and just think carefully through, so the dimensions of the matrix I’m working with.</p>
</section>
<section id="getting-your-matrix-dimensions-right">
<h3>Getting your Matrix Dimensions Right<a class="headerlink" href="#getting-your-matrix-dimensions-right" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/35_DNMcMW-w">Video</a></p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/4-3.png"><img alt="_images/4-3.png" src="_images/4-3.png" style="height: 250px;" /></a>
</figure>
<ul class="simple">
<li><p>For one sample:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{w}^{[l]} &amp;: \big(  n^{[l]}, n^{[l-1]} \big) \\
b^{[l]} &amp;: \big(  n^{[l]}, 1 \big) \\
\mathrm{d} \mathbf{w}^{[l]} &amp;: \big(  n^{[l]}, n^{[l-1]} \big) \\
\mathrm{d} b^{[l]} &amp;: \big(  n^{[l]}, 1 \big) \\
\mathbf{z}^{[l]}, \mathbf{a}^{[l]} &amp;: \big(  n^{[l]}, 1 \big) \\
\text{if } l = 0 &amp;: \mathbf{a}^{[0]} = \mathbf{x} = \big(  n^{[0]}, 1 \big) \\
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(m\)</span> samples and vectorized implementation:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{W}^{[l]} &amp;: \big(  n^{[l]}, n^{[l-1]} \big) \\
b^{[l]} &amp;: \big(  n^{[l]}, m \big) \\
\mathrm{d} \mathbf{W}^{[l]} &amp;: \big(  n^{[l]}, n^{[l-1]} \big) \\
\mathrm{d} b^{[l]} &amp;: \big(  n^{[l]}, m \big) \\
\mathbf{Z}^{[l]}, \mathbf{A}^{[l]} &amp;: \big(  n^{[l]}, m \big) \\
\text{if } l = 0 &amp;: \mathbf{A}^{[0]} = \mathbf{X} = \big(  n^{[0]}, m \big) \\
\mathrm{d} \mathbf{Z}^{[l]}, \mathrm{d} \mathbf{A}^{[l]} &amp;: \big(  n^{[l]}, m \big)
\end{aligned}
\end{split}\]</div>
<p>When you implement a deep neural network if you keep straight the dimensions of these various matrices and vectors you’re working with, hopefully, that will help you eliminate some class of possible bugs.</p>
</section>
<section id="why-deep-representations">
<h3>Why Deep Representations?<a class="headerlink" href="#why-deep-representations" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/52H13FH0yQo">Video</a></p>
<ul class="simple">
<li><p>Intuition about deep representation</p></li>
<li><p>Circuit theory and deep learning</p></li>
</ul>
<p>Circuit theory, informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute.</p>
</section>
<section id="building-blocks-of-deep-neural-networks">
<h3>Building Blocks of Deep Neural Networks<a class="headerlink" href="#building-blocks-of-deep-neural-networks" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/grQ1OvXLamY">Video</a></p>
<p>In the chapter 2 and 3, you have already learned the basic building blocks of forward propagation and back propagation, the key components you need to implement a deep neural network. Let’s see how you can put these components together to build your deep net.</p>
<p>In a deep neural network, picking one layer (layer <span class="math notranslate nohighlight">\(l\)</span>) and looking at the computations focusing on just that layer for now.</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/4-4.png"><img alt="_images/4-4.png" src="_images/4-4.png" style="height: 200px;" /></a>
</figure>
<p>For layer <span class="math notranslate nohighlight">\(l\)</span>, you have some parameters:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbf{w}^{[l]}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}^{[l]}\)</span></p></li>
<li><p>Forward: Input  <span class="math notranslate nohighlight">\(\mathbf{a}^{[l-1]}\)</span> Output <span class="math notranslate nohighlight">\(\mathbf{a}^{[l]}\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{z}^{[l]} = \mathbf{w}^{[l]}  \mathbf{a}^{[l-1]}+ \mathbf{b}^{[l]}\)</span>, cache <span class="math notranslate nohighlight">\(\mathbf{z}^{[l]}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{a}^{[l]} = g^{[l]}(\mathbf{z}^{[l]})\)</span></p></li>
</ul>
</li>
<li><p>Backward: Input  <span class="math notranslate nohighlight">\(\mathrm{d} \mathbf{a}^{[l]}\)</span> Output <span class="math notranslate nohighlight">\(\mathrm{d}\mathbf{a}^{[l-1]}\)</span></p></li>
</ul>
<p>To summarize, in layer <span class="math notranslate nohighlight">\(l\)</span>, you’re going to have these steps show below:</p>
<figure class="align-default" id="id5">
<a class="reference internal image-reference" href="_images/4-5.png"><img alt="_images/4-5.png" src="_images/4-5.png" style="height: 360px;" /></a>
</figure>
<p>So if you can implement these two functions then the basic computation of the neural network will be as follows.</p>
<figure class="align-default" id="id6">
<a class="reference internal image-reference" href="_images/4-6.png"><img alt="_images/4-6.png" src="_images/4-6.png" style="height: 400px;" /></a>
</figure>
<p>So that’s one iteration of gradient descent for your neural network.</p>
<div class="hint admonition">
<p class="admonition-title">Reminder</p>
<p>Conceptually, it will be useful to think of the cache here in the figure as storing the value of <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> for the backward functions. But when you implement this, and you see this in the programming exercise, When you implement this, you find that the cache may be a convenient way to get to this value of the parameters of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>, into the backward function as well. So for this exercise you actually store in your cache to <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> as well as <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. But from an implementation standpoint, I just find it a convenient way to just get the parameters, copy to where you need to use them later when you’re computing back propagation. So that’s just an implementational detail that you see when you do the programming exercise.</p>
</div>
<p>So you’ve now seen what are the basic building blocks for implementing a deep neural network. In each layer there’s a forward propagation step and there’s a corresponding backward propagation step. And has a cache to pass information from one to the other.</p>
</section>
<section id="forward-and-backward-propagation">
<h3>Forward and Backward Propagation<a class="headerlink" href="#forward-and-backward-propagation" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/8k5XEQDrT-8">Video</a></p>
<section id="forward-propogation-for-layer-l">
<h4>Forward propogation for layer <span class="math notranslate nohighlight">\(l\)</span><a class="headerlink" href="#forward-propogation-for-layer-l" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Input <span class="math notranslate nohighlight">\(\mathbf{a}^{[l-1]}\)</span></p></li>
<li><p>Output <span class="math notranslate nohighlight">\(\mathbf{a}^{[l]}\)</span>, cache <span class="math notranslate nohighlight">\(\mathbf{z}^{[l]}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{z}^{[l]} &amp;= \mathbf{w}^{[l]}  \mathbf{a}^{[l-1]}+ \mathbf{b}^{[l]} \\
\mathbf{a}^{[l]} &amp;= g^{[l]}(\mathbf{z}^{[l]})
\end{aligned}
\end{split}\]</div>
<p>Vectorised:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{[l]} &amp;= \mathbf{W}^{[l]}  \mathbf{A}^{[l-1]}+ \mathbf{b}^{[l]} \\
\mathbf{A}^{[l]} &amp;= g^{[l]}(\mathbf{Z}^{[l]})
\end{aligned}
\end{split}\]</div>
</section>
<section id="backward-propogation-for-layer-l">
<h4>Backward propogation for layer <span class="math notranslate nohighlight">\(l\)</span><a class="headerlink" href="#backward-propogation-for-layer-l" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>Input <span class="math notranslate nohighlight">\(\mathrm{d} \mathbf{a}^{[l]}\)</span></p></li>
<li><p>Output <span class="math notranslate nohighlight">\(\mathrm{d}\mathbf{a}^{[l-1]}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{d}\mathbf{W}^{[l]}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{d}\mathbf{b}^{[l]}\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}\mathbf{z}^{[l]} &amp;= \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}z} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times  \dfrac{\mathrm{d}a}{\mathrm{d}z} = \mathrm{d}\mathbf{a}^{[l]} *  g^{[l]'}(\mathbf{z}^{[l]}) \\
\mathrm{d}\mathbf{W}^{[l]} &amp;= \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}W} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}W} = \mathrm{d}\mathbf{z}^{[l]} *  \mathbf{a}^{[l-1]} \\
\mathrm{d}\mathbf{b}^{[l]} &amp;= \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}b} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}b} = \mathrm{d}\mathbf{z}^{[l]} \\
\mathrm{d}\mathbf{a}^{[l-1]} &amp;= \mathbf{W}^{[l]T} \cdot \mathrm{d}\mathbf{z}^{[l]}
\end{aligned}
\end{split}\]</div>
<p>Vectorised:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}\mathbf{Z}^{[l]} &amp;= \mathrm{d}\mathbf{A}^{[l]} *  g^{[l]'}(\mathbf{Z}^{[l]}) \\
\mathrm{d}\mathbf{W}^{[l]} &amp;= \dfrac{1}{m} \mathrm{d}\mathbf{Z}^{[l]} \cdot \mathbf{A}^{[l-1]T} \\
\mathrm{d}\mathbf{b}^{[l]} &amp;= \dfrac{1}{m} \text{np.sum}(\mathrm{d}\mathbf{Z}^{[l]} \text{, axis = }1 \text{, keepdims = True} )\\
\mathrm{d}\mathbf{A}^{[l-1]} &amp;= \mathrm{d}\mathbf{W}^{[l]T} \cdot \mathrm{d}\mathbf{Z}^{[l]} 
\end{aligned}
\end{split}\]</div>
<p>For a more in depth explaination of Feedforward Neural Networks:</p>
<p><a class="reference external" href="https://jonaslalin.com/2021/12/10/feedforward-neural-networks-part-1/">Feedforward Neural Networks in Depth, Part 1: Forward and Backward Propagations</a></p>
<p><a class="reference external" href="https://jonaslalin.com/2021/12/21/feedforward-neural-networks-part-2/">Feedforward Neural Networks in Depth, Part 2: Activation Functions</a></p>
<p><a class="reference external" href="https://jonaslalin.com/2021/12/22/feedforward-neural-networks-part-3/">Feedforward Neural Networks in Depth, Part 3: Cost Functions</a></p>
</section>
</section>
<section id="parameters-and-hyperparameters">
<h3>Parameters and Hyperparameters<a class="headerlink" href="#parameters-and-hyperparameters" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/RGScA6l3KiM">Video</a></p>
<p>Being effective in developing your deep Neural Nets requires that you not only organize your parameters well but also your hyper parameters.</p>
<ul class="simple">
<li><p>Parameters: <span class="math notranslate nohighlight">\(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}, \cdots\)</span></p></li>
<li><p>Hyperparameters:</p>
<ul>
<li><p>Learning rate <span class="math notranslate nohighlight">\(\alpha\)</span></p></li>
<li><p>number of iterations</p></li>
<li><p>number of hidden layers</p></li>
<li><p>number of hidden units <span class="math notranslate nohighlight">\(n^{[1]}, n^{[2]}, \cdots\)</span></p></li>
<li><p>choice of activation function</p></li>
</ul>
</li>
</ul>
<p>So all of these things are things that you need to tell your learning algorithm and so these are parameters that control the ultimate parameters <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(b\)</span> and so we call all of these things below hyper parameters.</p>
<p>We call these things hyper parameters, because it is the hyper parameters that somehow determine the final value of the parameters <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(b\)</span> that you end up with. So when you’re training a deep net for your own application you find that there may be a lot of possible settings for the hyper parameters that you need to just try out.</p>
</section>
<section id="summary-with-formula">
<h3>Summary with formula<a class="headerlink" href="#summary-with-formula" title="Link to this heading">#</a></h3>
<div class="admonition important">
<p class="admonition-title">Important</p>
<p><strong>Forward propogation</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathbf{Z}^{[1]} &amp;= \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{A}^{[1]} &amp;= g^{[1]}({\mathbf{Z}^{[1]}}) \\
\mathbf{Z}^{[2]} &amp;= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{A}^{[2]} &amp;= g^{[2]}({\mathbf{Z}^{[2]}}) \\
\vdots \\
\mathbf{Z}^{[L]} &amp;= \mathbf{W}^{[L]} \mathbf{A}^{[L-1]} + \mathbf{b}^{[L]} \\
\mathbf{A}^{[L]} &amp;= g^{[L]}({\mathbf{Z}^{[L]}}) = \hat{Y}
\end{aligned}
\end{split}\]</div>
<p><strong>Backward propogation</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}\mathbf{Z}^{[l]} &amp;= \mathrm{d}\mathbf{A}^{[l]} *  g^{[l]'}(\mathbf{Z}^{[l]}) \\
\mathrm{d}\mathbf{W}^{[l]} &amp;= \dfrac{1}{m} \mathrm{d}\mathbf{Z}^{[l]} \cdot \mathbf{A}^{[l-1]T} \\
\mathrm{d}\mathbf{b}^{[l]} &amp;= \dfrac{1}{m} \text{np.sum}(\mathrm{d}\mathbf{Z}^{[l]} \text{, axis = }1 \text{, keepdims = True} )\\
\mathrm{d}\mathbf{Z}^{[l-1]} &amp;= \mathrm{d}\mathbf{W}^{[l]T} \cdot \mathrm{d}\mathbf{Z}^{[l]} * g^{[l-1]'}(\mathbf{Z}^{[l-1]}) \\
\vdots \\
\mathrm{d}\mathbf{Z}^{[1]} &amp;= \mathrm{d}\mathbf{W}^{[2]T} \cdot \mathrm{d}\mathbf{Z}^{[2]} * g^{[1]'}(\mathbf{Z}^{[1]}) \\
\mathrm{d}\mathbf{W}^{[1]} &amp;= \dfrac{1}{m} \mathrm{d}\mathbf{Z}^{[1]} \cdot \mathbf{A}^{[0]T} \\
\mathrm{d}\mathbf{b}^{[1]} &amp;= \dfrac{1}{m} \text{np.sum}(\mathrm{d}\mathbf{Z}^{[1]} \text{, axis = }1 \text{, keepdims = True} )\\
\end{aligned}
\end{split}\]</div>
<ul class="simple">
<li><p>Note that <span class="math notranslate nohighlight">\(*\)</span> denotes element-wise multiplication.</p></li>
<li><p>Note that <span class="math notranslate nohighlight">\(\mathbf{A}^{[0]T}\)</span> ia another way to denote the input features, which is also written as <span class="math notranslate nohighlight">\(\mathbf{X}^{T}\)</span>.</p></li>
</ul>
</div>
</section>
<section id="quiz">
<h3>Quiz<a class="headerlink" href="#quiz" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p><strong>True/False</strong> We use the “cache” in our implementation of forward and backward propagation to pass useful values to the next layer in the forward propagation.   __________</p></li>
<li><p>Among the following, which ones are “hyperparameters”? (Check all that apply.)</p>
<p>A. Size of the hidden layers <span class="math notranslate nohighlight">\(n^{[l]}\)</span></p>
<p>B. number of iterations</p>
<p>C. Weight matrices <span class="math notranslate nohighlight">\(W^{[l]}\)</span></p>
<p>D. Bias veactors <span class="math notranslate nohighlight">\(b^{[l]}\)</span></p>
<p>E. Activation values <span class="math notranslate nohighlight">\(a^{[l]}\)</span></p>
<p>F. Number of layers <span class="math notranslate nohighlight">\(L\)</span> in the neural network</p>
<p>G. Learning rate <span class="math notranslate nohighlight">\(\alpha\)</span></p>
</li>
<li><p>Which of the following is more likely related to the early layers of a deep neural network?</p>
<figure class="align-default" id="q3">
<a class="reference internal image-reference" href="_images/4-q3.png"><img alt="_images/4-q3.png" src="_images/4-q3.png" style="height: 220px;" /></a>
</figure>
</li>
<li><p><strong>True/False</strong> Vectorization allows us to compute <span class="math notranslate nohighlight">\(a^{[l]}\)</span> for all the examples on a batch at the same time without using a for loop.  __________</p></li>
<li><p>Consider the following neural network:</p>
<figure class="align-default" id="q5">
<a class="reference internal image-reference" href="_images/4-q5.png"><img alt="_images/4-q5.png" src="_images/4-q5.png" style="height: 160px;" /></a>
</figure>
<p>How many layers does this network have?</p>
<p>A. The number of layers <span class="math notranslate nohighlight">\(L\)</span> is 4. The number of hidden layers is 4.</p>
<p>B. The number of layers <span class="math notranslate nohighlight">\(L\)</span> is 5. The number of hidden layers is 4.</p>
<p>C. The number of layers <span class="math notranslate nohighlight">\(L\)</span> is 4. The number of hidden layers is 3.</p>
<p>D. The number of layers <span class="math notranslate nohighlight">\(L\)</span> is 3. The number of hidden layers is 3.</p>
</li>
<li><p><strong>True/False</strong> During forward propagation, in the forward function for a layer <span class="math notranslate nohighlight">\(l\)</span> you need to know what is the activation function in a layer (sigmoid, tanh, ReLU, etc.). During backpropagation, the corresponding backward function also needs to know what is the activation function for layer <span class="math notranslate nohighlight">\(l\)</span>, since the gradient depends on it.   __________</p></li>
<li><p><strong>True/False</strong> For any mathematical function you can compute with an L-layered deep neural network with N hidden units there is a shallow neural network that requires only <span class="math notranslate nohighlight">\(\text{log}(N)\)</span> units, but it is very difficult to train.  __________</p></li>
<li><p>Consider the following 2 hidden layer neural network:</p>
<figure class="align-default" id="q8">
<a class="reference internal image-reference" href="_images/4-q8.png"><img alt="_images/4-q8.png" src="_images/4-q8.png" style="height: 260px;" /></a>
</figure>
<p>Which of the following statements are True? (Check all that apply).</p>
<p>A. <span class="math notranslate nohighlight">\(b^{[3]}\)</span> will have shape <span class="math notranslate nohighlight">\((1,1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(W^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,4)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(W^{[3]}\)</span> will have shape <span class="math notranslate nohighlight">\((1,3)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(b^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((1,1)\)</span></p>
<p>E. <span class="math notranslate nohighlight">\(W^{[3]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,1)\)</span></p>
<p>F. <span class="math notranslate nohighlight">\(b^{[3]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,1)\)</span></p>
<p>G. <span class="math notranslate nohighlight">\(b^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,1)\)</span></p>
<p>H. <span class="math notranslate nohighlight">\(W^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((4,4)\)</span></p>
<p>I. <span class="math notranslate nohighlight">\(b^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((4,1)\)</span></p>
<p>J. <span class="math notranslate nohighlight">\(b^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,1)\)</span></p>
<p>K. <span class="math notranslate nohighlight">\(W^{[1]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,4)\)</span></p>
<p>L. <span class="math notranslate nohighlight">\(W^{[2]}\)</span> will have shape <span class="math notranslate nohighlight">\((3,1)\)</span></p>
</li>
<li><p>Whereas the previous question used a specific network, in the general case what is the dimension of <span class="math notranslate nohighlight">\(b^{[l]}\)</span>, the bias vector associated with layer <span class="math notranslate nohighlight">\(l\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(b^{[l]}\)</span> has shape <span class="math notranslate nohighlight">\((n^{[l]},1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(b^{[l]}\)</span> has shape <span class="math notranslate nohighlight">\((n^{[l+1]},1)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(b^{[l]}\)</span> has shape <span class="math notranslate nohighlight">\((1, n^{[l-1]})\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(b^{[l]}\)</span> has shape <span class="math notranslate nohighlight">\((1, n^{[l]})\)</span></p>
</li>
<li><p>What is the “cache” used for in our implementation of forward propagation and backward propagation?</p>
<p>A. It is used to keep track of the hyperparameters that we are searching over, to speed up computation.</p>
<p>B. We use it to pass variables computed during backward propagation to the corresponding forward propagation step. It contains useful values for forward propagation to compute activations.</p>
<p>C. It is used to cache the intermediate values of the cost function during training.</p>
<p>D. We use it to pass <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.</p>
</li>
<li><p><strong>True/False</strong> During the backpropagation process, we use gradient descent to change the hyperparameters.   __________</p></li>
<li><p><strong>True/False</strong> We can not use vectorization to calculate <span class="math notranslate nohighlight">\(\mathrm{d}A^{[l]}\)</span> in backpropagation, we must use a for loop over all the examples.  __________</p></li>
<li><p>Consider the following neural network:  What are all the values of <span class="math notranslate nohighlight">\(n^{[0]}\)</span>, <span class="math notranslate nohighlight">\(n^{[1]}\)</span>, <span class="math notranslate nohighlight">\(n^{[2]}\)</span>, <span class="math notranslate nohighlight">\(n^{[3]}\)</span> and <span class="math notranslate nohighlight">\(n^{[4]}\)</span>.</p>
<figure class="align-default" id="q13">
<a class="reference internal image-reference" href="_images/4-q13.png"><img alt="_images/4-q13.png" src="_images/4-q13.png" style="height: 260px;" /></a>
</figure>
</li>
<li><p><strong>True/False</strong> During forward propagation, to calculate <span class="math notranslate nohighlight">\(\mathrm{d}A^{[l]}\)</span>, you use the activation function <span class="math notranslate nohighlight">\(g^{[l]}\)</span> with the values of <span class="math notranslate nohighlight">\(Z^{[l]}\)</span>. During backward propagation, you calculate <span class="math notranslate nohighlight">\(\mathrm{d}A^{[l]}\)</span> from <span class="math notranslate nohighlight">\(Z^{[l]}\)</span>.  __________</p></li>
<li><p><strong>True/False</strong> A shallow neural network with a single hidden layer and 6 hidden units can compute any function that a neural network with 2 hidden layers and 6 hidden units can compute.  __________</p></li>
<li><p>Suppose <code class="docutils literal notranslate"><span class="pre">W[i]</span></code> is the array with the weights of the i-th layer, <code class="docutils literal notranslate"><span class="pre">b[i]</span></code> is the vector of biases of the i-th layer, and <code class="docutils literal notranslate"><span class="pre">g</span></code> is the activation function used in all layers. Which of the following calculates the forward propagation for the neural network with L layers.</p></li>
</ol>
<p>A.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> 
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
<p>B.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
<p>C.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
<p>D.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
    <span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">Z</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
<ol class="arabic" start="17">
<li><p>Which of the following are “parameters” of a neural network? (Check all that apply.)</p>
<p>A. <span class="math notranslate nohighlight">\(W^{[l]}\)</span> the weight matrices.</p>
<p>B. <span class="math notranslate nohighlight">\(b^{[l]}\)</span> the bias vector.</p>
<p>C. <span class="math notranslate nohighlight">\(g^{[l]}\)</span> the activation functions.</p>
<p>D. <span class="math notranslate nohighlight">\(L\)</span> the number of layers of the neural network.</p>
</li>
<li><p>Which of the following statements is true?</p>
<p>A. The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.</p>
<p>B. The earlier layers of a neural network are typically computing more complex features of the input than the deeper layers.</p>
</li>
<li><p>In the general case if we are training with <span class="math notranslate nohighlight">\(m\)</span> examples what is the shape of <span class="math notranslate nohighlight">\(A^{[l]}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((m, n^{[l+1]})\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((m, n^{[l]})\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((n^{[l]}, m)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((n^{[l+1]}, m)\)</span></p>
</li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Click here for answers!</p>
<ol class="arabic">
<li><p>False </br></p>
<p>The “cache” is used in our implementation to store values computed during forward propagation to be used in backward propagation. </br></p>
</li>
<li><p>ABFG </br></p></li>
<li><p>C </br></p>
<p>The early layer of a neural network usually computes simple features such as edges and lines. </br></p>
</li>
<li><p>True </br></p>
<p>Vectorization allows us to compute the activation for all the training examples at the same time, avoiding the use of a for loop.  </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes.  As seen in lecture, the number of layers is counted as the number of hidden layers + 1. The input and output layers are not counted as hidden layers. </br></p>
</li>
<li><p>True </br></p>
<p>Yes, as you’ve seen in chapter 3 each activation has a different derivative. Thus, during backpropagation you need to know which activation was used in the forward propagation to be able to compute the correct derivative. </br></p>
</li>
<li><p>False </br></p>
<p>On the contrary, some mathematical functions can be computed using an L-layered neural network and a given number of hidden units; but using a shallow neural network the number of necessary hidden units grows exponentially. </br></p>
</li>
<li><p>ABCHIG </br></p>
<p>More generally, the shape of <span class="math notranslate nohighlight">\(W^{[l]}\)</span> is <span class="math notranslate nohighlight">\((n^{[l]}, n^{[l-1]})\)</span>, and the the shape of <span class="math notranslate nohighlight">\(b^{[l]}\)</span> is <span class="math notranslate nohighlight">\((n^{[l]}, m)\)</span></br></p>
</li>
<li><p>A </br></p>
<p>A. Yes. <span class="math notranslate nohighlight">\(b^{[l]}\)</span> is a column vector with the same number of rows as units in the respective layer. </br></p>
</li>
<li><p>D </br></p>
<p>D. Correct, the “cache” records values from the forward propagation units and are used in backward propagation units because it is needed to compute the chain rule derivatives.</p>
</li>
<li><p>False </br></p>
<p>During backpropagation, we use gradient descent to compute new values of <span class="math notranslate nohighlight">\(W^{[l]}\)</span> and <span class="math notranslate nohighlight">\(b^{[l]}\)</span>. These are the parameters of the network. </br></p>
</li>
<li><p>False </br></p>
<p>We can use vectorization in backpropagation to calculate <span class="math notranslate nohighlight">\(\mathrm{d}A^{[l]}\)</span> for each layer. This computation is done over all the training examples. </br></p>
</li>
<li><p>4, 4, 3, 2, 1 </br></p></li>
<li><p>False </br></p>
<p>During backward propagation we are interested in computing <span class="math notranslate nohighlight">\(\mathrm{d}W^{[l]}\)</span> and <span class="math notranslate nohighlight">\(\mathrm{d}b^{[l]}\)</span>. For that we use  <span class="math notranslate nohighlight">\(g^{'[l]}\)</span>, <span class="math notranslate nohighlight">\(\mathrm{d}Z^{[l]}\)</span>, <span class="math notranslate nohighlight">\(Z^{[l]}\)</span> and <span class="math notranslate nohighlight">\(W^{[l]}\)</span>. </br></p>
</li>
<li><p>False </br></p>
<p>As seen during the lectures there are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute. </br></p>
</li>
<li><p>A </br></p>
<p>A. Remember that the range omits the last number thus the range from 1 to L calculates only the A up to the L-1 layer.</p>
</li>
<li><p>AB </br></p>
<p>AB. The weight matrices and the bias vectors are the parameters of the network. </br></p>
</li>
<li><p>A  </br></p></li>
<li><p>C </br></p>
<p>The number of rows in <span class="math notranslate nohighlight">\(A^{[l]}\)</span> corresponds to the number of units in the <span class="math notranslate nohighlight">\(l\)</span>-th layer. </br></p>
</li>
</ol>
</div>
</section>
<div class="toctree-wrapper compound">
<span id="document-C4_Practical_Test_P1"></span><section class="tex2jax_ignore mathjax_ignore" id="practical-3-building-your-deep-neural-network-step-by-step">
<h3>Practical 3: Building your Deep Neural Network: Step by Step<a class="headerlink" href="#practical-3-building-your-deep-neural-network-step-by-step" title="Link to this heading">#</a></h3>
<p>Welcome to your chapter 4 assignment (part 1 of 2)! Previously you trained a 2-layer Neural Network with a single hidden layer. In this chapter, you will build a deep neural network with as many layers as possible!</p>
<ul class="simple">
<li><p>In this notebook, you’ll implement all the functions required to build a deep neural network.</p></li>
<li><p>For the next assignment, you’ll use these functions to build a deep neural network for image classification.</p></li>
</ul>
<p><strong>By the end of this assignment, you’ll be able to:</strong></p>
<ul class="simple">
<li><p>Use non-linear units like ReLU to improve your model</p></li>
<li><p>Build a deeper neural network (with more than 1 hidden layer)</p></li>
<li><p>Implement an easy-to-use neural network class</p></li>
</ul>
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>Superscript <span class="math notranslate nohighlight">\([l]\)</span> denotes a quantity associated with the <span class="math notranslate nohighlight">\(l^{th}\)</span> layer.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(a^{[L]}\)</span> is the <span class="math notranslate nohighlight">\(L^{th}\)</span> layer activation. <span class="math notranslate nohighlight">\(W^{[L]}\)</span> and <span class="math notranslate nohighlight">\(b^{[L]}\)</span> are the <span class="math notranslate nohighlight">\(L^{th}\)</span> layer parameters.</p></li>
</ul>
</li>
<li><p>Superscript <span class="math notranslate nohighlight">\((i)\)</span> denotes a quantity associated with the <span class="math notranslate nohighlight">\(i^{th}\)</span> example.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(x^{(i)}\)</span> is the <span class="math notranslate nohighlight">\(i^{th}\)</span> training example.</p></li>
</ul>
</li>
<li><p>Lowerscript <span class="math notranslate nohighlight">\(i\)</span> denotes the <span class="math notranslate nohighlight">\(i^{th}\)</span> entry of a vector.</p>
<ul>
<li><p>Example: <span class="math notranslate nohighlight">\(a^{[l]}_i\)</span> denotes the <span class="math notranslate nohighlight">\(i^{th}\)</span> entry of the <span class="math notranslate nohighlight">\(l^{th}\)</span> layer’s activations).</p></li>
</ul>
</li>
</ul>
<p>Let’s get started!</p>
<section id="packages">
<h4>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h4>
<p>First, import all the packages you’ll need during this assignment.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#www.numpy.org"><span class="xref myst">numpy</span></a> is the main package for scientific computing with Python.</p></li>
<li><p><a class="reference external" href="http://matplotlib.org">matplotlib</a> is a library to plot graphs in Python.</p></li>
<li><p>dnn_utils provides some necessary functions for this notebook.</p></li>
<li><p>testCases provides some test cases to assess the correctness of your functions</p></li>
<li><p>np.random.seed(1) is used to keep all the random function calls consistent. It helps grade your work. Please don’t change the seed!</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">testCases_c1w4a1</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">dnn_utils_c1w4a1</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">sigmoid_backward</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">relu_backward</span>
<span class="kn">from</span> <span class="nn">public_tests_c1w4a1</span> <span class="kn">import</span> <span class="o">*</span>

<span class="kn">import</span> <span class="nn">copy</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span> <span class="c1"># set default size of plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.interpolation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.cmap&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="outline">
<h4>2 - Outline<a class="headerlink" href="#outline" title="Link to this heading">#</a></h4>
<p>To build your neural network, you’ll be implementing several “helper functions”. These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network.</p>
<p>Each small helper function will have detailed instructions to walk you through the necessary steps. Here’s an outline of the steps in this assignment:</p>
<ul class="simple">
<li><p>Initialize the parameters for a two-layer network and for an <span class="math notranslate nohighlight">\(L\)</span>-layer neural network</p></li>
<li><p>Implement the forward propagation module (shown in purple in the figure below)</p>
<ul>
<li><p>Complete the LINEAR part of a layer’s forward propagation step (resulting in <span class="math notranslate nohighlight">\(Z^{[l]}\)</span>).</p></li>
<li><p>The ACTIVATION function is provided for you (relu/sigmoid)</p></li>
<li><p>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function.</p></li>
<li><p>Stack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer <span class="math notranslate nohighlight">\(L\)</span>). This gives you a new L_model_forward function.</p></li>
</ul>
</li>
<li><p>Compute the loss</p></li>
<li><p>Implement the backward propagation module (denoted in red in the figure below)</p>
<ul>
<li><p>Complete the LINEAR part of a layer’s backward propagation step</p></li>
<li><p>The gradient of the ACTIVATION function is provided for you(relu_backward/sigmoid_backward)</p></li>
<li><p>Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function</p></li>
<li><p>Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function</p></li>
</ul>
</li>
<li><p>Finally, update the parameters</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/final_outline.png"><img alt="_images/final_outline.png" src="_images/final_outline.png" style="height: 500px;" /></a>
</figure>
<p><strong>Note</strong>:</p>
<p>For every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients.</p>
<p>In the backpropagation module, you can then use the cache to calculate the gradients. Don’t worry, this assignment will show you exactly how to carry out each of these steps!</p>
</section>
<section id="initialization">
<h4>3 - Initialization<a class="headerlink" href="#initialization" title="Link to this heading">#</a></h4>
<p>You will write two helper functions to initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one generalizes this initialization process to <span class="math notranslate nohighlight">\(L\)</span> layers.</p>
<section id="layer-neural-network">
<h5>3.1 - 2-layer Neural Network<a class="headerlink" href="#layer-neural-network" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-1-initialize-parameters">
<h5>Exercise 1 - initialize_parameters<a class="headerlink" href="#exercise-1-initialize-parameters" title="Link to this heading">#</a></h5>
<p>Create and initialize the parameters of the 2-layer neural network.</p>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p>The model’s structure is: <em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em>.</p></li>
<li><p>Use this random initialization for the weight matrices: <code class="docutils literal notranslate"><span class="pre">np.random.randn(d0,</span> <span class="pre">d1,</span> <span class="pre">...,</span> <span class="pre">dn)</span> <span class="pre">*</span> <span class="pre">0.01</span></code> with the correct shape. The documentation for <a class="reference external" href="https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html">np.random.randn</a></p></li>
<li><p>Use zero initialization for the biases: <code class="docutils literal notranslate"><span class="pre">np.zeros(shape)</span></code>. The documentation for <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html">np.zeros</a></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters</span>

<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Argument:</span>
<span class="sd">    n_x -- size of the input layer</span>
<span class="sd">    n_h -- size of the hidden layer</span>
<span class="sd">    n_y -- size of the output layer</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters:</span>
<span class="sd">                    W1 -- weight matrix of shape (n_h, n_x)</span>
<span class="sd">                    b1 -- bias vector of shape (n_h, 1)</span>
<span class="sd">                    W2 -- weight matrix of shape (n_y, n_h)</span>
<span class="sd">                    b2 -- bias vector of shape (n_y, 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># W1 = ...</span>
    <span class="c1"># b1 = ...</span>
    <span class="c1"># W2 = ...</span>
    <span class="c1"># b2 = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span> <span class="n">n_x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_h</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_y</span><span class="p">,</span> <span class="n">n_h</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;W1&quot;</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s2">&quot;b1&quot;</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s2">&quot;W2&quot;</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s2">&quot;b2&quot;</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">parameters</span>    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Case 1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="n">initialize_parameters_test_1</span><span class="p">(</span><span class="n">initialize_parameters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[90m</span><span class="se">\n</span><span class="s2">Test Case 2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="n">initialize_parameters_test_2</span><span class="p">(</span><span class="n">initialize_parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Case 1:

W1 = [[ 0.01624345 -0.00611756 -0.00528172]
 [-0.01072969  0.00865408 -0.02301539]]
b1 = [[0.]
 [0.]]
W2 = [[ 0.01744812 -0.00761207]]
b2 = [[0.]]
 All tests passed.

Test Case 2:

W1 = [[ 0.01624345 -0.00611756 -0.00528172 -0.01072969]
 [ 0.00865408 -0.02301539  0.01744812 -0.00761207]
 [ 0.00319039 -0.0024937   0.01462108 -0.02060141]]
b1 = [[0.]
 [0.]
 [0.]]
W2 = [[-0.00322417 -0.00384054  0.01133769]
 [-0.01099891 -0.00172428 -0.00877858]]
b2 = [[0.]
 [0.]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="l-layer-neural-network">
<h5>3.2 - L-layer Neural Network<a class="headerlink" href="#l-layer-neural-network" title="Link to this heading">#</a></h5>
<p>The initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the <code class="docutils literal notranslate"><span class="pre">initialize_parameters_deep</span></code> function, you should make sure that your dimensions match between each layer. Recall that <span class="math notranslate nohighlight">\(n^{[l]}\)</span> is the number of units in layer <span class="math notranslate nohighlight">\(l\)</span>. For example, if the size of your input <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\((12288, 209)\)</span> (with <span class="math notranslate nohighlight">\(m=209\)</span> examples) then:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>Shape of W</p></th>
<th class="head"><p>Shape of b</p></th>
<th class="head"><p>Activation</p></th>
<th class="head"><p>Shape of Activation</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Layer 1</strong></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[1]},12288)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[1]},1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Z^{[1]} = W^{[1]}  X + b^{[1]} \)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[1]},209)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Layer 2</strong></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[2]}, n^{[1]})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[2]},1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[2]}, 209)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Layer L-1</strong></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[L-1]}, n^{[L-2]})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[L-1]}, 1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[L-1]}, 209)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>Layer L</strong></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[L]}, n^{[L-1]})\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[L]}, 1)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\((n^{[L]}, 209)\)</span></p></td>
</tr>
</tbody>
</table>
<p>Remember that when you compute <span class="math notranslate nohighlight">\(W X + b\)</span> in python, it carries out broadcasting. For example, if:</p>
<div class="math notranslate nohighlight">
\[\begin{split} W = \begin{bmatrix}
    w_{00}  &amp; w_{01} &amp; w_{02} \\
    w_{10}  &amp; w_{11} &amp; w_{12} \\
    w_{20}  &amp; w_{21} &amp; w_{22} 
\end{bmatrix}\;\;\; X = \begin{bmatrix}
    x_{00}  &amp; x_{01} &amp; x_{02} \\
    x_{10}  &amp; x_{11} &amp; x_{12} \\
    x_{20}  &amp; x_{21} &amp; x_{22} 
\end{bmatrix} \;\;\; b =\begin{bmatrix}
    b_0  \\
    b_1  \\
    b_2
\end{bmatrix}\end{split}\]</div>
<p>Then <span class="math notranslate nohighlight">\(WX + b\)</span> will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split} WX + b = \begin{bmatrix}
    (w_{00}x_{00} + w_{01}x_{10} + w_{02}x_{20}) + b_0 &amp; (w_{00}x_{01} + w_{01}x_{11} + w_{02}x_{21}) + b_0 &amp; \cdots \\
    (w_{10}x_{00} + w_{11}x_{10} + w_{12}x_{20}) + b_1 &amp; (w_{10}x_{01} + w_{11}x_{11} + w_{12}x_{21}) + b_1 &amp; \cdots \\
    (w_{20}x_{00} + w_{21}x_{10} + w_{22}x_{20}) + b_2 &amp;  (w_{20}x_{01} + w_{21}x_{11} + w_{22}x_{21}) + b_2 &amp; \cdots
\end{bmatrix}\end{split}\]</div>
</section>
<section id="exercise-2-initialize-parameters-deep">
<h5>Exercise 2 -  initialize_parameters_deep<a class="headerlink" href="#exercise-2-initialize-parameters-deep" title="Link to this heading">#</a></h5>
<p>Implement initialization for an L-layer Neural Network.</p>
<p><strong>Instructions</strong>:</p>
<ul class="simple">
<li><p>The model’s structure is <em>[LINEAR -&gt; RELU] <span class="math notranslate nohighlight">\( \times\)</span> (L-1) -&gt; LINEAR -&gt; SIGMOID</em>. I.e., it has <span class="math notranslate nohighlight">\(L-1\)</span> layers using a ReLU activation function followed by an output layer with a sigmoid activation function.</p></li>
<li><p>Use random initialization for the weight matrices. Use <code class="docutils literal notranslate"><span class="pre">np.random.randn(d0,</span> <span class="pre">d1,</span> <span class="pre">...,</span> <span class="pre">dn)</span> <span class="pre">*</span> <span class="pre">0.01</span></code>.</p></li>
<li><p>Use zeros initialization for the biases. Use <code class="docutils literal notranslate"><span class="pre">np.zeros(shape)</span></code>.</p></li>
<li><p>You’ll store <span class="math notranslate nohighlight">\(n^{[l]}\)</span>, the number of units in different layers, in a variable <code class="docutils literal notranslate"><span class="pre">layer_dims</span></code>. For example, the <code class="docutils literal notranslate"><span class="pre">layer_dims</span></code> for last week’s Planar Data classification model would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. This means <code class="docutils literal notranslate"><span class="pre">W1</span></code>’s shape was (4,2), <code class="docutils literal notranslate"><span class="pre">b1</span></code> was (4,1), <code class="docutils literal notranslate"><span class="pre">W2</span></code> was (1,4) and <code class="docutils literal notranslate"><span class="pre">b2</span></code> was (1,1). Now you will generalize this to <span class="math notranslate nohighlight">\(L\)</span> layers!</p></li>
<li><p>Here is the implementation for <span class="math notranslate nohighlight">\(L=1\)</span> (one layer neural network). It should inspire you to implement the general case (L-layer neural network).</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">if</span> <span class="n">L</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer_dims</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_deep</span>

<span class="k">def</span> <span class="nf">initialize_parameters_deep</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the dimensions of each layer in our network</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])</span>
<span class="sd">                    bl -- bias vector of shape (layer_dims[l], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">)</span> <span class="c1"># number of layers in the network</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = ...</span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="k">assert</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]))</span>
        <span class="k">assert</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="n">layer_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>

        
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test Case 1:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_deep</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="n">initialize_parameters_deep_test_1</span><span class="p">(</span><span class="n">initialize_parameters_deep</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\033</span><span class="s2">[90m</span><span class="se">\n</span><span class="s2">Test Case 2:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_deep</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>
<span class="n">initialize_parameters_deep_test_2</span><span class="p">(</span><span class="n">initialize_parameters_deep</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Test Case 1:

W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]
 [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]
 [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]
 [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]
 [-0.01023785 -0.00712993  0.00625245 -0.00160513]
 [-0.00768836 -0.00230031  0.00745056  0.01976111]]
b2 = [[0.]
 [0.]
 [0.]]
 All tests passed.

Test Case 2:

W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493]
 [-0.00277388 -0.00354759 -0.00082741 -0.00627001]
 [-0.00043818 -0.00477218 -0.01313865  0.00884622]]
b1 = [[0.]
 [0.]
 [0.]]
W2 = [[ 0.00881318  0.01709573  0.00050034]
 [-0.00404677 -0.0054536  -0.01546477]]
b2 = [[0.]
 [0.]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="forward-propagation-module">
<h4>4 - Forward Propagation Module<a class="headerlink" href="#forward-propagation-module" title="Link to this heading">#</a></h4>
<section id="linear-forward">
<h5>4.1 - Linear Forward<a class="headerlink" href="#linear-forward" title="Link to this heading">#</a></h5>
<p>Now that you have initialized your parameters, you can do the forward propagation module. Start by implementing some basic functions that you can use again later when implementing the model. Now, you’ll complete three functions in this order:</p>
<ul class="simple">
<li><p>LINEAR</p></li>
<li><p>LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid.</p></li>
<li><p>[LINEAR -&gt; RELU] <span class="math notranslate nohighlight">\(\times\)</span> (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model)</p></li>
</ul>
<p>The linear forward module (vectorized over all the examples) computes the following equations:</p>
<div class="math notranslate nohighlight">
\[Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}\]</div>
<p>where <span class="math notranslate nohighlight">\(A^{[0]} = X\)</span>.</p>
</section>
<section id="exercise-3-linear-forward">
<h5>Exercise 3 - linear_forward<a class="headerlink" href="#exercise-3-linear-forward" title="Link to this heading">#</a></h5>
<p>Build the linear part of forward propagation.</p>
<p><strong>Reminder</strong>:
The mathematical representation of this unit is <span class="math notranslate nohighlight">\(Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\)</span>. You may also find <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code> useful. If your dimensions don’t match, printing <code class="docutils literal notranslate"><span class="pre">W.shape</span></code> may help.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: linear_forward</span>

<span class="k">def</span> <span class="nf">linear_forward</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the linear part of a layer&#39;s forward propagation.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    A -- activations from previous layer (or input data): (size of previous layer, number of examples)</span>
<span class="sd">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span>
<span class="sd">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span>

<span class="sd">    Returns:</span>
<span class="sd">    Z -- the input of the activation function, also called pre-activation parameter </span>
<span class="sd">    cache -- a python tuple containing &quot;A&quot;, &quot;W&quot; and &quot;b&quot; ; stored for computing the backward pass efficiently</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># Z = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_A</span><span class="p">,</span> <span class="n">t_W</span><span class="p">,</span> <span class="n">t_b</span> <span class="o">=</span> <span class="n">linear_forward_test_case</span><span class="p">()</span>
<span class="n">t_Z</span><span class="p">,</span> <span class="n">t_linear_cache</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">t_A</span><span class="p">,</span> <span class="n">t_W</span><span class="p">,</span> <span class="n">t_b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Z = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_Z</span><span class="p">))</span>

<span class="n">linear_forward_test</span><span class="p">(</span><span class="n">linear_forward</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Z = [[ 3.26295337 -1.23429987]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="linear-activation-forward">
<h5>4.2 - Linear-Activation Forward<a class="headerlink" href="#linear-activation-forward" title="Link to this heading">#</a></h5>
<p>In this notebook, you will use two activation functions:</p>
<ul class="simple">
<li><p><strong>Sigmoid</strong>: <span class="math notranslate nohighlight">\(\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)}}\)</span>. You’ve been provided with the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function which returns <strong>two</strong> items: the activation value “<code class="docutils literal notranslate"><span class="pre">a</span></code>” and a “<code class="docutils literal notranslate"><span class="pre">cache</span></code>” that contains “<code class="docutils literal notranslate"><span class="pre">Z</span></code>” (it’s what we will feed in to the corresponding backward function). To use it you could just call:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>ReLU</strong>: The mathematical formula for ReLu is <span class="math notranslate nohighlight">\(A = RELU(Z) = \text{max}(0, Z)\)</span>. You’ve been provided with the <code class="docutils literal notranslate"><span class="pre">relu</span></code> function. This function returns <strong>two</strong> items: the activation value “<code class="docutils literal notranslate"><span class="pre">A</span></code>” and a “<code class="docutils literal notranslate"><span class="pre">cache</span></code>” that contains “<code class="docutils literal notranslate"><span class="pre">Z</span></code>” (it’s what you’ll feed in to the corresponding backward function). To use it you could just call:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
</pre></div>
</div>
<p>For added convenience, you’re going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you’ll implement a function that does the LINEAR forward step, followed by an ACTIVATION forward step.</p>
</section>
<section id="exercise-4-linear-activation-forward">
<h5>Exercise 4 - linear_activation_forward<a class="headerlink" href="#exercise-4-linear-activation-forward" title="Link to this heading">#</a></h5>
<p>Implement the forward propagation of the <em>LINEAR-&gt;ACTIVATION</em> layer. Mathematical relation is: <span class="math notranslate nohighlight">\(A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})\)</span> where the activation “g” can be sigmoid() or relu(). Use <code class="docutils literal notranslate"><span class="pre">linear_forward()</span></code> and the correct activation function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: linear_activation_forward</span>

<span class="k">def</span> <span class="nf">linear_activation_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer</span>

<span class="sd">    Arguments:</span>
<span class="sd">    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)</span>
<span class="sd">    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)</span>
<span class="sd">    b -- bias vector, numpy array of shape (size of the current layer, 1)</span>
<span class="sd">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span>

<span class="sd">    Returns:</span>
<span class="sd">    A -- the output of the activation function, also called the post-activation value </span>
<span class="sd">    cache -- a python tuple containing &quot;linear_cache&quot; and &quot;activation_cache&quot;;</span>
<span class="sd">             stored for computing the backward pass efficiently</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># Z, linear_cache = ...</span>
        <span class="c1"># A, activation_cache = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">Z</span><span class="p">,</span> <span class="n">linear_cache</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">Z</span><span class="p">)),</span> <span class="n">Z</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># Z, linear_cache = ...</span>
        <span class="c1"># A, activation_cache = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">Z</span><span class="p">,</span> <span class="n">linear_cache</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Z</span><span class="p">),</span> <span class="n">Z</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">linear_cache</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_A_prev</span><span class="p">,</span> <span class="n">t_W</span><span class="p">,</span> <span class="n">t_b</span> <span class="o">=</span> <span class="n">linear_activation_forward_test_case</span><span class="p">()</span>

<span class="n">t_A</span><span class="p">,</span> <span class="n">t_linear_activation_cache</span> <span class="o">=</span> <span class="n">linear_activation_forward</span><span class="p">(</span><span class="n">t_A_prev</span><span class="p">,</span> <span class="n">t_W</span><span class="p">,</span> <span class="n">t_b</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With sigmoid: A = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_A</span><span class="p">))</span>

<span class="n">t_A</span><span class="p">,</span> <span class="n">t_linear_activation_cache</span> <span class="o">=</span> <span class="n">linear_activation_forward</span><span class="p">(</span><span class="n">t_A_prev</span><span class="p">,</span> <span class="n">t_W</span><span class="p">,</span> <span class="n">t_b</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With ReLU: A = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_A</span><span class="p">))</span>

<span class="n">linear_activation_forward_test</span><span class="p">(</span><span class="n">linear_activation_forward</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>With sigmoid: A = [[0.96890023 0.11013289]]
With ReLU: A = [[3.43896131 0.        ]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="l-layer-model">
<h5>4.3 - L-Layer Model<a class="headerlink" href="#l-layer-model" title="Link to this heading">#</a></h5>
<p>For even <em>more</em> convenience when implementing the <span class="math notranslate nohighlight">\(L\)</span>-layer Neural Net, you will need a function that replicates the previous one (<code class="docutils literal notranslate"><span class="pre">linear_activation_forward</span></code> with RELU) <span class="math notranslate nohighlight">\(L-1\)</span> times, then follows that with one <code class="docutils literal notranslate"><span class="pre">linear_activation_forward</span></code> with SIGMOID.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/model_architecture_kiank.png"><img alt="_images/model_architecture_kiank.png" src="_images/model_architecture_kiank.png" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">[LINEAR -&gt; RELU] <span class="math notranslate nohighlight">\(\times\)</span> (L-1) -&gt; LINEAR -&gt; SIGMOID model</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="exercise-5-l-model-forward">
<h5>Exercise 5 -  L_model_forward<a class="headerlink" href="#exercise-5-l-model-forward" title="Link to this heading">#</a></h5>
<p>Implement the forward propagation of the above model.</p>
<p><strong>Instructions</strong>: In the code below, the variable <code class="docutils literal notranslate"><span class="pre">AL</span></code> will denote <span class="math notranslate nohighlight">\(A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]} A^{[L-1]} + b^{[L]})\)</span>. (This is sometimes also called <code class="docutils literal notranslate"><span class="pre">Yhat</span></code>, i.e., this is <span class="math notranslate nohighlight">\(\hat{Y}\)</span>.)</p>
<p><strong>Hints</strong>:</p>
<ul class="simple">
<li><p>Use the functions you’ve previously written</p></li>
<li><p>Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times</p></li>
<li><p>Don’t forget to keep track of the caches in the “caches” list. To add a new value <code class="docutils literal notranslate"><span class="pre">c</span></code> to a <code class="docutils literal notranslate"><span class="pre">list</span></code>, you can use <code class="docutils literal notranslate"><span class="pre">list.append(c)</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: L_model_forward</span>

<span class="k">def</span> <span class="nf">L_model_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- data, numpy array of shape (input size, number of examples)</span>
<span class="sd">    parameters -- output of initialize_parameters_deep()</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    AL -- activation value from the output (last) layer</span>
<span class="sd">    caches -- list of caches containing:</span>
<span class="sd">                every cache of linear_activation_forward() (there are L of them, indexed from 0 to L-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">caches</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">X</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span>                  <span class="c1"># number of layers in the neural network</span>
    
    <span class="c1"># Implement [LINEAR -&gt; RELU]*(L-1). Add &quot;cache&quot; to the &quot;caches&quot; list.</span>
    <span class="c1"># The for loop starts at 1 because layer 0 is the input</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="n">A_prev</span> <span class="o">=</span> <span class="n">A</span> 
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># A, cache = ...</span>
        <span class="c1"># caches ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">linear_activation_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)],</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Implement LINEAR -&gt; SIGMOID. Add &quot;cache&quot; to the &quot;caches&quot; list.</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># AL, cache = ...</span>
    <span class="c1"># caches ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">AL</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">linear_activation_forward</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)],</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)],</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
    <span class="n">caches</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cache</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
          
    <span class="k">return</span> <span class="n">AL</span><span class="p">,</span> <span class="n">caches</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_parameters</span> <span class="o">=</span> <span class="n">L_model_forward_test_case_2hidden</span><span class="p">()</span>
<span class="n">t_AL</span><span class="p">,</span> <span class="n">t_caches</span> <span class="o">=</span> <span class="n">L_model_forward</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_parameters</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AL = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_AL</span><span class="p">))</span>

<span class="n">L_model_forward_test</span><span class="p">(</span><span class="n">L_model_forward</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
<p><strong>Awesome!</strong> You’ve implemented a full forward propagation that takes the input X and outputs a row vector <span class="math notranslate nohighlight">\(A^{[L]}\)</span> containing your predictions. It also records all intermediate values in “caches”. Using <span class="math notranslate nohighlight">\(A^{[L]}\)</span>, you can compute the cost of your predictions.</p>
</section>
</section>
<section id="cost-function">
<h4>5 - Cost Function<a class="headerlink" href="#cost-function" title="Link to this heading">#</a></h4>
<p>Now you can implement forward and backward propagation! You need to compute the cost, in order to check whether your model is actually learning.</p>
<section id="exercise-6-compute-cost">
<h5>Exercise 6 - compute_cost<a class="headerlink" href="#exercise-6-compute-cost" title="Link to this heading">#</a></h5>
<p>Compute the cross-entropy cost <span class="math notranslate nohighlight">\(J\)</span>, using the following formula:</p>
<div class="math notranslate nohighlight">
\[-\dfrac{1}{m} \sum\limits_{i = 1}^{m} \Big(y^{(i)}\log(a^{[L] (i)}) + (1-y^{(i)})\log(1- a^{[L](i)})\Big) \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: compute_cost</span>

<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the cost function defined by equation (7).</span>

<span class="sd">    Arguments:</span>
<span class="sd">    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)</span>

<span class="sd">    Returns:</span>
<span class="sd">    cost -- cross-entropy cost</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Compute loss from aL and y.</span>
    <span class="c1"># (≈ 1 lines of code)</span>
    <span class="c1"># cost = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">AL</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">AL</span><span class="p">)))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>      <span class="c1"># To make sure your cost&#39;s shape is what we expect (e.g. this turns [[17]] into 17).</span>

    
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_Y</span><span class="p">,</span> <span class="n">t_AL</span> <span class="o">=</span> <span class="n">compute_cost_test_case</span><span class="p">()</span>
<span class="n">t_cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">t_AL</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_cost</span><span class="p">))</span>

<span class="n">compute_cost_test</span><span class="p">(</span><span class="n">compute_cost</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost: 0.2797765635793422
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="backward-propagation-module">
<h4>6 - Backward Propagation Module<a class="headerlink" href="#backward-propagation-module" title="Link to this heading">#</a></h4>
<p>Just as you did for the forward propagation, you’ll implement helper functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters.</p>
<p><strong>Reminder</strong>:</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/backprop_kiank.png"><img alt="_images/backprop_kiank.png" src="_images/backprop_kiank.png" style="height: 260px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Forward and Backward propagation for LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For those of you who are experts in calculus (which you don’t need to be to do this assignment!), the chain rule of calculus can be used to derive the derivative of the loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to <span class="math notranslate nohighlight">\(z^{[1]}\)</span> in a 2-layer network as follows:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\mathrm{d} \mathcal{L}(a^{[2]},y)}{{\mathrm{d}z^{[1]}}} = \dfrac{\mathrm{d}\mathcal{L}(a^{[2]},y)}{{\mathrm{d}a^{[2]}}}\dfrac{{\mathrm{d}a^{[2]}}}{{\mathrm{d}z^{[2]}}}\dfrac{{\mathrm{d}z^{[2]}}}{{\mathrm{d}a^{[1]}}}\dfrac{{\mathrm{d}a^{[1]}}}{{\mathrm{d}z^{[1]}}} \tag{8} \]</div>
<p>In order to calculate the gradient <span class="math notranslate nohighlight">\(\mathrm{d}W^{[1]} = \dfrac{\partial L}{\partial W^{[1]}}\)</span>, use the previous chain rule and you do <span class="math notranslate nohighlight">\(\mathrm{d}W^{[1]} = \mathrm{d}z^{[1]} \times \dfrac{\partial z^{[1]} }{\partial W^{[1]}}\)</span>. During backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.</p>
<p>Equivalently, in order to calculate the gradient <span class="math notranslate nohighlight">\(\mathrm{d}b^{[1]} = \dfrac{\partial L}{\partial b^{[1]}}\)</span>, you use the previous chain rule and you do <span class="math notranslate nohighlight">\(\mathrm{d}b^{[1]} = \mathrm{d}z^{[1]} \times \dfrac{\partial z^{[1]} }{\partial b^{[1]}}\)</span>.</p>
<p>This is why we talk about <strong>backpropagation</strong>.</p>
<p>Now, similarly to forward propagation, you’re going to build the backward propagation in three steps:</p>
<ol class="arabic simple">
<li><p>LINEAR backward</p></li>
<li><p>LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation</p></li>
<li><p>[LINEAR -&gt; RELU] <span class="math notranslate nohighlight">\(\times\)</span> (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model)</p></li>
</ol>
<p>For the next exercise, you will need to remember that:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> is a matrix(np.ndarray) with 1 column and n rows, i.e: b = [[1.0], [2.0]] (remember that <code class="docutils literal notranslate"><span class="pre">b</span></code> is a constant)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">np.sum</span></code> performs a sum over the elements of a ndarray</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">axis=1</span></code> or <code class="docutils literal notranslate"><span class="pre">axis=0</span></code> specify if the sum is carried out by rows or by columns respectively</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">keepdims</span></code> specifies if the original dimensions of the matrix must be kept.</p></li>
<li><p>Look at the following example to clarify:</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;axis=1 and keepdims=True&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;axis=1 and keepdims=False&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;axis=0 and keepdims=True&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;axis=0 and keepdims=False&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>axis=1 and keepdims=True
[[3]
 [7]]
axis=1 and keepdims=False
[3 7]
axis=0 and keepdims=True
[[4 6]]
axis=0 and keepdims=False
[4 6]
</pre></div>
</div>
</div>
</div>
<section id="linear-backward">
<h5>6.1 - Linear Backward<a class="headerlink" href="#linear-backward" title="Link to this heading">#</a></h5>
<p>For layer <span class="math notranslate nohighlight">\(l\)</span>, the linear part is: <span class="math notranslate nohighlight">\(Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\)</span> (followed by an activation).</p>
<p>Suppose you have already calculated the derivative <span class="math notranslate nohighlight">\(\mathrm{d}Z^{[l]} = \dfrac{\partial \mathcal{L} }{\partial Z^{[l]}}\)</span>. You want to get <span class="math notranslate nohighlight">\((\mathrm{d}W^{[l]}, \mathrm{d}b^{[l]}, \mathrm{d}A^{[l-1]})\)</span>.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="_images/linearback_kiank.png"><img alt="_images/linearback_kiank.png" src="_images/linearback_kiank.png" style="height: 300px;" /></a>
</figure>
<p>The three outputs <span class="math notranslate nohighlight">\((\mathrm{d}W^{[l]}, \mathrm{d}b^{[l]}, \mathrm{d}A^{[l-1]})\)</span> are computed using the input <span class="math notranslate nohighlight">\(\mathrm{d}Z^{[l]}\)</span>.</p>
<p>Here are the formulas you need:</p>
<div class="math notranslate nohighlight">
\[ \mathrm{d}W^{[l]} = \dfrac{\partial \mathcal{J} }{\partial W^{[l]}} = \dfrac{1}{m} \mathrm{d}Z^{[l]} A^{[l-1] T} \tag{8}\]</div>
<div class="math notranslate nohighlight">
\[ \mathrm{d}b^{[l]} = \dfrac{\partial \mathcal{J} }{\partial b^{[l]}} = \dfrac{1}{m} \sum_{i = 1}^{m} \mathrm{d}Z^{[l](i)}\tag{9}\]</div>
<div class="math notranslate nohighlight">
\[ \mathrm{d}A^{[l-1]} = \dfrac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} \mathrm{d}Z^{[l]} \tag{10}\]</div>
<p><span class="math notranslate nohighlight">\(A^{[l-1] T}\)</span> is the transpose of <span class="math notranslate nohighlight">\(A^{[l-1]}\)</span>.</p>
</section>
<section id="exercise-7-linear-backward">
<h5>Exercise 7 - linear_backward<a class="headerlink" href="#exercise-7-linear-backward" title="Link to this heading">#</a></h5>
<p>Use the 3 formulas above to implement <code class="docutils literal notranslate"><span class="pre">linear_backward()</span></code>.</p>
<p><strong>Hint</strong>:</p>
<ul class="simple">
<li><p>In numpy you can get the transpose of an ndarray <code class="docutils literal notranslate"><span class="pre">A</span></code> using <code class="docutils literal notranslate"><span class="pre">A.T</span></code> or <code class="docutils literal notranslate"><span class="pre">A.transpose()</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: linear_backward</span>

<span class="k">def</span> <span class="nf">linear_backward</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the linear portion of backward propagation for a single layer (layer l)</span>

<span class="sd">    Arguments:</span>
<span class="sd">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)</span>
<span class="sd">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer</span>

<span class="sd">    Returns:</span>
<span class="sd">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span>
<span class="sd">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span>
<span class="sd">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1">### START CODE HERE ### (≈ 3 lines of code)</span>
    <span class="c1"># dW = ...</span>
    <span class="c1"># db = ... sum by the rows of dZ with keepdims=True</span>
    <span class="c1"># dA_prev = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dW</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_dZ</span><span class="p">,</span> <span class="n">t_linear_cache</span> <span class="o">=</span> <span class="n">linear_backward_test_case</span><span class="p">()</span>
<span class="n">t_dA_prev</span><span class="p">,</span> <span class="n">t_dW</span><span class="p">,</span> <span class="n">t_db</span> <span class="o">=</span> <span class="n">linear_backward</span><span class="p">(</span><span class="n">t_dZ</span><span class="p">,</span> <span class="n">t_linear_cache</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dA_prev: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_dA_prev</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dW: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_dW</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;db: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_db</span><span class="p">))</span>

<span class="n">linear_backward_test</span><span class="p">(</span><span class="n">linear_backward</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]
 [ 0.60345879 -3.72508701  5.81700741 -3.84326836]
 [-0.4319552  -1.30987417  1.72354705  0.05070578]
 [-0.38981415  0.60811244 -1.25938424  1.47191593]
 [-2.52214926  2.67882552 -0.67947465  1.48119548]]
dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]
 [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]
 [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]
db: [[-0.14713786]
 [-0.11313155]
 [-0.13209101]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="linear-activation-backward">
<h5>6.2 - Linear-Activation Backward<a class="headerlink" href="#linear-activation-backward" title="Link to this heading">#</a></h5>
<p>Next, you will create a function that merges the two helper functions: <strong><code class="docutils literal notranslate"><span class="pre">linear_backward</span></code></strong> and the backward step for the activation <strong><code class="docutils literal notranslate"><span class="pre">linear_activation_backward</span></code></strong>.</p>
<p>To help you implement <code class="docutils literal notranslate"><span class="pre">linear_activation_backward</span></code>, two backward functions have been provided:</p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">sigmoid_backward</span></code></strong>: Implements the backward propagation for SIGMOID unit. You can call it as follows:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dZ</span> <span class="o">=</span> <span class="n">sigmoid_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">relu_backward</span></code></strong>: Implements the backward propagation for RELU unit. You can call it as follows:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dZ</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
</pre></div>
</div>
<p>If <span class="math notranslate nohighlight">\(g(.)\)</span> is the activation function,
<code class="docutils literal notranslate"><span class="pre">sigmoid_backward</span></code> and <code class="docutils literal notranslate"><span class="pre">relu_backward</span></code> compute</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}Z^{[l]} = \mathrm{d}A^{[l]} * g'(Z^{[l]}). \tag{11}\]</div>
</section>
<section id="exercise-8-linear-activation-backward">
<h5>Exercise 8 -  linear_activation_backward<a class="headerlink" href="#exercise-8-linear-activation-backward" title="Link to this heading">#</a></h5>
<p>Implement the backpropagation for the <em>LINEAR-&gt;ACTIVATION</em> layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: linear_activation_backward</span>

<span class="k">def</span> <span class="nf">linear_activation_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    dA -- post-activation gradient for current layer l </span>
<span class="sd">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently</span>
<span class="sd">    activation -- the activation to be used in this layer, stored as a text string: &quot;sigmoid&quot; or &quot;relu&quot;</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev</span>
<span class="sd">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W</span>
<span class="sd">    db -- Gradient of the cost with respect to b (current layer l), same shape as b</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">linear_cache</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># dZ =  ...</span>
        <span class="c1"># dA_prev, dW, db =  ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">dZ</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">linear_backward</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">linear_cache</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">:</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># dZ =  ...</span>
        <span class="c1"># dA_prev, dW, db =  ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">dZ</span> <span class="o">=</span> <span class="n">sigmoid_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">linear_backward</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">linear_cache</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_dAL</span><span class="p">,</span> <span class="n">t_linear_activation_cache</span> <span class="o">=</span> <span class="n">linear_activation_backward_test_case</span><span class="p">()</span>

<span class="n">t_dA_prev</span><span class="p">,</span> <span class="n">t_dW</span><span class="p">,</span> <span class="n">t_db</span> <span class="o">=</span> <span class="n">linear_activation_backward</span><span class="p">(</span><span class="n">t_dAL</span><span class="p">,</span> <span class="n">t_linear_activation_cache</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With sigmoid: dA_prev = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_dA_prev</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With sigmoid: dW = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_dW</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With sigmoid: db = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_db</span><span class="p">))</span>

<span class="n">t_dA_prev</span><span class="p">,</span> <span class="n">t_dW</span><span class="p">,</span> <span class="n">t_db</span> <span class="o">=</span> <span class="n">linear_activation_backward</span><span class="p">(</span><span class="n">t_dAL</span><span class="p">,</span> <span class="n">t_linear_activation_cache</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With relu: dA_prev = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_dA_prev</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With relu: dW = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_dW</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With relu: db = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_db</span><span class="p">))</span>

<span class="n">linear_activation_backward_test</span><span class="p">(</span><span class="n">linear_activation_backward</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>With sigmoid: dA_prev = [[ 0.11017994  0.01105339]
 [ 0.09466817  0.00949723]
 [-0.05743092 -0.00576154]]
With sigmoid: dW = [[ 0.10266786  0.09778551 -0.01968084]]
With sigmoid: db = [[-0.05729622]]
With relu: dA_prev = [[ 0.44090989  0.        ]
 [ 0.37883606  0.        ]
 [-0.2298228   0.        ]]
With relu: dW = [[ 0.44513824  0.37371418 -0.10478989]]
With relu: db = [[-0.20837892]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="l-model-backward">
<h5>6.3 - L-Model Backward<a class="headerlink" href="#l-model-backward" title="Link to this heading">#</a></h5>
<p>Now you will implement the backward function for the whole network!</p>
<p>Recall that when you implemented the <code class="docutils literal notranslate"><span class="pre">L_model_forward</span></code> function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you’ll use those variables to compute the gradients. Therefore, in the <code class="docutils literal notranslate"><span class="pre">L_model_backward</span></code> function, you’ll iterate through all the hidden layers backward, starting from layer <span class="math notranslate nohighlight">\(L\)</span>. On each step, you will use the cached values for layer <span class="math notranslate nohighlight">\(l\)</span> to backpropagate through layer <span class="math notranslate nohighlight">\(l\)</span>. Figure 5 below shows the backward pass.</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/mn_backward.png"><img alt="_images/mn_backward.png" src="_images/mn_backward.png" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Backward pass</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Initializing backpropagation</strong>:</p>
<p>To backpropagate through this network, you know that the output is:
<span class="math notranslate nohighlight">\(A^{[L]} = \sigma(Z^{[L]})\)</span>. Your code thus needs to compute <code class="docutils literal notranslate"><span class="pre">dAL</span></code> <span class="math notranslate nohighlight">\(= \dfrac{\partial \mathcal{L}}{\partial A^{[L]}}\)</span>.
To do so, use this formula (derived using calculus which, again, you don’t need in-depth knowledge of!):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dAL</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">AL</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">AL</span><span class="p">))</span> <span class="c1"># derivative of cost with respect to AL</span>
</pre></div>
</div>
<p>You can then use this post-activation gradient <code class="docutils literal notranslate"><span class="pre">dAL</span></code> to keep going backward. As seen in Figure 5, you can now feed in <code class="docutils literal notranslate"><span class="pre">dAL</span></code> into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function).</p>
<p>After that, you will have to use a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :</p>
<div class="math notranslate nohighlight">
\[\text{grads}[\mathrm{d}W + \text{str(l)}] = \mathrm{d}W^{[l]}\tag{15} \]</div>
<p>For example, for <span class="math notranslate nohighlight">\(l=3\)</span> this would store <span class="math notranslate nohighlight">\(\mathrm{d}W^{[l]}\)</span> in <code class="docutils literal notranslate"><span class="pre">grads[&quot;dW3&quot;]</span></code>.</p>
</section>
<section id="exercise-9-l-model-backward">
<h5>Exercise 9 -  L_model_backward<a class="headerlink" href="#exercise-9-l-model-backward" title="Link to this heading">#</a></h5>
<p>Implement backpropagation for the <em>[LINEAR-&gt;RELU] <span class="math notranslate nohighlight">\(\times\)</span> (L-1) -&gt; LINEAR -&gt; SIGMOID</em> model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: L_model_backward</span>

<span class="k">def</span> <span class="nf">L_model_backward</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    AL -- probability vector, output of the forward propagation (L_model_forward())</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 0 if non-cat, 1 if cat)</span>
<span class="sd">    caches -- list of caches containing:</span>
<span class="sd">                every cache of linear_activation_forward() with &quot;relu&quot; (it&#39;s caches[l], for l in range(L-1) i.e l = 0...L-2)</span>
<span class="sd">                the cache of linear_activation_forward() with &quot;sigmoid&quot; (it&#39;s caches[L-1])</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    grads -- A dictionary with the gradients</span>
<span class="sd">             grads[&quot;dA&quot; + str(l)] = ... </span>
<span class="sd">             grads[&quot;dW&quot; + str(l)] = ...</span>
<span class="sd">             grads[&quot;db&quot; + str(l)] = ... </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">caches</span><span class="p">)</span> <span class="c1"># the number of layers</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">AL</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">AL</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># after this line, Y is the same shape as AL</span>
    
    <span class="c1"># Initializing the backpropagation</span>
    <span class="c1">#(1 line of code)</span>
    <span class="c1"># dAL = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dAL</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">AL</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">AL</span><span class="p">))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &quot;dAL, current_cache&quot;. Outputs: &quot;grads[&quot;dAL-1&quot;], grads[&quot;dWL&quot;], grads[&quot;dbL&quot;]</span>
    <span class="c1">#(approx. 5 lines)</span>
    <span class="c1"># current_cache = ...</span>
    <span class="c1"># dA_prev_temp, dW_temp, db_temp = ...</span>
    <span class="c1"># grads[&quot;dA&quot; + str(L-1)] = ...</span>
    <span class="c1"># grads[&quot;dW&quot; + str(L)] = ...</span>
    <span class="c1"># grads[&quot;db&quot; + str(L)] = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">current_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">dA_prev_temp</span><span class="p">,</span> <span class="n">dW_temp</span><span class="p">,</span> <span class="n">db_temp</span> <span class="o">=</span> <span class="n">linear_activation_backward</span><span class="p">(</span><span class="n">dAL</span><span class="p">,</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
    <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dA_prev_temp</span>
    <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dW_temp</span>
    <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">L</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db_temp</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Loop from l=L-2 to l=0</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
        <span class="c1"># lth layer: (RELU -&gt; LINEAR) gradients.</span>
        <span class="c1"># Inputs: &quot;grads[&quot;dA&quot; + str(l + 1)], current_cache&quot;. Outputs: &quot;grads[&quot;dA&quot; + str(l)] , grads[&quot;dW&quot; + str(l + 1)] , grads[&quot;db&quot; + str(l + 1)] </span>
        <span class="c1">#(approx. 5 lines)</span>
        <span class="c1"># current_cache = ...</span>
        <span class="c1"># dA_prev_temp, dW_temp, db_temp = ...</span>
        <span class="c1"># grads[&quot;dA&quot; + str(l)] = ...</span>
        <span class="c1"># grads[&quot;dW&quot; + str(l + 1)] = ...</span>
        <span class="c1"># grads[&quot;db&quot; + str(l + 1)] = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">current_cache</span> <span class="o">=</span> <span class="n">caches</span><span class="p">[</span><span class="n">l</span><span class="p">]</span>
        <span class="n">dA_prev_temp</span><span class="p">,</span> <span class="n">dW_temp</span><span class="p">,</span> <span class="n">db_temp</span> <span class="o">=</span> <span class="n">linear_activation_backward</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)],</span> <span class="n">current_cache</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">)</span>
        <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dA&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dA_prev_temp</span>
        <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dW_temp</span>
        <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">db_temp</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="k">return</span> <span class="n">grads</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_AL</span><span class="p">,</span> <span class="n">t_Y_assess</span><span class="p">,</span> <span class="n">t_caches</span> <span class="o">=</span> <span class="n">L_model_backward_test_case</span><span class="p">()</span>
<span class="n">grads</span> <span class="o">=</span> <span class="n">L_model_backward</span><span class="p">(</span><span class="n">t_AL</span><span class="p">,</span> <span class="n">t_Y_assess</span><span class="p">,</span> <span class="n">t_caches</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dA0 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dA0&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dA1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dA1&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dW1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW1&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;dW2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW2&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;db1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db1&#39;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;db2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db2&#39;</span><span class="p">]))</span>

<span class="n">L_model_backward_test</span><span class="p">(</span><span class="n">L_model_backward</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dA0 = [[ 0.          0.52257901]
 [ 0.         -0.3269206 ]
 [ 0.         -0.32070404]
 [ 0.         -0.74079187]]
dA1 = [[ 0.12913162 -0.44014127]
 [-0.14175655  0.48317296]
 [ 0.01663708 -0.05670698]]
dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]
 [0.         0.         0.         0.        ]
 [0.05283652 0.01005865 0.01777766 0.0135308 ]]
dW2 = [[-0.39202432 -0.13325855 -0.04601089]]
db1 = [[-0.22007063]
 [ 0.        ]
 [-0.02835349]]
db2 = [[0.15187861]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="update-parameters">
<h5>6.4 - Update Parameters<a class="headerlink" href="#update-parameters" title="Link to this heading">#</a></h5>
<p>In this section, you’ll update the parameters of the model, using gradient descent:</p>
<div class="math notranslate nohighlight">
\[ W^{[l]} = W^{[l]} - \alpha \text{ } \mathrm{d}W^{[l]} \tag{16}\]</div>
<div class="math notranslate nohighlight">
\[ b^{[l]} = b^{[l]} - \alpha \text{ } \mathrm{d}b^{[l]} \tag{17}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is the learning rate.</p>
<p>After computing the updated parameters, store them in the parameters dictionary.</p>
</section>
<section id="exercise-10-update-parameters">
<h5>Exercise 10 - update_parameters<a class="headerlink" href="#exercise-10-update-parameters" title="Link to this heading">#</a></h5>
<p>Implement <code class="docutils literal notranslate"><span class="pre">update_parameters()</span></code> to update your parameters using gradient descent.</p>
<p><strong>Instructions</strong>:
Update parameters using gradient descent on every <span class="math notranslate nohighlight">\(W^{[l]}\)</span> and <span class="math notranslate nohighlight">\(b^{[l]}\)</span> for <span class="math notranslate nohighlight">\(l = 1, 2, ..., L\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: update_parameters</span>

<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Update parameters using gradient descent</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    params -- python dictionary containing your parameters </span>
<span class="sd">    grads -- python dictionary containing your gradients, output of L_model_backward</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your updated parameters </span>
<span class="sd">                  parameters[&quot;W&quot; + str(l)] = ... </span>
<span class="sd">                  parameters[&quot;b&quot; + str(l)] = ...</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="c1"># number of layers in the neural network</span>

    <span class="c1"># Update rule for each parameter. Use a for loop.</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="c1"># parameters[&quot;W&quot; + str(l+1)] = ...</span>
        <span class="c1"># parameters[&quot;b&quot; + str(l+1)] = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s2">&quot;db&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_parameters</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">update_parameters_test_case</span><span class="p">()</span>
<span class="n">t_parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">t_parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;W1 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;b1 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;W2 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;b2 = &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">t_parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="n">update_parameters_test</span><span class="p">(</span><span class="n">update_parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]
 [-1.76569676 -0.80627147  0.51115557 -1.18258802]
 [-1.0535704  -0.86128581  0.68284052  2.20374577]]
b1 = [[-0.04659241]
 [-1.28888275]
 [ 0.53405496]]
W2 = [[-0.55569196  0.0354055   1.32964895]]
b2 = [[-0.84610769]]
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="congratulations">
<h5>Congratulations!<a class="headerlink" href="#congratulations" title="Link to this heading">#</a></h5>
<p>You’ve just implemented all the functions required for building a deep neural network, including:</p>
<ul class="simple">
<li><p>Using non-linear units improve your model</p></li>
<li><p>Building a deeper neural network (with more than 1 hidden layer)</p></li>
<li><p>Implementing an easy-to-use neural network class</p></li>
</ul>
<p>This was indeed a long assignment, but the next part of the assignment is easier. ;)</p>
<p>In the next assignment, you’ll be putting all these together to build two models:</p>
<ul class="simple">
<li><p>A two-layer neural network</p></li>
<li><p>An L-layer neural network</p></li>
</ul>
<p>You will in fact use these models to classify cat vs non-cat images! (Meow!) Great work and see you next time.</p>
</section>
</section>
</section>
<span id="document-C4_Practical_Test_P2"></span><section class="tex2jax_ignore mathjax_ignore" id="practical-4-deep-neural-network-for-image-classification-application">
<h3>Practical 4: Deep Neural Network for Image Classification: Application<a class="headerlink" href="#practical-4-deep-neural-network-for-image-classification-application" title="Link to this heading">#</a></h3>
<p>By the time you complete this notebook, you will have finished the last programming assignment of Chapter 4, and also the last programming assignment of Course 1! Go you!</p>
<p>To build your cat/not-a-cat classifier, you’ll use the functions from the previous assignment to build a deep network. Hopefully, you’ll see an improvement in accuracy over your previous logistic regression implementation.</p>
<p><strong>After this assignment you will be able to:</strong></p>
<ul class="simple">
<li><p>Build and train a deep L-layer neural network, and apply it to supervised learning</p></li>
</ul>
<p>Let’s get started!</p>
<section id="packages">
<h4>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h4>
<p>Begin by importing all the packages you’ll need during this assignment.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.numpy.org/">numpy</a> is the fundamental package for scientific computing with Python.</p></li>
<li><p><a class="reference external" href="http://matplotlib.org">matplotlib</a> is a library to plot graphs in Python.</p></li>
<li><p><a class="reference external" href="http://www.h5py.org">h5py</a> is a common package to interact with a dataset that is stored on an H5 file.</p></li>
<li><p><a class="reference external" href="http://www.pythonware.com/products/pil/">PIL</a> and <a class="reference external" href="https://www.scipy.org/">scipy</a> are used here to test your model with your own picture at the end.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dnn_app_utils</span></code> provides the functions implemented in the “Building your Deep Neural Network: Step by Step” assignment to this notebook.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">np.random.seed(1)</span></code> is used to keep all the random function calls consistent. It helps grade your work - so please don’t change it!</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">h5py</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">ndimage</span>
<span class="kn">from</span> <span class="nn">dnn_app_utils_v3_c1w4a2</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">public_tests_c1w4a2</span> <span class="kn">import</span> <span class="o">*</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span> <span class="c1"># set default size of plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.interpolation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.cmap&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="load-and-process-the-dataset">
<h4>2 - Load and Process the Dataset<a class="headerlink" href="#load-and-process-the-dataset" title="Link to this heading">#</a></h4>
<p>You’ll be using the same “Cat vs non-Cat” dataset as in “Logistic Regression as a Neural Network” (Assignment 2). The model you built back then had 70% test accuracy on classifying cat vs non-cat images. Hopefully, your new model will perform even better!</p>
<p><strong>Problem Statement</strong>: You are given a dataset (“data.h5”) containing:
- a training set of <code class="docutils literal notranslate"><span class="pre">m_train</span></code> images labelled as cat (1) or non-cat (0)
- a test set of <code class="docutils literal notranslate"><span class="pre">m_test</span></code> images labelled as cat and non-cat
- each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB).</p>
<p>Let’s get more familiar with the dataset. Load the data by running the cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_x_orig</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_x_orig</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">classes</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to check out other images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example of a picture</span>
<span class="n">index</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">train_x_orig</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;y = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">index</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot;. It&#39;s a &quot;</span> <span class="o">+</span> <span class="n">classes</span><span class="p">[</span><span class="n">train_y</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="n">index</span><span class="p">]]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">+</span>  <span class="s2">&quot; picture.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y = 0. It&#39;s a non-cat picture.
</pre></div>
</div>
<img alt="_images/2315915c0ab02bd329828ee1355dae9ac6102e26253d2c20d5fc9ecd5831e752.png" src="_images/2315915c0ab02bd329828ee1355dae9ac6102e26253d2c20d5fc9ecd5831e752.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Explore your dataset </span>
<span class="n">m_train</span> <span class="o">=</span> <span class="n">train_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">num_px</span> <span class="o">=</span> <span class="n">train_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">m_test</span> <span class="o">=</span> <span class="n">test_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Number of training examples: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">m_train</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Number of testing examples: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">m_test</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;Each image is of size: (&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_px</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">num_px</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;, 3)&quot;</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_x_orig shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_y shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_x_orig shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_y shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_y</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Number of training examples: 209
Number of testing examples: 50
Each image is of size: (64, 64, 3)
train_x_orig shape: (209, 64, 64, 3)
train_y shape: (1, 209)
test_x_orig shape: (50, 64, 64, 3)
test_y shape: (1, 50)
</pre></div>
</div>
</div>
</div>
<p>As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below.</p>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/imvectorkiank.png"><img alt="_images/imvectorkiank.png" src="_images/imvectorkiank.png" style="height: 300px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Image to vector conversion</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reshape the training and test examples </span>
<span class="n">train_x_flatten</span> <span class="o">=</span> <span class="n">train_x_orig</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">train_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>   <span class="c1"># The &quot;-1&quot; makes reshape flatten the remaining dimensions</span>
<span class="n">test_x_flatten</span> <span class="o">=</span> <span class="n">test_x_orig</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_x_orig</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Standardize data to have feature values between 0 and 1.</span>
<span class="n">train_x</span> <span class="o">=</span> <span class="n">train_x_flatten</span><span class="o">/</span><span class="mf">255.</span>
<span class="n">test_x</span> <span class="o">=</span> <span class="n">test_x_flatten</span><span class="o">/</span><span class="mf">255.</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;train_x&#39;s shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">train_x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;test_x&#39;s shape: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">test_x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train_x&#39;s shape: (12288, 209)
test_x&#39;s shape: (12288, 50)
</pre></div>
</div>
</div>
</div>
<p><strong>Note</strong>:
<span class="math notranslate nohighlight">\(12,288\)</span> equals <span class="math notranslate nohighlight">\(64 \times 64 \times 3\)</span>, which is the size of one reshaped image vector.</p>
</section>
<section id="model-architecture">
<h4>3 - Model Architecture<a class="headerlink" href="#model-architecture" title="Link to this heading">#</a></h4>
<section id="layer-neural-network">
<h5>3.1 - 2-layer Neural Network<a class="headerlink" href="#layer-neural-network" title="Link to this heading">#</a></h5>
<p>Now that you’re familiar with the dataset, it’s time to build a deep neural network to distinguish cat images from non-cat images!</p>
<p>You’re going to build two different models:</p>
<ul class="simple">
<li><p>A 2-layer neural network</p></li>
<li><p>An L-layer deep neural network</p></li>
</ul>
<p>Then, you’ll compare the performance of these models, and try out some different values for <span class="math notranslate nohighlight">\(L\)</span>.</p>
<p>Let’s look at the two architectures:</p>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/2layerNN_kiank.png"><img alt="_images/2layerNN_kiank.png" src="_images/2layerNN_kiank.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">2-layer neural network. The model can be summarized as: INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><u><b>Detailed Architecture of Figure 7</b></u>:</p>
<ul class="simple">
<li><p>The input is a (64,64,3) image which is flattened to a vector of size <span class="math notranslate nohighlight">\((12288,1)\)</span>.</p></li>
<li><p>The corresponding vector: <span class="math notranslate nohighlight">\([x_0,x_1,...,x_{12287}]^T\)</span> is then multiplied by the weight matrix <span class="math notranslate nohighlight">\(W^{[1]}\)</span> of size <span class="math notranslate nohighlight">\((n^{[1]}, 12288)\)</span>.</p></li>
<li><p>Then, add a bias term and take its relu to get the following vector: <span class="math notranslate nohighlight">\([a_0^{[1]}, a_1^{[1]},..., a_{n^{[1]}-1}^{[1]}]^T\)</span>.</p></li>
<li><p>Multiply the resulting vector by <span class="math notranslate nohighlight">\(W^{[2]}\)</span> and add the intercept (bias).</p></li>
<li><p>Finally, take the sigmoid of the result. If it’s greater than 0.5, classify it as a cat.</p></li>
</ul>
</section>
<section id="l-layer-deep-neural-network">
<h5>3.2 - L-layer Deep Neural Network<a class="headerlink" href="#l-layer-deep-neural-network" title="Link to this heading">#</a></h5>
<p>It’s pretty difficult to represent an L-layer deep neural network using the above representation. However, here is a simplified network representation:</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/LlayerNN_kiank.png"><img alt="_images/LlayerNN_kiank.png" src="_images/LlayerNN_kiank.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">L-layer neural network. The model can be summarized as: [LINEAR -&gt; RELU] <span class="math notranslate nohighlight">\(\times\)</span> (L-1) -&gt; LINEAR -&gt; SIGMOID.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><u><b>Detailed Architecture of Figure 8</b></u>:</p>
<ul class="simple">
<li><p>The input is a (64,64,3) image which is flattened to a vector of size (12288,1).</p></li>
<li><p>The corresponding vector: <span class="math notranslate nohighlight">\([x_0,x_1,...,x_{12287}]^T\)</span> is then multiplied by the weight matrix <span class="math notranslate nohighlight">\(W^{[1]}\)</span> and then you add the intercept <span class="math notranslate nohighlight">\(b^{[1]}\)</span>. The result is called the linear unit.</p></li>
<li><p>Next, take the relu of the linear unit. This process could be repeated several times for each <span class="math notranslate nohighlight">\((W^{[l]}, b^{[l]})\)</span> depending on the model architecture.</p></li>
<li><p>Finally, take the sigmoid of the final linear unit. If it is greater than 0.5, classify it as a cat.</p></li>
</ul>
</section>
<section id="general-methodology">
<h5>3.3 - General Methodology<a class="headerlink" href="#general-methodology" title="Link to this heading">#</a></h5>
<p>As usual, you’ll follow the Deep Learning methodology to build the model:</p>
<ol class="arabic simple">
<li><p>Initialize parameters / Define hyperparameters</p></li>
<li><p>Loop for num_iterations:
a. Forward propagation
b. Compute cost function
c. Backward propagation
d. Update parameters (using parameters, and grads from backprop)</p></li>
<li><p>Use trained parameters to predict labels</p></li>
</ol>
<p>Now go ahead and implement those two models!</p>
</section>
</section>
<section id="two-layer-neural-network">
<h4>4 - Two-layer Neural Network<a class="headerlink" href="#two-layer-neural-network" title="Link to this heading">#</a></h4>
<section id="exercise-1-two-layer-model">
<h5>Exercise 1 - two_layer_model<a class="headerlink" href="#exercise-1-two-layer-model" title="Link to this heading">#</a></h5>
<p>Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: <em>LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</em>. The functions and their inputs are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">parameters</span> 
<span class="k">def</span> <span class="nf">linear_activation_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">cache</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">cost</span>
<span class="k">def</span> <span class="nf">linear_activation_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span>
<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### CONSTANTS DEFINING THE MODEL ####</span>
<span class="n">n_x</span> <span class="o">=</span> <span class="mi">12288</span>     <span class="c1"># num_px * num_px * 3</span>
<span class="n">n_h</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">n_y</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">layers_dims</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0075</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: two_layer_model</span>

<span class="k">def</span> <span class="nf">two_layer_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">layers_dims</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0075</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input data, of shape (n_x, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)</span>
<span class="sd">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)</span>
<span class="sd">    num_iterations -- number of iterations of the optimization loop</span>
<span class="sd">    learning_rate -- learning rate of the gradient descent update rule</span>
<span class="sd">    print_cost -- If set to True, this will print the cost every 100 iterations </span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- a dictionary containing W1, W2, b1, and b2</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>                              <span class="c1"># to keep track of the cost</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>                           <span class="c1"># number of examples</span>
    <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">layers_dims</span>
    
    <span class="c1"># Initialize parameters dictionary, by calling one of the functions you&#39;d previously implemented</span>
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># parameters = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Get W1, b1, W2 and b2 from the dictionary parameters.</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>
    
    <span class="c1"># Loop (gradient descent)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: &quot;X, W1, b1, W2, b2&quot;. Output: &quot;A1, cache1, A2, cache2&quot;.</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># A1, cache1 = ...</span>
        <span class="c1"># A2, cache2 = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">A1</span><span class="p">,</span> <span class="n">cache1</span> <span class="o">=</span> <span class="n">linear_activation_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        <span class="n">A2</span><span class="p">,</span> <span class="n">cache2</span> <span class="o">=</span> <span class="n">linear_activation_forward</span><span class="p">(</span><span class="n">A1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Compute cost</span>
        <span class="c1">#(≈ 1 line of code)</span>
        <span class="c1"># cost = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">A2</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Initializing backward propagation</span>
        <span class="n">dA2</span> <span class="o">=</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">Y</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">A2</span><span class="p">))</span>
        
        <span class="c1"># Backward propagation. Inputs: &quot;dA2, cache2, cache1&quot;. Outputs: &quot;dA1, dW2, db2; also dA0 (not used), dW1, db1&quot;.</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># dA1, dW2, db2 = ...</span>
        <span class="c1"># dA0, dW1, db1 = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">dA1</span><span class="p">,</span> <span class="n">dW2</span><span class="p">,</span> <span class="n">db2</span> <span class="o">=</span> <span class="n">linear_activation_backward</span><span class="p">(</span><span class="n">dA2</span><span class="p">,</span> <span class="n">cache2</span><span class="p">,</span> <span class="s1">&#39;sigmoid&#39;</span><span class="p">)</span>
        <span class="n">dA0</span><span class="p">,</span> <span class="n">dW1</span><span class="p">,</span> <span class="n">db1</span> <span class="o">=</span> <span class="n">linear_activation_backward</span><span class="p">(</span><span class="n">dA1</span><span class="p">,</span> <span class="n">cache1</span><span class="p">,</span> <span class="s1">&#39;relu&#39;</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Set grads[&#39;dWl&#39;] to dW1, grads[&#39;db1&#39;] to db1, grads[&#39;dW2&#39;] to dW2, grads[&#39;db2&#39;] to db2</span>
        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dW1</span>
        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db1&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">db1</span>
        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;dW2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dW2</span>
        <span class="n">grads</span><span class="p">[</span><span class="s1">&#39;db2&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">db2</span>
        
        <span class="c1"># Update parameters.</span>
        <span class="c1">#(approx. 1 line of code)</span>
        <span class="c1"># parameters = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>

        <span class="c1"># Retrieve W1, b1, W2, b2 from parameters</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>
        
        <span class="c1"># Print the cost every 100 iterations</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_iterations</span><span class="p">:</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">costs</span>

<span class="k">def</span> <span class="nf">plot_costs</span><span class="p">(</span><span class="n">costs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0075</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">costs</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (per hundreds)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning rate =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">two_layer_model</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">layers_dims</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">),</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after first iteration: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">costs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">two_layer_model_test</span><span class="p">(</span><span class="n">two_layer_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1: 0.6926114346158595
Cost after first iteration: 0.693049735659989
Cost after iteration 1: 0.6915746967050506
Cost after iteration 1: 0.6915746967050506
Cost after iteration 1: 0.6915746967050506
Cost after iteration 2: 0.6524135179683452
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-model">
<h5>4.1 - Train the model<a class="headerlink" href="#train-the-model" title="Link to this heading">#</a></h5>
<p>If your code passed the previous cell, run the cell below to train your parameters.</p>
<ul class="simple">
<li><p>The cost should decrease on every iteration.</p></li>
<li><p>It may take up to 5 minutes to run 2500 iterations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">two_layer_model</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">layers_dims</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">),</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">2500</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plot_costs</span><span class="p">(</span><span class="n">costs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.693049735659989
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 100: 0.6464320953428849
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 200: 0.6325140647912677
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 300: 0.6015024920354664
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 400: 0.5601966311605747
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 500: 0.5158304772764729
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 600: 0.4754901313943324
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 700: 0.4339163151225749
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 800: 0.4007977536203889
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 900: 0.3580705011323798
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1000: 0.3394281538366412
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1100: 0.3052753636196265
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1200: 0.27491377282130197
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1300: 0.24681768210614827
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1400: 0.19850735037466102
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1500: 0.1744831811255664
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1600: 0.17080762978096403
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1700: 0.11306524562164721
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1800: 0.09629426845937147
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1900: 0.08342617959726861
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2000: 0.0743907870431908
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2100: 0.06630748132267929
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2200: 0.059193295010381654
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2300: 0.05336140348560555
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2400: 0.04855478562877016
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2499: 0.04421498215868953
</pre></div>
</div>
<img alt="_images/ed8f61de61f86121cfeee5c69c2dbf6a9de1efcb3f8b01fcde99c5c21bc41e49.png" src="_images/ed8f61de61f86121cfeee5c69c2dbf6a9de1efcb3f8b01fcde99c5c21bc41e49.png" />
</div>
</div>
<p><strong>Nice!</strong> You successfully trained the model. Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this.</p>
<p>Now, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.9999999999999998
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.72
</pre></div>
</div>
</div>
</div>
</section>
<section id="congratulations-it-seems-that-your-2-layer-neural-network-has-better-performance-72-than-the-logistic-regression-implementation-70-practical-1-let-s-see-if-you-can-do-even-better-with-an-l-layer-model">
<h5>Congratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, practical 1). Let’s see if you can do even better with an <span class="math notranslate nohighlight">\(L\)</span>-layer model.<a class="headerlink" href="#congratulations-it-seems-that-your-2-layer-neural-network-has-better-performance-72-than-the-logistic-regression-implementation-70-practical-1-let-s-see-if-you-can-do-even-better-with-an-l-layer-model" title="Link to this heading">#</a></h5>
<p><strong>Note</strong>: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and you’ll hear more about it in the next course. Early stopping is a way to prevent overfitting.</p>
</section>
</section>
<section id="l-layer-neural-network">
<h4>5 - L-layer Neural Network<a class="headerlink" href="#l-layer-neural-network" title="Link to this heading">#</a></h4>
<section id="exercise-2-l-layer-model">
<h5>Exercise 2 - L_layer_model<a class="headerlink" href="#exercise-2-l-layer-model" title="Link to this heading">#</a></h5>
<p>Use the helper functions you implemented previously to build an <span class="math notranslate nohighlight">\(L\)</span>-layer neural network with the following structure: <em>[LINEAR -&gt; RELU]<span class="math notranslate nohighlight">\(\times\)</span>(L-1) -&gt; LINEAR -&gt; SIGMOID</em>. The functions and their inputs are:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">initialize_parameters_deep</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">parameters</span> 
<span class="k">def</span> <span class="nf">L_model_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">AL</span><span class="p">,</span> <span class="n">caches</span>
<span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">cost</span>
<span class="k">def</span> <span class="nf">L_model_backward</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">grads</span>
<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### CONSTANTS ###</span>
<span class="n">layers_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12288</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="c1">#  4-layer model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: L_layer_model</span>

<span class="k">def</span> <span class="nf">L_layer_model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">layers_dims</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.0075</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">3000</span><span class="p">,</span> <span class="n">print_cost</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input data, of shape (n_x, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)</span>
<span class="sd">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).</span>
<span class="sd">    learning_rate -- learning rate of the gradient descent update rule</span>
<span class="sd">    num_iterations -- number of iterations of the optimization loop</span>
<span class="sd">    print_cost -- if True, it prints the cost every 100 steps</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- parameters learnt by the model. They can then be used to predict.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>                         <span class="c1"># keep track of cost</span>
    
    <span class="c1"># Parameters initialization.</span>
    <span class="c1">#(≈ 1 line of code)</span>
    <span class="c1"># parameters = ...</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_deep</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="c1"># Loop (gradient descent)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span>
        <span class="c1">#(≈ 1 line of code)</span>
        <span class="c1"># AL, caches = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">AL</span><span class="p">,</span> <span class="n">caches</span> <span class="o">=</span> <span class="n">L_model_forward</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
        <span class="c1"># Compute cost.</span>
        <span class="c1">#(≈ 1 line of code)</span>
        <span class="c1"># cost = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    
        <span class="c1"># Backward propagation.</span>
        <span class="c1">#(≈ 1 line of code)</span>
        <span class="c1"># grads = ...    </span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">L_model_backward</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">caches</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
 
        <span class="c1"># Update parameters.</span>
        <span class="c1">#(≈ 1 line of code)</span>
        <span class="c1"># parameters = ...</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
                
        <span class="c1"># Print the cost every 100 iterations</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">i</span> <span class="o">==</span> <span class="n">num_iterations</span><span class="p">:</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">costs</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">L_layer_model</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">layers_dims</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after first iteration: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">costs</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

<span class="n">L_layer_model_test</span><span class="p">(</span><span class="n">L_layer_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.7717493284237686
Cost after first iteration: 0.7717493284237686
Cost after iteration 1: 0.7070709008912568
Cost after iteration 1: 0.7070709008912568
Cost after iteration 1: 0.7070709008912568
Cost after iteration 2: 0.7063462654190897
 All tests passed.
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h5>5.1 - Train the model<a class="headerlink" href="#id1" title="Link to this heading">#</a></h5>
<p>If your code passed the previous cell, run the cell below to train your model as a 4-layer neural network.</p>
<ul class="simple">
<li><p>The cost should decrease on every iteration.</p></li>
<li><p>It may take up to 5 minutes to run 2500 iterations.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span><span class="p">,</span> <span class="n">costs</span> <span class="o">=</span> <span class="n">L_layer_model</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">layers_dims</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">2500</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.7717493284237686
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 100: 0.6720534400822918
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 200: 0.6482632048575216
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 300: 0.6115068816101354
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 400: 0.5670473268366112
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 500: 0.5401376634547802
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 600: 0.5279299569455271
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 700: 0.46547737717668486
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 800: 0.36912585249592783
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 900: 0.39174697434805333
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1000: 0.3151869888600614
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1100: 0.27269984417893844
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1200: 0.23741853400268118
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1300: 0.19960120532208628
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1400: 0.189263003884633
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1500: 0.16118854665827767
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1600: 0.14821389662363324
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1700: 0.13777487812972944
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1800: 0.12974017549190134
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1900: 0.12122535068005211
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2000: 0.1138206066863372
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2100: 0.1078392852625413
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2200: 0.10285466069352683
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2300: 0.10089745445261779
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2400: 0.09287821526472398
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 2499: 0.08843994344170203
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_x</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.9856459330143539
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pred_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.8
</pre></div>
</div>
</div>
</div>
</section>
<section id="congrats-it-seems-that-your-4-layer-neural-network-has-better-performance-80-than-your-2-layer-neural-network-72-on-the-same-test-set">
<h5>Congrats! It seems that your 4-layer neural network has better performance (80%) than your 2-layer neural network (72%) on the same test set.<a class="headerlink" href="#congrats-it-seems-that-your-4-layer-neural-network-has-better-performance-80-than-your-2-layer-neural-network-72-on-the-same-test-set" title="Link to this heading">#</a></h5>
<p>This is pretty good performance for this task. Nice job!</p>
<p>In the next course on “Improving deep neural networks,” you’ll be able to obtain even higher accuracy by systematically searching for better hyperparameters: learning_rate, layers_dims, or num_iterations, for example.</p>
</section>
</section>
<section id="results-analysis">
<h4>6 - Results Analysis<a class="headerlink" href="#results-analysis" title="Link to this heading">#</a></h4>
<p>First, take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">print_mislabeled_images</span><span class="p">(</span><span class="n">classes</span><span class="p">,</span> <span class="n">test_x</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">pred_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c315a754c9ebf524935cdb2e10add2fd63b803282fb51295c1c544a54c5995d7.png" src="_images/c315a754c9ebf524935cdb2e10add2fd63b803282fb51295c1c544a54c5995d7.png" />
</div>
</div>
<p><strong>A few types of images the model tends to do poorly on include:</strong></p>
<ul class="simple">
<li><p>Cat body in an unusual position</p></li>
<li><p>Cat appears against a background of a similar color</p></li>
<li><p>Unusual cat color and species</p></li>
<li><p>Camera Angle</p></li>
<li><p>Brightness of the picture</p></li>
<li><p>Scale variation (cat is very large or small in image)</p></li>
</ul>
<section id="congratulations-on-finishing-this-assignment">
<h5>Congratulations on finishing this assignment!<a class="headerlink" href="#congratulations-on-finishing-this-assignment" title="Link to this heading">#</a></h5>
<p>You just built and trained a deep L-layer neural network, and applied it in order to distinguish cats from non-cats, a very serious and important task in deep learning. ;)</p>
<p>By now, you’ve also completed all the assignments for Course 1 in the Deep Learning Specialization. Amazing work! If you’d like to test out how closely you resemble a cat yourself, there’s an optional ungraded exercise below, where you can test your own image.</p>
<p>Great work and hope to see you in the next course!</p>
</section>
</section>
<section id="test-with-your-own-image-optional-ungraded-exercise">
<h4>7 - Test with your own image (optional/ungraded exercise)<a class="headerlink" href="#test-with-your-own-image-optional-ungraded-exercise" title="Link to this heading">#</a></h4>
<p>From this point, if you so choose, you can use your own image to test  the output of your model. To do that follow these steps:</p>
<ol class="arabic simple">
<li><p>Change your image’s name in the following code</p></li>
<li><p>Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)!</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## START CODE HERE ##</span>
<span class="n">my_image</span> <span class="o">=</span> <span class="s2">&quot;my_cat.jpg&quot;</span> <span class="c1"># change this to the name of your image file </span>
<span class="n">my_label_y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)</span>
<span class="c1">## END CODE HERE ##</span>

<span class="n">fname</span> <span class="o">=</span> <span class="s2">&quot;images/&quot;</span> <span class="o">+</span> <span class="n">my_image</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="n">num_px</span><span class="p">,</span> <span class="n">num_px</span><span class="p">)))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span> <span class="o">/</span> <span class="mf">255.</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_px</span> <span class="o">*</span> <span class="n">num_px</span> <span class="o">*</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

<span class="n">my_predicted_image</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">my_label_y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>


<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;y = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">my_predicted_image</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;, your L-layer model predicts a </span><span class="se">\&quot;</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">classes</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">my_predicted_image</span><span class="p">)),]</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="o">+</span>  <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2"> picture.&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Accuracy: 1.0
y = 1.0, your L-layer model predicts a &quot;cat&quot; picture.
</pre></div>
</div>
<img alt="_images/c22781a069becfc622a78b473582889a4cea1d0260e85245aa9110aca0c6a30f.png" src="_images/c22781a069becfc622a78b473582889a4cea1d0260e85245aa9110aca0c6a30f.png" />
</div>
</div>
</section>
</section>
</div>
</section>
<span id="document-C5"></span><section class="tex2jax_ignore mathjax_ignore" id="practical-aspects-of-deep-learning">
<span id="paofdl"></span><h2>5 Practical Aspects of Deep Learning<a class="headerlink" href="#practical-aspects-of-deep-learning" title="Link to this heading">#</a></h2>
<p>Discover and experiment with a variety of different initialization methods, apply L2 regularization and dropout to avoid model overfitting, then apply gradient checking to identify errors in a fraud detection model.</p>
<p><strong>Learning Objectives</strong></p>
<ul class="simple">
<li><p>Give examples of how different types of initializations can lead to different results</p></li>
<li><p>Examine the importance of initialization in complex neural networks</p></li>
<li><p>Explain the difference between train/dev/test sets</p></li>
<li><p>Diagnose the bias and variance issues in your model</p></li>
<li><p>Assess the right time and place for using regularization methods such as dropout or L2 regularization</p></li>
<li><p>Explain Vanishing and Exploding gradients and how to deal with them</p></li>
<li><p>Use gradient checking to verify the accuracy of your backpropagation implementation</p></li>
<li><p>Apply zeros initialization, random initialization, and He initialization</p></li>
<li><p>Apply regularization to a deep learning model</p></li>
</ul>
<hr class="docutils" />
<section id="setting-up-your-machine-learning-application">
<h3>Setting up your Machine Learning Application<a class="headerlink" href="#setting-up-your-machine-learning-application" title="Link to this heading">#</a></h3>
<section id="train-dev-test-sets">
<h4>Train / Dev / Test Sets<a class="headerlink" href="#train-dev-test-sets" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/AzaEKXlZ4cM">Video</a></p>
</section>
<section id="bias-variance">
<h4>Bias / Variance<a class="headerlink" href="#bias-variance" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/2hIwfc5ak_w">Video</a></p>
</section>
<section id="bias-recipe-for-machine-learning">
<h4>Bias Recipe for Machine Learning<a class="headerlink" href="#bias-recipe-for-machine-learning" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/zIxFN41JEyY">Video</a></p>
</section>
</section>
<section id="regularizing-your-neural-network">
<h3>Regularizing your Neural Network<a class="headerlink" href="#regularizing-your-neural-network" title="Link to this heading">#</a></h3>
<section id="regularization">
<h4>Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/ZlrNwgvycNw">Video</a></p>
</section>
<section id="why-regularization-reduces-overfitting">
<h4>Why regularization Reduces Overfitting?<a class="headerlink" href="#why-regularization-reduces-overfitting" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/E-e1LRzRz0o">Video</a></p>
</section>
<section id="dropout-regularization">
<h4>Dropout Regularization<a class="headerlink" href="#dropout-regularization" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/62AiUPL_g18">Video</a></p>
</section>
<section id="understanding-dropout">
<h4>Understanding Dropout<a class="headerlink" href="#understanding-dropout" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/mcNkV_hFoY8">Video</a></p>
</section>
<section id="other-regularization-methods">
<h4>Other Regularization Methods<a class="headerlink" href="#other-regularization-methods" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://youtu.be/4tVRKn4sZiI">Video</a></p>
</section>
</section>
<section id="setting-up-your-opimization-problem">
<h3>Setting up your Opimization Problem<a class="headerlink" href="#setting-up-your-opimization-problem" title="Link to this heading">#</a></h3>
<section id="normalizing-inputs">
<h4>Normalizing Inputs<a class="headerlink" href="#normalizing-inputs" title="Link to this heading">#</a></h4>
</section>
<section id="vanishing-exploding-gradients">
<h4>Vanishing / Exploding Gradients<a class="headerlink" href="#vanishing-exploding-gradients" title="Link to this heading">#</a></h4>
</section>
<section id="weight-initialization-for-deep-networks">
<h4>Weight Initialization for Deep Networks<a class="headerlink" href="#weight-initialization-for-deep-networks" title="Link to this heading">#</a></h4>
</section>
<section id="numerical-approximation-of-gradients">
<h4>Numerical Approximation of Gradients<a class="headerlink" href="#numerical-approximation-of-gradients" title="Link to this heading">#</a></h4>
</section>
<section id="gradient-checking">
<h4>Gradient Checking<a class="headerlink" href="#gradient-checking" title="Link to this heading">#</a></h4>
</section>
<section id="gradient-checking-implementation-notes">
<h4>Gradient Checking Implementation Notes<a class="headerlink" href="#gradient-checking-implementation-notes" title="Link to this heading">#</a></h4>
</section>
</section>
<section id="quiz">
<h3>Quiz<a class="headerlink" href="#quiz" title="Link to this heading">#</a></h3>
<ol class="arabic">
<li><p>If you have 20,000,000 examples, how would you split the train/dev/test set? Choose the best option.</p>
<p>A. 90% train. 5% dev. 5% test.</p>
<p>B. 99% train. 0.5% dev. 0.5% test.</p>
<p>C. 60% train. 20% dev. 20% test.</p>
</li>
<li><p>In a personal experiment, an M.L. student decides to not use a test set, only train-dev sets. In this case which of the following is true?</p>
<p>A. He won’t be able to measure the bias of the model.</p>
<p>B. He won’t be able to measure the variance of the model.</p>
<p>C. He might be overfitting to the dev set.</p>
<p>D. Not having a test set is unacceptable under any circumstance.</p>
</li>
<li><p>If your Neural Network model seems to have high variance, what of the following would be promising things to try?</p>
<p>A. Make the Neural Network deeper</p>
<p>B. Add regularization</p>
<p>C. Get more training data</p>
<p>D. Increase the number of units in each hidden layer</p>
<p>E. Get more test data</p>
</li>
<li><p>You are working on an automated check-out kiosk for a supermarket and are building a classifier for apples, bananas, and oranges. Suppose your classifier obtains a training set error of 19% and a dev set error of 21%. Which of the following are promising things to try to improve your classifier? (Check all that apply, suppose the human error is approximately 0%)</p>
<p>A. Use a bigger network.</p>
<p>B. Increase the regularization parameter lambda.</p>
<p>C. Get more training data.</p>
</li>
<li><p><strong>True/False</strong> In every case it is a good practice to use dropout when training a deep neural network because it can help to prevent overfitting.   __________</p></li>
<li><p><strong>True/False</strong> The regularization hyperparameter must be set to zero during testing to avoid getting random results.    __________</p></li>
<li><p>Which of the following are true about dropout?</p>
<p>A. It helps to reduce overfitting.</p>
<p>B. In practice, it eliminates units of each layer with probability of <span class="math notranslate nohighlight">\(1-\)</span>keep_prob.</p>
<p>C. In practice, it eliminates units of each layer with probability of keep_prob.</p>
<p>D. It helps to reduce the bias of model.</p>
</li>
<li><p>During training a deep neural network that uses the tanh activation function, the value of the gradients is practically zero. Which of the following is most likely to help the vanishing gradient problem?</p>
<p>A. Use Xavier initialization.</p>
<p>B. Increase the number of cycles during the training.</p>
<p>C. Increase the number of layers of the network.</p>
<p>D. Use a larger regularization parameter.</p>
</li>
<li><p>Which of these techniques are useful for reducing variance (reducing overfitting)?</p>
<p>A. Xavier initialization</p>
<p>B. L2 regularization</p>
<p>C. Drop out</p>
<p>D. Data augmentation</p>
<p>E. Gradient checking</p>
<p>F. Vanishing gradient</p>
<p>G. Exploding gradient</p>
</li>
<li><p>Which of the following is the correct expression to normalize the input <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(x = \dfrac{x-\mu}{\sigma}\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(x = \dfrac{x}{\sigma}\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(x = \dfrac{1}{m} \sum_{i=1}^{m}\Big(x^{(i)}\Big)^2\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(x = \dfrac{1}{m} \sum_{i=1}^{m} x^{(i)}\)</span></p>
</li>
<li><p>The dev and test set should:</p>
<p>A. Be identical to each other (same (x,y) pairs)</p>
<p>B. Have the same number of examples</p>
<p>C. Come from the same distribution</p>
<p>D. Come from different distributions</p>
</li>
<li><p>A model developed for a project is presenting high bias. One of the sponsors of the project offers some resources that might help reduce the bias. Which of the following additional resources has a better chance to help reduce the bias?</p>
<p>A. Give access to more computational resources like GPUs.</p>
<p>B. Use different sources to gather data and better test the model.</p>
<p>C. Gather more data for the project.</p>
</li>
<li><p>Which of the following are regularization techniques?</p>
<p>A. Gradient Checking.</p>
<p>B. Increase the number of layers of the network.</p>
<p>C. Dropout.</p>
<p>D. Weight decay.</p>
</li>
<li><p><strong>True/False</strong> To reduce high variance, the regularization hyperparameter lambda must be increased.   __________</p></li>
<li><p>Decreasing the parameter keep_prob from (say) 0.6 to 0.4 will likely cause the following:</p>
<p>A. Increasing the regularization effect.</p>
<p>B. Causing the neural network to have a higher variance.</p>
<p>C. Reducing the regularization effect.</p>
</li>
<li><p>Which of the following actions increase the regularization of a model? (Check all that apply)</p>
<p>A. Increase the value of the hyperparameter lambda.</p>
<p>B. Decrease the value of the hyperparameter lambda.</p>
<p>C. Normalizing the data.</p>
<p>D. Increase the value of keep_prob in dropout.</p>
<p>E. Make use of data augmentation.</p>
</li>
<li><p>Suppose that a model uses, as one feature, the total number of kilometers walked by a person during a year, and another feature is the height of the person in meters. What is the most likely effect of normalization of the input data?</p>
<p>A. It will make the training faster.</p>
<p>B. It won’t have any positive or negative effects.</p>
<p>C. It will make the data easier to visualize.</p>
<p>D. It will increase the variance of the model.</p>
</li>
<li><p>When designing a neural network to detect if a house cat is present in the picture, 500,000 pictures of cats were taken by their owners. These are used to make the training, dev and test sets. It is decided that to increase the size of the test set, 10,000 new images of cats taken from security cameras are going to be used in the test set. Which of the following is true?</p>
<p>A. This will increase the bias of the model so the new images shouldn’t be used.</p>
<p>B. This will be harmful to the project since now dev and test sets have different distributions.</p>
<p>C. This will reduce the bias of the model and help improve it.</p>
</li>
<li><p>You are working on an automated check-out kiosk for a supermarket, and are building a classifier for apples, bananas and oranges. Suppose your classifier obtains a training set error of 0.5%, and a dev set error of 7%. Which of the following are promising things to try to improve your classifier? (Check all that apply.)</p>
<p>A. Increase the regularization parameter lambda.</p>
<p>B. Decrease the regularization parameter lambda.</p>
<p>C. Getting more training data.</p>
<p>D. Use a bigger neural network.</p>
</li>
<li><p>What is weight decay?</p>
<p>A. A technique to avoid vanishing gradient by imposing a ceiling on the values of the weights.</p>
<p>B. A regularization technique (such as L2 regularization) that results in gradient descent shrinking the weights on every iteration.</p>
<p>C. The process of gradually decreasing the learning rate during training.</p>
<p>D. General corruption of the weights in the neural network if it is trained on noisy data.</p>
</li>
<li><p>Which of the following actions increase the regularization of a model? (Check all that apply)</p>
<p>A. Use Xavier initialization.</p>
<p>B. Increase the value of keep_prob in dropout.</p>
<p>C. Increase the value of the hyperparameter lambda.</p>
<p>D. Decrease the value of keep_prob in dropout.</p>
<p>E. Decrease the value of the hyperparameter lambda.</p>
</li>
<li><p>Why do we normalize the inputs <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?</p>
<p>A. It makes it easier to visualize the data.</p>
<p>B. It makes the parameter initialization faster.</p>
<p>C. Normalization is another word for regularization–It helps to reduce variance.</p>
<p>D. It makes the cost function faster to optimize.</p>
</li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Click here for answers!</p>
<ol class="arabic">
<li><p>B </br></p>
<p>B. Given the size of the dataset, 0.5% of the samples are enough to get a good estimate of how well the model is doing. </br></p>
</li>
<li><p>C </br></p>
<p>C.  Although not recommended, if a more accurate measure of the performance is not necessary it is ok to not use a test set. However, this might cause an overfit to the dev set.</p>
</li>
<li><p>BC </br></p></li>
<li><p>A </br></p>
<p>A. This can be helpful to reduce the bias of the model, and then we can start trying to reduce the high variance if this happens.  </br></p>
</li>
<li><p>False </br></p>
<p>In most cases, it is recommended to not use dropout if there is no overfit. Although in computer vision, due to the nature of the data, it is the default practice. </br></p>
</li>
<li><p>False </br></p>
<p>The regularization parameter affects how the weights change during training, this means during backpropagation. It has no effect during the forward propagation that is when predictions for the test are made. </br></p>
</li>
<li><p>AB </br></p>
<p>A. The dropout is a regularization technique and thus helps to reduce the overfit. </br>
B. The probability that dropout doesn’t eliminate a neuron is keep_prob.</p>
</li>
<li><p>A </br></p>
<p>A careful initialization can help reduce the vanishing gradient problem. </br></p>
</li>
<li><p>BCD </br></p></li>
<li><p>A </br></p>
<p>A. This shifts the mean of the input to the origin and makes the variance one in each coordinate of the input examples.</p>
</li>
<li><p>C </br></p></li>
<li><p>A </br></p>
<p>A. This can allow the developers to try bigger networks, train for more cycles, and test different architectures. </br></p>
</li>
<li><p>CD</br></p>
<p>Using dropout layers is a regularization technique. Weight decay also is a form of regularization.</br></p>
</li>
<li><p>True </br></p>
<p>By increasing the regularization parameter the magnitude of the weight parameters is reduced. This helps avoid overfitting and reduces the variance. </br></p>
</li>
<li><p>A </br></p>
<p>A. This will make the dropout have a higher probability of eliminating a node in the neural network, increasing the regularization effect. </br></p>
</li>
<li><p>AE </br></p>
<p>A. When increasing the hyperparameter lambda we increase the effect of the L_2 penalization. </br>
E. Data augmentation has a way to generate “new” data at a relatively low cost. Thus making use of data augmentation can reduce the variance. </br></p>
</li>
<li><p>A </br></p>
<p>A. Since the difference between the ranges of the features is very different, this will likely cause the process of gradient descent to oscillate, making the optimization process longer. </br></p>
</li>
<li><p>B </br></p>
<p>B. The quality and type of images are quite different thus we can’t consider that the dev and the test sets came from the same distribution. </br></p>
</li>
<li><p>AC </br></p></li>
<li><p>B </br></p></li>
<li><p>CD </br></p>
<p>C. When increasing the hyperparameter lambda, we increase the effect of the L_2 penalization. </br>
D. When decreasing the keep_prob value, the probability that a node gets discarded during training is higher, thus reducing the regularization effect. </br></p>
</li>
<li><p>D </br></p></li>
</ol>
</div>
</section>
<div class="toctree-wrapper compound">
<span id="document-C5_Initialization"></span><section class="tex2jax_ignore mathjax_ignore" id="practical-5-initialization">
<h3>Practical 5: Initialization<a class="headerlink" href="#practical-5-initialization" title="Link to this heading">#</a></h3>
<p>Training your neural network requires specifying an initial value of the weights. A well-chosen initialization method helps the learning process.</p>
<p>If you completed the previous course of this specialization, you probably followed the instructions for weight initialization, and seen that it’s worked pretty well so far. But how do you choose the initialization for a new neural network? In this notebook, you’ll try out a few different initializations, including random, zeros, and He initialization, and see how each leads to different results.</p>
<p>A well-chosen initialization can:</p>
<ul class="simple">
<li><p>Speed up the convergence of gradient descent</p></li>
<li><p>Increase the odds of gradient descent converging to a lower training (and generalization) error</p></li>
</ul>
<p>Let’s get started!</p>
<section id="packages">
<h4>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">from</span> <span class="nn">init_utils_c2w1a1</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">compute_loss</span><span class="p">,</span> <span class="n">forward_propagation</span><span class="p">,</span> <span class="n">backward_propagation</span>
<span class="kn">from</span> <span class="nn">init_utils_c2w1a1</span> <span class="kn">import</span> <span class="n">update_parameters</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">plot_decision_boundary</span><span class="p">,</span> <span class="n">predict_dec</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span> <span class="c1"># set default size of plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.interpolation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.cmap&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="c1"># load image dataset: blue/red dots in circles</span>
<span class="c1"># train_X, train_Y, test_X, test_Y = load_dataset()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-dataset">
<h4>2 - Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/78b92f161a025e8f8e4f8eebc98c3db03235993c3b2b4ac377a293562a2ae051.png" src="_images/78b92f161a025e8f8e4f8eebc98c3db03235993c3b2b4ac377a293562a2ae051.png" />
</div>
</div>
<p>For this classifier, you want to separate the blue dots from the red dots.</p>
</section>
<section id="neural-network-model">
<h4>3 - Neural Network Model<a class="headerlink" href="#neural-network-model" title="Link to this heading">#</a></h4>
<p>You’ll use a 3-layer neural network (already implemented for you). These are the initialization methods you’ll experiment with:</p>
<ul class="simple">
<li><p><em>Zeros initialization</em> –  setting <code class="docutils literal notranslate"><span class="pre">initialization</span> <span class="pre">=</span> <span class="pre">&quot;zeros&quot;</span></code> in the input argument.</p></li>
<li><p><em>Random initialization</em> – setting <code class="docutils literal notranslate"><span class="pre">initialization</span> <span class="pre">=</span> <span class="pre">&quot;random&quot;</span></code> in the input argument. This initializes the weights to large random values.</p></li>
<li><p><em>He initialization</em> – setting <code class="docutils literal notranslate"><span class="pre">initialization</span> <span class="pre">=</span> <span class="pre">&quot;he&quot;</span></code> in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.</p></li>
</ul>
<p><strong>Instructions</strong>: Instructions: Read over the code below, and run it. In the next part, you’ll implement the three initialization methods that this <code class="docutils literal notranslate"><span class="pre">model()</span></code> calls.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">15000</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;he&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input data, of shape (2, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span>
<span class="sd">    learning_rate -- learning rate for gradient descent </span>
<span class="sd">    num_iterations -- number of iterations to run gradient descent</span>
<span class="sd">    print_cost -- if True, print the cost every 1000 iterations</span>
<span class="sd">    initialization -- flag to choose which initialization to use (&quot;zeros&quot;,&quot;random&quot; or &quot;he&quot;)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- parameters learnt by the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
        
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># to keep track of the loss</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of examples</span>
    <span class="n">layers_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Initialize parameters dictionary.</span>
    <span class="k">if</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_zeros</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_random</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;he&quot;</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_he</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>

    <span class="c1"># Loop (gradient descent)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>
        <span class="n">a3</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        
        <span class="c1"># Loss</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">a3</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

        <span class="c1"># Backward propagation.</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
        
        <span class="c1"># Update parameters.</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="c1"># Print the loss every 1000 iterations</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
            
    <span class="c1"># plot the loss</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (per hundreds)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning rate =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="zero-initialization">
<h4>4 - Zero Initialization<a class="headerlink" href="#zero-initialization" title="Link to this heading">#</a></h4>
<p>There are two types of parameters to initialize in a neural network:</p>
<ul class="simple">
<li><p>the weight matrices <span class="math notranslate nohighlight">\((W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})\)</span></p></li>
<li><p>the bias vectors <span class="math notranslate nohighlight">\((b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})\)</span></p></li>
</ul>
<section id="exercise-1-initialize-parameters-zeros">
<h5>Exercise 1 - initialize_parameters_zeros<a class="headerlink" href="#exercise-1-initialize-parameters-zeros" title="Link to this heading">#</a></h5>
<p>Implement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry,” but try it anyway and see what happens. Use <code class="docutils literal notranslate"><span class="pre">np.zeros((..,..))</span></code> with the correct shapes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_zeros </span>

<span class="k">def</span> <span class="nf">initialize_parameters_zeros</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the size of each layer.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span>
<span class="sd">                    b1 -- bias vector of shape (layers_dims[1], 1)</span>
<span class="sd">                    ...</span>
<span class="sd">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span>
<span class="sd">                    bL -- bias vector of shape (layers_dims[L], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>            <span class="c1"># number of layers in the network</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = </span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] = </span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[0. 0. 0.]
 [0. 0. 0.]]
b1 = [[0.]
 [0.]]
W2 = [[0. 0.]]
b2 = [[0.]]
</pre></div>
</div>
</div>
</div>
<p>Run the following code to train your model on 15,000 iterations using zeros initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.6931471805599453
Cost after iteration 1000: 0.6931471805599453
Cost after iteration 2000: 0.6931471805599453
Cost after iteration 3000: 0.6931471805599453
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 4000: 0.6931471805599453
Cost after iteration 5000: 0.6931471805599453
Cost after iteration 6000: 0.6931471805599453
Cost after iteration 7000: 0.6931471805599453
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 8000: 0.6931471805599453
Cost after iteration 9000: 0.6931471805599453
Cost after iteration 10000: 0.6931471805599453
Cost after iteration 11000: 0.6931471805599453
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 12000: 0.6931471805599453
Cost after iteration 13000: 0.6931471805599453
Cost after iteration 14000: 0.6931471805599453
</pre></div>
</div>
<img alt="_images/a33578c59351da63463c61a1e05a2bbbbf48fcf861655eaa69029fcbe391a87f.png" src="_images/a33578c59351da63463c61a1e05a2bbbbf48fcf861655eaa69029fcbe391a87f.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.5
On the test set:
Accuracy: 0.5
</pre></div>
</div>
</div>
</div>
<p>The performance is terrible, the cost doesn’t decrease, and the algorithm performs no better than random guessing. Why? Take a look at the details of the predictions and the decision boundary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;predictions_train = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">predictions_train</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;predictions_test = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">predictions_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0]]
predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model with Zeros initialization&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c253b732069bc0bc104468614d8c3ba47f77cf095f89fb3c02142a72a137a5cf.png" src="_images/c253b732069bc0bc104468614d8c3ba47f77cf095f89fb3c02142a72a137a5cf.png" />
</div>
</div>
<p>For a comprehensive explanation of this, you can read <em>Paul Mielke</em>’s post, <a class="reference external" href="https://community.deeplearning.ai/t/symmetry-breaking-versus-zero-initialization/16061">Symmetry Breaking versus Zero Initialization</a>.</p>
<p>A simple explanation is provided below:</p>
<p><strong>Note</strong>: For sake of simplicity calculations below are done using only one example at a time.</p>
<p>Since the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">0</span></code></p>
<div class="math notranslate nohighlight">
\[a = ReLU(z) = max(0, z) = 0\]</div>
<p>At the classification layer, where the activation function is sigmoid you then get (for either input):</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{ 1 + e^{-(z)}} = \frac{1}{2} = y_{pred}\]</div>
<p>As for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.</p>
<p>Your loss function:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(a, y) =  - y  \ln(y_{pred}) - (1-y)  \ln(1-y_{pred})\]</div>
<p>For <code class="docutils literal notranslate"><span class="pre">y=1</span></code>, <code class="docutils literal notranslate"><span class="pre">y_pred=0.5</span></code> it becomes:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(0, 1) =  - (1)  \ln(\frac{1}{2}) = 0.6931471805599453\]</div>
<p>For <code class="docutils literal notranslate"><span class="pre">y=0</span></code>, <code class="docutils literal notranslate"><span class="pre">y_pred=0.5</span></code> it becomes:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(0, 0) =  - (1)  \ln(\frac{1}{2}) = 0.6931471805599453\]</div>
<p>As you can see with the prediction being 0.5 whether the actual (<code class="docutils literal notranslate"><span class="pre">y</span></code>) value is 1 or 0 you get the same loss value for both, so none of the weights get adjusted and you are stuck with the same old value of the weights.</p>
<p>This is why you can see that the model is predicting 0 for every example! No wonder it’s doing so badly.</p>
<p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with <span class="math notranslate nohighlight">\(n^{[l]}=1\)</span> for every layer. This way, the network is no more powerful than a linear classifier like logistic regression.</p>
<div class="warning admonition">
<p class="admonition-title">What you should remember</p>
<ul class="simple">
<li><p>The weights <span class="math notranslate nohighlight">\(W^{[l]}\)</span> should be initialized randomly to break symmetry.</p></li>
<li><p>However, it’s okay to initialize the biases <span class="math notranslate nohighlight">\(b^{[l]}\)</span> to zeros. Symmetry is still broken so long as <span class="math notranslate nohighlight">\(W^{[l]}\)</span> is initialized randomly.</p></li>
</ul>
</div>
</section>
</section>
<section id="random-initialization">
<h4>5 - Random Initialization<a class="headerlink" href="#random-initialization" title="Link to this heading">#</a></h4>
<p>To break symmetry, initialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you’ll see what happens when the weights are initialized randomly, but to very large values.</p>
<section id="exercise-2-initialize-parameters-random">
<h5>Exercise 2 - initialize_parameters_random<a class="headerlink" href="#exercise-2-initialize-parameters-random" title="Link to this heading">#</a></h5>
<p>Implement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use <code class="docutils literal notranslate"><span class="pre">np.random.randn(..,..)</span> <span class="pre">*</span> <span class="pre">10</span></code> for weights and <code class="docutils literal notranslate"><span class="pre">np.zeros((..,</span> <span class="pre">..))</span></code> for biases. You’re using a fixed <code class="docutils literal notranslate"><span class="pre">np.random.seed(..)</span></code> to make sure your “random” weights  match ours, so don’t worry if running your code several times always gives you the same initial values for the parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_random</span>

<span class="k">def</span> <span class="nf">initialize_parameters_random</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the size of each layer.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span>
<span class="sd">                    b1 -- bias vector of shape (layers_dims[1], 1)</span>
<span class="sd">                    ...</span>
<span class="sd">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span>
<span class="sd">                    bL -- bias vector of shape (layers_dims[L], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>               <span class="c1"># This seed makes sure your &quot;random&quot; numbers will be the as ours</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>            <span class="c1"># integer representing the number of layers</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = </span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] =</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mi">10</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_random</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[ 17.88628473   4.36509851   0.96497468]
 [-18.63492703  -2.77388203  -3.54758979]]
b1 = [[0.]
 [0.]]
W2 = [[-0.82741481 -6.27000677]]
b2 = [[0.]]
</pre></div>
</div>
</div>
</div>
<p>Run the following code to train your model on 15,000 iterations using random initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: inf
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1000: 0.6229180433609748
Cost after iteration 2000: 0.5977815578395076
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/chao/Downloads/DLS_C1/init_utils_c2w1a1.py:145: RuntimeWarning: divide by zero encountered in log
  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)
/Users/chao/Downloads/DLS_C1/init_utils_c2w1a1.py:145: RuntimeWarning: invalid value encountered in multiply
  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 3000: 0.5635780597997223
Cost after iteration 4000: 0.5500778186719544
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 5000: 0.5443350780361031
Cost after iteration 6000: 0.5373501033809833
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 7000: 0.46969127036420355
Cost after iteration 8000: 0.3976580931050209
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 9000: 0.3934419967603351
Cost after iteration 10000: 0.39201738235219885
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 11000: 0.3891208899328107
Cost after iteration 12000: 0.3861245285297949
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 13000: 0.38496976596639154
Cost after iteration 14000: 0.3827514414987576
</pre></div>
</div>
<img alt="_images/b26456635ed02a62189289a0965cc74027809c94b2d0b61477c2da6c518fcfc4.png" src="_images/b26456635ed02a62189289a0965cc74027809c94b2d0b61477c2da6c518fcfc4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.83
On the test set:
Accuracy: 0.86
</pre></div>
</div>
</div>
</div>
<p>If you see “inf” as the cost after the iteration 0, this is because of numerical roundoff. A more numerically sophisticated implementation would fix this, but for the purposes of this notebook, it isn’t really worth worrying about.</p>
<p>In any case, you’ve now broken the symmetry, and this gives noticeably better accuracy than before. The model is no longer outputting all 0s. Progress!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="n">predictions_train</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">predictions_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1
  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0
  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0
  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0
  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1
  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1
  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1
  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1
  1 1 1 1 0 0 0 1 1 1 1 0]]
[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1
  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0
  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/678e035ab188806e520958f9fe697da5821cebe4085eaf6f8d971e7f09a57a23.png" src="_images/678e035ab188806e520958f9fe697da5821cebe4085eaf6f8d971e7f09a57a23.png" />
</div>
</div>
<p><strong>Observations</strong>:</p>
<ul class="simple">
<li><p>The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when <span class="math notranslate nohighlight">\(\log(a^{[3]}) = \log(0)\)</span>, the loss goes to infinity.</p></li>
<li><p>Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm.</p></li>
<li><p>If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">In summary</p>
<ul class="simple">
<li><p>Initializing weights to very large random values doesn’t work well.</p></li>
<li><p>Initializing with small random values should do better. The important question is, how small should be these random values be? Let’s find out up next!</p></li>
</ul>
</div>
<p><strong>Optional Read:</strong></p>
<p>The main difference between Gaussian variable (<code class="docutils literal notranslate"><span class="pre">numpy.random.randn()</span></code>) and uniform random variable is the distribution of the generated random numbers:</p>
<ul class="simple">
<li><p>numpy.random.rand() produces numbers in a <a class="reference external" href="https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/rand.jpg">uniform distribution</a>.</p></li>
<li><p>and numpy.random.randn() produces numbers in a <a class="reference external" href="https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/randn.jpg">normal distribution</a>.</p></li>
</ul>
<p>When used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range.</p>
<p>An intuitive way to see it is, for example, if you take the <a class="reference external" href="https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/sigmoid.jpg">sigmoid() activation function</a>.</p>
<p>You’ll remember that the slope near 0 or near 1 is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence.</p>
</section>
</section>
<section id="he-initialization">
<h4>6 - He Initialization<a class="headerlink" href="#he-initialization" title="Link to this heading">#</a></h4>
<p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights <span class="math notranslate nohighlight">\(W^{[l]}\)</span> of <code class="docutils literal notranslate"><span class="pre">sqrt(1./layers_dims[l-1])</span></code> where He initialization would use <code class="docutils literal notranslate"><span class="pre">sqrt(2./layers_dims[l-1])</span></code>.)</p>
<section id="exercise-3-initialize-parameters-he">
<h5>Exercise 3 - initialize_parameters_he<a class="headerlink" href="#exercise-3-initialize-parameters-he" title="Link to this heading">#</a></h5>
<p>Implement the following function to initialize your parameters with He initialization. This function is similar to the previous <code class="docutils literal notranslate"><span class="pre">initialize_parameters_random(...)</span></code>. The only difference is that instead of multiplying <code class="docutils literal notranslate"><span class="pre">np.random.randn(..,..)</span></code> by 10, you will multiply it by <span class="math notranslate nohighlight">\(\sqrt{\frac{2}{\text{dimension of the previous layer}}}\)</span>, which is what He initialization recommends for layers with a ReLU activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_he</span>

<span class="k">def</span> <span class="nf">initialize_parameters_he</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the size of each layer.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span>
<span class="sd">                    b1 -- bias vector of shape (layers_dims[1], 1)</span>
<span class="sd">                    ...</span>
<span class="sd">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span>
<span class="sd">                    bL -- bias vector of shape (layers_dims[L], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># integer representing the number of layers</span>
     
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = </span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] =</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">/</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_he</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="c1"># parameters</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[ 1.78862847  0.43650985]
 [ 0.09649747 -1.8634927 ]
 [-0.2773882  -0.35475898]
 [-0.08274148 -0.62700068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]
b2 = [[0.]]
</pre></div>
</div>
</div>
</div>
<p>Run the following code to train your model on 15,000 iterations using He initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;he&quot;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.8830537463419761
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1000: 0.6879825919728063
Cost after iteration 2000: 0.6751286264523371
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 3000: 0.6526117768893807
Cost after iteration 4000: 0.6082958970572938
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 5000: 0.5304944491717495
Cost after iteration 6000: 0.41386458170717944
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 7000: 0.31178034648444414
Cost after iteration 8000: 0.23696215330322562
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 9000: 0.18597287209206834
Cost after iteration 10000: 0.15015556280371808
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 11000: 0.12325079292273548
Cost after iteration 12000: 0.09917746546525932
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 13000: 0.08457055954024276
Cost after iteration 14000: 0.07357895962677367
</pre></div>
</div>
<img alt="_images/52ea93ba66ced89cb46d825053f5a727c5f7a4e9f0c3bbe86bb8ca31efb08068.png" src="_images/52ea93ba66ced89cb46d825053f5a727c5f7a4e9f0c3bbe86bb8ca31efb08068.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.9933333333333333
On the test set:
Accuracy: 0.96
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model with He initialization&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d39c7f928e9d715ad7ae2513fdd3e874b32044c7ed5647c783e29eeb9d5a1c38.png" src="_images/d39c7f928e9d715ad7ae2513fdd3e874b32044c7ed5647c783e29eeb9d5a1c38.png" />
</div>
</div>
<p><strong>Observations</strong>:</p>
<ul class="simple">
<li><p>The model with He initialization separates the blue and the red dots very well in a small number of iterations.</p></li>
</ul>
</section>
</section>
<section id="conclusions">
<h4>7 - Conclusions<a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h4>
<p><strong>Congratulations</strong>! You’ve completed this notebook on Initialization.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Train accuracy</p></th>
<th class="head"><p>Problem/Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3-layer NN with zeros initialization</p></td>
<td><p>50%</p></td>
<td><p>fails to break symmetry</p></td>
</tr>
<tr class="row-odd"><td><p>3-layer NN with large random initialization</p></td>
<td><p>83%</p></td>
<td><p>too large weights</p></td>
</tr>
<tr class="row-even"><td><p>3-layer NN with He initialization</p></td>
<td><p>99%</p></td>
<td><p>recommended method</p></td>
</tr>
</tbody>
</table>
<div class="warning admonition">
<p class="admonition-title">In summary</p>
<p>Here’s a quick recap of the main takeaways:</p>
<ul class="simple">
<li><p>Different initializations lead to very different results</p></li>
<li><p>Random initialization is used to break symmetry and make sure different hidden units can learn different things</p></li>
<li><p>Resist initializing to values that are too large!</p></li>
<li><p>He initialization works well for networks with ReLU activations</p></li>
</ul>
</div>
</section>
</section>
<span id="document-C5_Regularization"></span><section class="tex2jax_ignore mathjax_ignore" id="practical-6-regularization">
<h3>Practical 6: Regularization<a class="headerlink" href="#practical-6-regularization" title="Link to this heading">#</a></h3>
<p>Welcome to the second assignment of this week. Deep Learning models have so much flexibility and capacity that <strong>overfitting can be a serious problem</strong>, if the training dataset is not big enough. Sure it does well on the training set, but the learned network <strong>doesn’t generalize to new examples</strong> that it has never seen!</p>
<p><strong>You will learn to:</strong> Use regularization in your deep learning models.</p>
<p>Let’s get started!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import packages</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">import</span> <span class="nn">scipy.io</span>
<span class="kn">from</span> <span class="nn">reg_utils_c2w1a2</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">plot_decision_boundary</span><span class="p">,</span> <span class="n">initialize_parameters</span><span class="p">,</span> <span class="n">load_2D_dataset</span><span class="p">,</span> <span class="n">predict_dec</span>
<span class="kn">from</span> <span class="nn">reg_utils_c2w1a2</span> <span class="kn">import</span> <span class="n">compute_cost</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">forward_propagation</span><span class="p">,</span> <span class="n">backward_propagation</span><span class="p">,</span> <span class="n">update_parameters</span>
<span class="kn">from</span> <span class="nn">testCases_c2w1a2</span> <span class="kn">import</span> <span class="o">*</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span> <span class="c1"># set default size of plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.interpolation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.cmap&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="problem-statement">
<h4>2 - Problem Statement<a class="headerlink" href="#problem-statement" title="Link to this heading">#</a></h4>
<p>You have just been hired as an AI expert by the French Football Corporation. They would like you to recommend positions where France’s goal keeper should kick the ball so that the French team’s players can then hit it with their head.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/field_kiank.png"><img alt="_images/field_kiank.png" src="_images/field_kiank.png" style="height: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Football field. The goal keeper kicks the ball in the air, the players of each team are fighting to hit the ball with their head</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>They give you the following 2D dataset from France’s past 10 games.</p>
</section>
<section id="loading-the-dataset">
<h4>3 - Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span> <span class="o">=</span> <span class="n">load_2D_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0247c9a05afc59c89e6fadbdfacc5d9c6133a6ddf64daab034ed0d7c3dca8e71.png" src="_images/0247c9a05afc59c89e6fadbdfacc5d9c6133a6ddf64daab034ed0d7c3dca8e71.png" />
</div>
</div>
<p>Each dot corresponds to a position on the football field where a football player has hit the ball with his/her head after the French goal keeper has shot the ball from the left side of the football field.</p>
<ul class="simple">
<li><p>If the dot is blue, it means the French player managed to hit the ball with his/her head</p></li>
<li><p>If the dot is red, it means the other team’s player hit the ball with their head</p></li>
</ul>
<p><strong>Your goal</strong>: Use a deep learning model to find the positions on the field where the goalkeeper should kick the ball.</p>
<p><strong>Analysis of the dataset</strong>: This dataset is a little noisy, but it looks like a diagonal line separating the upper left half (blue) from the lower right half (red) would work well.</p>
<p>You will first try a non-regularized model. Then you’ll learn how to regularize it and decide which model you will choose to solve the French Football Corporation’s problem.</p>
</section>
<section id="non-regularized-model">
<h4>4 - Non-Regularized Model<a class="headerlink" href="#non-regularized-model" title="Link to this heading">#</a></h4>
<p>You will use the following neural network (already implemented for you below). This model can be used:</p>
<ul class="simple">
<li><p>in <em>regularization mode</em> – by setting the <code class="docutils literal notranslate"><span class="pre">lambd</span></code> input to a non-zero value. We use “<code class="docutils literal notranslate"><span class="pre">lambd</span></code>” instead of “<code class="docutils literal notranslate"><span class="pre">lambda</span></code>” because “<code class="docutils literal notranslate"><span class="pre">lambda</span></code>” is a reserved keyword in Python.</p></li>
<li><p>in <em>dropout mode</em> – by setting the <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> to a value less than one</p></li>
</ul>
<p>You will first try the model without any regularization. Then, you will implement:</p>
<ul class="simple">
<li><p><em>L2 regularization</em> – functions: “<code class="docutils literal notranslate"><span class="pre">compute_cost_with_regularization()</span></code>” and “<code class="docutils literal notranslate"><span class="pre">backward_propagation_with_regularization()</span></code>”</p></li>
<li><p><em>Dropout</em> – functions: “<code class="docutils literal notranslate"><span class="pre">forward_propagation_with_dropout()</span></code>” and “<code class="docutils literal notranslate"><span class="pre">backward_propagation_with_dropout()</span></code>”</p></li>
</ul>
<p>In each part, you will run this model with the correct inputs so that it calls the functions you’ve implemented. Take a look at the code below to familiarize yourself with the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">30000</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input data, of shape (input size, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)</span>
<span class="sd">    learning_rate -- learning rate of the optimization</span>
<span class="sd">    num_iterations -- number of iterations of the optimization loop</span>
<span class="sd">    print_cost -- If True, print the cost every 10000 iterations</span>
<span class="sd">    lambd -- regularization hyperparameter, scalar</span>
<span class="sd">    keep_prob - probability of keeping a neuron active during drop-out, scalar.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- parameters learned by the model. They can then be used to predict.</span>
<span class="sd">    &quot;&quot;&quot;</span>
        
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span>                            <span class="c1"># to keep track of the cost</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>                        <span class="c1"># number of examples</span>
    <span class="n">layers_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Initialize parameters dictionary.</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>

    <span class="c1"># Loop (gradient descent)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>
        <span class="k">if</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">a3</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">a3</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation_with_dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
        
        <span class="c1"># Cost function</span>
        <span class="k">if</span> <span class="n">lambd</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">a3</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost_with_regularization</span><span class="p">(</span><span class="n">a3</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>
            
        <span class="c1"># Backward propagation.</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">lambd</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>   <span class="c1"># it is possible to use both L2 regularization and dropout, </span>
                                                <span class="c1"># but this assignment will only explore one at a time</span>
        <span class="k">if</span> <span class="n">lambd</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">keep_prob</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">lambd</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation_with_regularization</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation_with_dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">)</span>
        
        <span class="c1"># Update parameters.</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="c1"># Print the loss every 10000 iterations</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    
    <span class="c1"># plot the cost</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (x1,000)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning rate =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s train the model without any regularization, and observe the accuracy on the train/test sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the training set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.6557412523481002
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 10000: 0.16329987525724177
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 20000: 0.13851642423233596
</pre></div>
</div>
<img alt="_images/2dbfa2d9a218adb605aabea60411c2d3859e06f291f554fd0c3877046d08fb23.png" src="_images/2dbfa2d9a218adb605aabea60411c2d3859e06f291f554fd0c3877046d08fb23.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the training set:
Accuracy: 0.9478672985781991
On the test set:
Accuracy: 0.915
</pre></div>
</div>
</div>
</div>
<p>The train accuracy is 94.8% while the test accuracy is 91.5%. This is the <strong>baseline model</strong> (you will observe the impact of regularization on this model). Run the following code to plot the decision boundary of your model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model without regularization&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.40</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.65</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5710f67af3605f032492253666928c91194a6a848c2144f92e45cd102388c4e9.png" src="_images/5710f67af3605f032492253666928c91194a6a848c2144f92e45cd102388c4e9.png" />
</div>
</div>
<p>The non-regularized model is obviously overfitting the training set. It is fitting the noisy points! Lets now look at two techniques to reduce overfitting.</p>
</section>
<section id="l2-regularization">
<h4>5 - L2 Regularization<a class="headerlink" href="#l2-regularization" title="Link to this heading">#</a></h4>
<p>The standard way to avoid overfitting is called <strong>L2 regularization</strong>. It consists of appropriately modifying your cost function, from:</p>
<div class="math notranslate nohighlight">
\[J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}\]</div>
<p>To:</p>
<div class="math notranslate nohighlight">
\[J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}\]</div>
<p>Let’s modify your cost and observe the consequences.</p>
<section id="exercise-1-compute-cost-with-regularization">
<h5>Exercise 1 - compute_cost_with_regularization<a class="headerlink" href="#exercise-1-compute-cost-with-regularization" title="Link to this heading">#</a></h5>
<p>Implement <code class="docutils literal notranslate"><span class="pre">compute_cost_with_regularization()</span></code> which computes the cost given by formula (2). To calculate <span class="math notranslate nohighlight">\(\sum\limits_k\sum\limits_j W_{k,j}^{[l]2}\)</span>  , use :</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">Wl</span><span class="p">))</span>
</pre></div>
</div>
<p>Note that you have to do this for <span class="math notranslate nohighlight">\(W^{[1]}\)</span>, <span class="math notranslate nohighlight">\(W^{[2]}\)</span> and <span class="math notranslate nohighlight">\(W^{[3]}\)</span>, then sum the three terms and multiply by <span class="math notranslate nohighlight">\( \frac{1}{m} \frac{\lambda}{2} \)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: compute_cost_with_regularization</span>

<span class="k">def</span> <span class="nf">compute_cost_with_regularization</span><span class="p">(</span><span class="n">A3</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">lambd</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implement the cost function with L2 regularization. See formula (2) above.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)</span>
<span class="sd">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span>
<span class="sd">    parameters -- python dictionary containing parameters of the model</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    cost - value of the regularized loss function (formula (2))</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W3&quot;</span><span class="p">]</span>
    
    <span class="n">cross_entropy_cost</span> <span class="o">=</span> <span class="n">compute_cost</span><span class="p">(</span><span class="n">A3</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> <span class="c1"># This gives you the cross-entropy part of the cost</span>
    
    <span class="c1">#(≈ 1 lines of code)</span>
    <span class="c1"># L2_regularization_cost = </span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">L2_regularization_cost</span> <span class="o">=</span> <span class="p">(</span><span class="n">lambd</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">m</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">W1</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">W2</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">W3</span><span class="p">)))</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    
    <span class="n">cost</span> <span class="o">=</span> <span class="n">cross_entropy_cost</span> <span class="o">+</span> <span class="n">L2_regularization_cost</span>
    
    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A3</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">compute_cost_with_regularization_test_case</span><span class="p">()</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">compute_cost_with_regularization</span><span class="p">(</span><span class="n">A3</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;cost = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>cost = 1.7864859451590758
</pre></div>
</div>
</div>
</div>
<p>Of course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost.</p>
</section>
<section id="exercise-2-backward-propagation-with-regularization">
<h5>Exercise 2 - backward_propagation_with_regularization<a class="headerlink" href="#exercise-2-backward-propagation-with-regularization" title="Link to this heading">#</a></h5>
<p>Implement the changes needed in backward propagation to take into account regularization. The changes only concern dW1, dW2 and dW3. For each, you have to add the regularization term’s gradient (<span class="math notranslate nohighlight">\(\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W\)</span>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: backward_propagation_with_regularization</span>

<span class="k">def</span> <span class="nf">backward_propagation_with_regularization</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">lambd</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the backward propagation of our baseline model to which we added an L2 regularization.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input dataset, of shape (input size, number of examples)</span>
<span class="sd">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span>
<span class="sd">    cache -- cache output from forward_propagation()</span>
<span class="sd">    lambd -- regularization hyperparameter, scalar</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">Z3</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="n">dZ3</span> <span class="o">=</span> <span class="n">A3</span> <span class="o">-</span> <span class="n">Y</span>
    <span class="c1">#(≈ 1 lines of code)</span>
    <span class="c1"># dW3 = 1./m * np.dot(dZ3, A2.T) + None</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dW3</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ3</span><span class="p">,</span> <span class="n">A2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambd</span><span class="o">*</span><span class="n">W3</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">db3</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">dA2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ3</span><span class="p">)</span>
    <span class="n">dZ2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="c1">#(≈ 1 lines of code)</span>
    <span class="c1"># dW2 = 1./m * np.dot(dZ2, A1.T) + None</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambd</span><span class="o">*</span><span class="n">W2</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">dA1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span>
    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A1</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="c1">#(≈ 1 lines of code)</span>
    <span class="c1"># dW1 = 1./m * np.dot(dZ1, X.T) + None</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">lambd</span><span class="o">*</span><span class="n">W1</span><span class="p">)</span><span class="o">/</span><span class="n">m</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dZ3&quot;</span><span class="p">:</span> <span class="n">dZ3</span><span class="p">,</span> <span class="s2">&quot;dW3&quot;</span><span class="p">:</span> <span class="n">dW3</span><span class="p">,</span> <span class="s2">&quot;db3&quot;</span><span class="p">:</span> <span class="n">db3</span><span class="p">,</span><span class="s2">&quot;dA2&quot;</span><span class="p">:</span> <span class="n">dA2</span><span class="p">,</span>
                 <span class="s2">&quot;dZ2&quot;</span><span class="p">:</span> <span class="n">dZ2</span><span class="p">,</span> <span class="s2">&quot;dW2&quot;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span> <span class="s2">&quot;db2&quot;</span><span class="p">:</span> <span class="n">db2</span><span class="p">,</span> <span class="s2">&quot;dA1&quot;</span><span class="p">:</span> <span class="n">dA1</span><span class="p">,</span> 
                 <span class="s2">&quot;dZ1&quot;</span><span class="p">:</span> <span class="n">dZ1</span><span class="p">,</span> <span class="s2">&quot;dW1&quot;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span> <span class="s2">&quot;db1&quot;</span><span class="p">:</span> <span class="n">db1</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">gradients</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">backward_propagation_with_regularization_test_case</span><span class="p">()</span>

<span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation_with_regularization</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dW1 = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dW2 = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW2&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dW3 = </span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">grads</span><span class="p">[</span><span class="s2">&quot;dW3&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dW1 = 
[[-0.25604646  0.12298827 -0.28297129]
 [-0.17706303  0.34536094 -0.4410571 ]]
dW2 = 
[[ 0.79276486  0.85133918]
 [-0.0957219  -0.01720463]
 [-0.13100772 -0.03750433]]
dW3 = 
[[-1.77691347 -0.11832879 -0.09397446]]
</pre></div>
</div>
</div>
</div>
<p>Let’s now run the model with L2 regularization <span class="math notranslate nohighlight">\((\lambda = 0.7)\)</span>. The <code class="docutils literal notranslate"><span class="pre">model()</span></code> function will call:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">compute_cost_with_regularization</span></code> instead of <code class="docutils literal notranslate"><span class="pre">compute_cost</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">backward_propagation_with_regularization</span></code> instead of <code class="docutils literal notranslate"><span class="pre">backward_propagation</span></code></p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">lambd</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.6974484493131264
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 10000: 0.2684918873282239
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 20000: 0.2680916337127301
</pre></div>
</div>
<img alt="_images/c64365498da7849d60b0339beba5e8c049421f26a29c97e532ce410318413ca8.png" src="_images/c64365498da7849d60b0339beba5e8c049421f26a29c97e532ce410318413ca8.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.9383886255924171
On the test set:
Accuracy: 0.93
</pre></div>
</div>
</div>
</div>
<p>Congrats, the test set accuracy increased to 93%. You have saved the French football team!</p>
<p>You are not overfitting the training data anymore. Let’s plot the decision boundary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model with L2-regularization&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.40</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.65</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/879e15066d968cab8f2dc74583e7833550ff9f5128de35ff690b31a33d26a9bc.png" src="_images/879e15066d968cab8f2dc74583e7833550ff9f5128de35ff690b31a33d26a9bc.png" />
</div>
</div>
<p><strong>Observations</strong>:</p>
<ul class="simple">
<li><p>The value of <span class="math notranslate nohighlight">\(\lambda\)</span> is a hyperparameter that you can tune using a dev set.</p></li>
<li><p>L2 regularization makes your decision boundary smoother. If <span class="math notranslate nohighlight">\(\lambda\)</span> is too large, it is also possible to “oversmooth”, resulting in a model with high bias.</p></li>
</ul>
<div class="admonition-in-summary admonition">
<p class="admonition-title">In summary</p>
<p><strong>What is L2-regularization actually doing?</strong>:</p>
<p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.</p>
<p><strong>What you should remember:</strong> the implications of L2-regularization on:</p>
<ul class="simple">
<li><p>The cost computation:</p>
<ul>
<li><p>A regularization term is added to the cost.</p></li>
</ul>
</li>
<li><p>The backpropagation function:</p>
<ul>
<li><p>There are extra terms in the gradients with respect to weight matrices.</p></li>
</ul>
</li>
<li><p>Weights end up smaller (“weight decay”):</p>
<ul>
<li><p>Weights are pushed to smaller values.</p></li>
</ul>
</li>
</ul>
</div>
</section>
</section>
<section id="dropout">
<h4>6 - Dropout<a class="headerlink" href="#dropout" title="Link to this heading">#</a></h4>
<p>Finally, <strong>dropout</strong> is a widely used regularization technique that is specific to deep learning.
<strong>It randomly shuts down some neurons in each iteration.</strong> Watch these two videos to see what this means!</p>
<!--
To understand drop-out, consider this conversation with a friend:
- Friend: "Why do you need all these neurons to train your network and classify images?". 
- You: "Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!"
- Friend: "I see, but are you sure that your neurons are learning different features and not all the same features?"
- You: "Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution."
!--> <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">HTML</span>

<span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;</span>
<span class="s2">        &lt;source src=&quot;images/dropout1_kiank.mp4&quot; type=&quot;video/mp4&quot;&gt;</span>
<span class="s2">    &lt;/video&gt;</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <video width="320" height="240" controls>
        <source src="images/dropout1_kiank.mp4" type="video/mp4">
    </video>
</div></div>
</div>
<p>Drop-out on the second hidden layer. At each iteration, you shut down (= set to zero) each neuron of a layer with probability <span class="math notranslate nohighlight">\(1 - keep\_prob\)</span> or keep it with probability <span class="math notranslate nohighlight">\(keep\_prob\)</span> (50% here). The dropped neurons don’t contribute to the training in both the forward and backward propagations of the iteration.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    &lt;video width=&quot;320&quot; height=&quot;240&quot; controls&gt;</span>
<span class="s2">        &lt;source src=&quot;images/dropout2_kiank.mp4&quot; type=&quot;video/mp4&quot;&gt;</span>
<span class="s2">    &lt;/video&gt;</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html">
    <video width="320" height="240" controls>
        <source src="images/dropout2_kiank.mp4" type="video/mp4">
    </video>
</div></div>
</div>
<p>Drop-out on the first and third hidden layers. <span class="math notranslate nohighlight">\(1^{st}\)</span> layer: we shut down on average 40% of the neurons.  <span class="math notranslate nohighlight">\(3^{rd}\)</span> layer: we shut down on average 20% of the neurons.</p>
<p>When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.</p>
<section id="forward-propagation-with-dropout">
<h5>6.1 - Forward Propagation with Dropout<a class="headerlink" href="#forward-propagation-with-dropout" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-3-forward-propagation-with-dropout">
<h5>Exercise 3 - forward_propagation_with_dropout<a class="headerlink" href="#exercise-3-forward-propagation-with-dropout" title="Link to this heading">#</a></h5>
<p>Implement the forward propagation with dropout. You are using a 3 layer neural network, and will add dropout to the first and second hidden layers. We will not apply dropout to the input layer or output layer.</p>
<p><strong>Instructions</strong>:
You would like to shut down some neurons in the first and second layers. To do that, you are going to carry out 4 Steps:</p>
<ol class="arabic simple">
<li><p>In lecture, we dicussed creating a variable <span class="math notranslate nohighlight">\(d^{[1]}\)</span> with the same shape as <span class="math notranslate nohighlight">\(a^{[1]}\)</span> using <code class="docutils literal notranslate"><span class="pre">np.random.rand()</span></code> to randomly get numbers between 0 and 1. Here, you will use a vectorized implementation, so create a random matrix <span class="math notranslate nohighlight">\(D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] \)</span> of the same dimension as <span class="math notranslate nohighlight">\(A^{[1]}\)</span>.</p></li>
<li><p>Set each entry of <span class="math notranslate nohighlight">\(D^{[1]}\)</span> to be 1 with probability (<code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>), and 0 otherwise.</p></li>
</ol>
<p><strong>Hint:</strong> Let’s say that keep_prob = 0.8, which means that we want to keep about 80% of the neurons and drop out about 20% of them.  We want to generate a vector that has 1’s and 0’s, where about 80% of them are 1 and about 20% are 0.
This python statement:<br />
<code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">=</span> <span class="pre">(X</span> <span class="pre">&lt;</span> <span class="pre">keep_prob).astype(int)</span></code></p>
<p>is conceptually the same as this if-else statement (for the simple case of a one-dimensional array) :</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">keep_prob</span><span class="p">:</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span> <span class="c1"># v &gt;= keep_prob</span>
        <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">X</span> <span class="pre">=</span> <span class="pre">(X</span> <span class="pre">&lt;</span> <span class="pre">keep_prob).astype(int)</span></code> works with multi-dimensional arrays, and the resulting output preserves the dimensions of the input array.</p>
<p>Also note that without using <code class="docutils literal notranslate"><span class="pre">.astype(int)</span></code>, the result is an array of booleans <code class="docutils literal notranslate"><span class="pre">True</span></code> and <code class="docutils literal notranslate"><span class="pre">False</span></code>, which Python automatically converts to 1 and 0 if we multiply it with numbers.  (However, it’s better practice to convert data into the data type that we intend, so try using <code class="docutils literal notranslate"><span class="pre">.astype(int)</span></code>.)</p>
<ol class="arabic simple" start="3">
<li><p>Set <span class="math notranslate nohighlight">\(A^{[1]}\)</span> to <span class="math notranslate nohighlight">\(A^{[1]} * D^{[1]}\)</span>. (You are shutting down some neurons). You can think of <span class="math notranslate nohighlight">\(D^{[1]}\)</span> as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.</p></li>
<li><p>Divide <span class="math notranslate nohighlight">\(A^{[1]}\)</span> by <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: forward_propagation_with_dropout</span>

<span class="k">def</span> <span class="nf">forward_propagation_with_dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the forward propagation: LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; RELU + DROPOUT -&gt; LINEAR -&gt; SIGMOID.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input dataset, of shape (2, number of examples)</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, &quot;W2&quot;, &quot;b2&quot;, &quot;W3&quot;, &quot;b3&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (20, 2)</span>
<span class="sd">                    b1 -- bias vector of shape (20, 1)</span>
<span class="sd">                    W2 -- weight matrix of shape (3, 20)</span>
<span class="sd">                    b2 -- bias vector of shape (3, 1)</span>
<span class="sd">                    W3 -- weight matrix of shape (1, 3)</span>
<span class="sd">                    b3 -- bias vector of shape (1, 1)</span>
<span class="sd">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    A3 -- last activation value, output of the forward propagation, of shape (1,1)</span>
<span class="sd">    cache -- tuple, information stored for computing the backward propagation</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># retrieve parameters</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]</span>
    <span class="n">W3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W3&quot;</span><span class="p">]</span>
    <span class="n">b3</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b3&quot;</span><span class="p">]</span>
    
    <span class="c1"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>
    <span class="n">Z1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z1</span><span class="p">)</span>
    <span class="c1">#(≈ 4 lines of code)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span>
    <span class="c1"># D1 =                                           # Step 1: initialize matrix D1 = np.random.rand(..., ...)</span>
    <span class="c1"># D1 =                                           # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span>
    <span class="c1"># A1 =                                           # Step 3: shut down some neurons of A1</span>
    <span class="c1"># A1 =                                           # Step 4: scale the value of neurons that haven&#39;t been shut down</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">D1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">A1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">D1</span> <span class="o">=</span> <span class="p">(</span><span class="n">D1</span> <span class="o">&lt;</span> <span class="n">keep_prob</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">A1</span> <span class="o">=</span> <span class="n">A1</span> <span class="o">*</span> <span class="n">D1</span>
    <span class="n">A1</span> <span class="o">/=</span> <span class="n">keep_prob</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">Z2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">A1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z2</span><span class="p">)</span>
    <span class="c1">#(≈ 4 lines of code)</span>
    <span class="c1"># D2 =                                           # Step 1: initialize matrix D2 = np.random.rand(..., ...)</span>
    <span class="c1"># D2 =                                           # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span>
    <span class="c1"># A2 =                                           # Step 3: shut down some neurons of A2</span>
    <span class="c1"># A2 =                                           # Step 4: scale the value of neurons that haven&#39;t been shut down</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">D2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">A2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">A2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">D2</span> <span class="o">=</span> <span class="p">(</span><span class="n">D2</span> <span class="o">&lt;</span> <span class="n">keep_prob</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">A2</span> <span class="o">=</span> <span class="n">A2</span> <span class="o">*</span> <span class="n">D2</span>
    <span class="n">A2</span> <span class="o">/=</span> <span class="n">keep_prob</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">Z3</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">A2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span>
    <span class="n">A3</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z3</span><span class="p">)</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">D1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">D2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">Z3</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">A3</span><span class="p">,</span> <span class="n">cache</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_X</span><span class="p">,</span> <span class="n">parameters</span> <span class="o">=</span> <span class="n">forward_propagation_with_dropout_test_case</span><span class="p">()</span>

<span class="n">A3</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation_with_dropout</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;A3 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">A3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A3 = [[0.36974721 0.00305176 0.04565099 0.49683389 0.36974721]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="backward-propagation-with-dropout">
<h5>6.2 - Backward Propagation with Dropout<a class="headerlink" href="#backward-propagation-with-dropout" title="Link to this heading">#</a></h5>
</section>
<section id="exercise-4-backward-propagation-with-dropout">
<h5>Exercise 4 - backward_propagation_with_dropout<a class="headerlink" href="#exercise-4-backward-propagation-with-dropout" title="Link to this heading">#</a></h5>
<p>Implement the backward propagation with dropout. As before, you are training a 3 layer network. Add dropout to the first and second hidden layers, using the masks <span class="math notranslate nohighlight">\(D^{[1]}\)</span> and <span class="math notranslate nohighlight">\(D^{[2]}\)</span> stored in the cache.</p>
<p><strong>Instruction</strong>:
Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:</p>
<ol class="arabic simple">
<li><p>You had previously shut down some neurons during forward propagation, by applying a mask <span class="math notranslate nohighlight">\(D^{[1]}\)</span> to <code class="docutils literal notranslate"><span class="pre">A1</span></code>. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask <span class="math notranslate nohighlight">\(D^{[1]}\)</span> to <code class="docutils literal notranslate"><span class="pre">dA1</span></code>.</p></li>
<li><p>During forward propagation, you had divided <code class="docutils literal notranslate"><span class="pre">A1</span></code> by <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>. In backpropagation, you’ll therefore have to divide <code class="docutils literal notranslate"><span class="pre">dA1</span></code> by <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code> again (the calculus interpretation is that if <span class="math notranslate nohighlight">\(A^{[1]}\)</span> is scaled by <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>, then its derivative <span class="math notranslate nohighlight">\(dA^{[1]}\)</span> is also scaled by the same <code class="docutils literal notranslate"><span class="pre">keep_prob</span></code>).</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: backward_propagation_with_dropout</span>

<span class="k">def</span> <span class="nf">backward_propagation_with_dropout</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">keep_prob</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements the backward propagation of our baseline model to which we added dropout.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input dataset, of shape (2, number of examples)</span>
<span class="sd">    Y -- &quot;true&quot; labels vector, of shape (output size, number of examples)</span>
<span class="sd">    cache -- cache output from forward_propagation_with_dropout()</span>
<span class="sd">    keep_prob - probability of keeping a neuron active during drop-out, scalar</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">(</span><span class="n">Z1</span><span class="p">,</span> <span class="n">D1</span><span class="p">,</span> <span class="n">A1</span><span class="p">,</span> <span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">Z2</span><span class="p">,</span> <span class="n">D2</span><span class="p">,</span> <span class="n">A2</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">,</span> <span class="n">Z3</span><span class="p">,</span> <span class="n">A3</span><span class="p">,</span> <span class="n">W3</span><span class="p">,</span> <span class="n">b3</span><span class="p">)</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="n">dZ3</span> <span class="o">=</span> <span class="n">A3</span> <span class="o">-</span> <span class="n">Y</span>
    <span class="n">dW3</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ3</span><span class="p">,</span> <span class="n">A2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db3</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">dA2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ3</span><span class="p">)</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># dA2 =                # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span>
    <span class="c1"># dA2 =                # Step 2: Scale the value of neurons that haven&#39;t been shut down</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dA2</span> <span class="o">=</span> <span class="n">D2</span> <span class="o">*</span> <span class="n">dA2</span>
    <span class="n">dA2</span> <span class="o">/=</span> <span class="n">keep_prob</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">dZ2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A2</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">dW2</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">A1</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db2</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">dA1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ2</span><span class="p">)</span>
    <span class="c1">#(≈ 2 lines of code)</span>
    <span class="c1"># dA1 =                # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span>
    <span class="c1"># dA1 =                # Step 2: Scale the value of neurons that haven&#39;t been shut down</span>
    <span class="c1"># YOUR CODE STARTS HERE</span>
    <span class="n">dA1</span> <span class="o">=</span> <span class="n">D1</span> <span class="o">*</span> <span class="n">dA1</span>
    <span class="n">dA1</span> <span class="o">/=</span> <span class="n">keep_prob</span>
    
    <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="n">dZ1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dA1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="n">A1</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">dW1</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db1</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="n">gradients</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;dZ3&quot;</span><span class="p">:</span> <span class="n">dZ3</span><span class="p">,</span> <span class="s2">&quot;dW3&quot;</span><span class="p">:</span> <span class="n">dW3</span><span class="p">,</span> <span class="s2">&quot;db3&quot;</span><span class="p">:</span> <span class="n">db3</span><span class="p">,</span><span class="s2">&quot;dA2&quot;</span><span class="p">:</span> <span class="n">dA2</span><span class="p">,</span>
                 <span class="s2">&quot;dZ2&quot;</span><span class="p">:</span> <span class="n">dZ2</span><span class="p">,</span> <span class="s2">&quot;dW2&quot;</span><span class="p">:</span> <span class="n">dW2</span><span class="p">,</span> <span class="s2">&quot;db2&quot;</span><span class="p">:</span> <span class="n">db2</span><span class="p">,</span> <span class="s2">&quot;dA1&quot;</span><span class="p">:</span> <span class="n">dA1</span><span class="p">,</span> 
                 <span class="s2">&quot;dZ1&quot;</span><span class="p">:</span> <span class="n">dZ1</span><span class="p">,</span> <span class="s2">&quot;dW1&quot;</span><span class="p">:</span> <span class="n">dW1</span><span class="p">,</span> <span class="s2">&quot;db1&quot;</span><span class="p">:</span> <span class="n">db1</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">gradients</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">backward_propagation_with_dropout_test_case</span><span class="p">()</span>

<span class="n">gradients</span> <span class="o">=</span> <span class="n">backward_propagation_with_dropout</span><span class="p">(</span><span class="n">t_X</span><span class="p">,</span> <span class="n">t_Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">keep_prob</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dA1 = </span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;dA1&quot;</span><span class="p">]))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;dA2 = </span><span class="se">\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">gradients</span><span class="p">[</span><span class="s2">&quot;dA2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dA1 = 
[[ 0.36544439  0.         -0.00188233  0.         -0.17408748]
 [ 0.65515713  0.         -0.00337459  0.         -0.        ]]
dA2 = 
[[ 0.58180856  0.         -0.00299679  0.         -0.27715731]
 [ 0.          0.53159854 -0.          0.53159854 -0.34089673]
 [ 0.          0.         -0.00292733  0.         -0.        ]]
</pre></div>
</div>
</div>
</div>
<p>Let’s now run the model with dropout (<code class="docutils literal notranslate"><span class="pre">keep_prob</span> <span class="pre">=</span> <span class="pre">0.86</span></code>). It means at every iteration you shut down each neurons of layer 1 and 2 with 14% probability. The function <code class="docutils literal notranslate"><span class="pre">model()</span></code> will now call:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">forward_propagation_with_dropout</span></code> instead of <code class="docutils literal notranslate"><span class="pre">forward_propagation</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">backward_propagation_with_dropout</span></code> instead of <code class="docutils literal notranslate"><span class="pre">backward_propagation</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.86</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.6543912405149825
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 10000: 0.0610169865749056
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 20000: 0.060582435798513114
</pre></div>
</div>
<img alt="_images/ffd541a4e426c82aa8af99f3c74e74d34ef34601e22028a2368596046a1354cd.png" src="_images/ffd541a4e426c82aa8af99f3c74e74d34ef34601e22028a2368596046a1354cd.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.9289099526066351
On the test set:
Accuracy: 0.95
</pre></div>
</div>
</div>
</div>
<p>Dropout works great! The test accuracy has increased again (to 95%)! Your model is not overfitting the training set and does a great job on the test set. The French football team will be forever grateful to you!</p>
<p>Run the code below to plot the decision boundary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model with dropout&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.40</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.75</span><span class="p">,</span><span class="mf">0.65</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/01d1f60b30c6e96687569d7147cd3170a892d453923ff7fc7277c41f31d1084e.png" src="_images/01d1f60b30c6e96687569d7147cd3170a892d453923ff7fc7277c41f31d1084e.png" />
</div>
</div>
<p><strong>Note</strong>:</p>
<ul class="simple">
<li><p>A <strong>common mistake</strong> when using dropout is to use it both in training and testing. You should use dropout (randomly eliminate nodes) only in training.</p></li>
<li><p>Deep learning frameworks like <a class="reference external" href="https://www.tensorflow.org/api_docs/python/tf/nn/dropout">TensorFlow</a>, <a class="reference external" href="https://www.paddlepaddle.org.cn/documentation/docs/en/api/paddle/nn/Dropout_en.html#dropout">PaddlePaddle</a>, <a class="reference external" href="https://keras.io/api/layers/regularization_layers/dropout/">Keras</a> or <a class="reference external" href="https://caffe.berkeleyvision.org/doxygen/classcaffe_1_1DropoutLayer.html">caffe</a> come with a dropout layer implementation. Don’t stress - you will soon learn some of these frameworks.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">What you should remember about dropout:</p>
<ul class="simple">
<li><p>Dropout is a regularization technique.</p></li>
<li><p>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</p></li>
<li><p>Apply dropout both during forward and backward propagation.</p></li>
<li><p>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.</p></li>
</ul>
</div>
</section>
</section>
<section id="conclusions">
<h4>7 - Conclusions<a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h4>
<p><strong>Here are the results of our three models</strong>:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Train accuracy</p></th>
<th class="head"><p>Test accuracy</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3-layer NN without regularization</p></td>
<td><p>95%</p></td>
<td><p>91.5%</p></td>
</tr>
<tr class="row-odd"><td><p>3-layer NN with L2-regularization</p></td>
<td><p>94%</p></td>
<td><p>93%</p></td>
</tr>
<tr class="row-even"><td><p>3-layer NN with dropout</p></td>
<td><p>93%</p></td>
<td><p>95%</p></td>
</tr>
</tbody>
</table>
<p>Note that regularization hurts training set performance! This is because it limits the ability of the network to overfit to the training set. But since it ultimately gives better test accuracy, it is helping your system.</p>
<div class="warning admonition">
<p class="admonition-title">In summary</p>
<p>Here’s a quick recap of the main takeaways:</p>
<ul class="simple">
<li><p>Regularization will help you reduce overfitting.</p></li>
<li><p>Regularization will drive your weights to lower values.</p></li>
<li><p>L2 regularization and Dropout are two very effective regularization techniques.</p></li>
</ul>
</div>
</section>
</section>
</div>
</section>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C1">1 Introduction to Deep Learning</a></li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C2">2 Neural Networks Basics</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C2_Practical">Pre-Practical: Python Basics with Numpy (optional assignment)</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C2_Practical_Test">Practicel 1: Logistic Regression with a Neural Network mindset</a></li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C3">3 Shallow Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C3_Practical_Test">Practical 2: Planar data classification with one hidden layer</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#packages">1 - Packages</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#load-the-dataset">2 - Load the Dataset</a></li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C4">4 Deep Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C4_Practical_Test_P1">Practical 3: Building your Deep Neural Network: Step by Step</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C4_Practical_Test_P2">Practical 4: Deep Neural Network for Image Classification: Application</a></li>
</ul>
</li>
<li class="toctree-l1 toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5">5 Practical Aspects of Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5_Initialization">Practical 5: Initialization</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5_Regularization">Practical 6: Regularization</a></li>
<li class="toctree-l2 toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="intro.html#document-C5_Gradient_Checking">Practical 7: Gradient Checking</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>