---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.11.5
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
(ShallowNN)=
# 3 Shallow Neural Networks

Build a neural network with one hidden layer, using forward propagation and backpropagation.

**Learning Objectives**

- Describe hidden units and hidden layers.

- Use units with a non-linear activation function, such as tanh.

- Implement forward and backward propagation.

- Apply random initialization to your neural network.

- Increase fluency in Deep Learning notations and Neural Network Representations.

- Implement a 2-class classification neural network with a single hidden layer.

- Compute the cross entropy loss.

---

## Neural Networks Overview

[Video](https://youtu.be/f53vwY_wNKY)

Let's give a quick overview of how you implement a neural network. Last chapter, we had learned about logistic regression, and we saw how this model corresponds to the following computation draft.

```{figure} images/3-1.png
---
height: 280px
name: 3-1
---
```
Whereas previously, the node corresponds to two steps to calculations. The first is compute the $z$-value, second is it computes the $a$ value. In the neural network, the stack of nodes will correspond to a $z$-like calculation like this, as well as, an $a$-like calculation like that. Then, the second node will correspond to another $z$ and another $a$ like calculation.  

New notation that we'll introduce is that we'll use superscript square bracket one ($[1]$) to refer to quantities associated with the first stack of nodes, it's called a layer. Then later, we'll use superscript square bracket two ($[2]$) to refer to quantities associated with the second node. That's called another layer of the neural network. 

The superscript square brackets $[\ ]$, like we have here, are not to be confused with the superscript round brackets $(\ )$ which we use to refer to individual training examples. 
- $x$ superscript round bracket $i$ ($x^i$) refer to the $i$-th training example
- superscript square bracket one and two refer to these different layers; layer one and layer two in this neural network. 

```{figure} images/3-2.png
---
height: 380px
name: 3-2
---
```

This gives you a quick overview of what a neural network looks like. It's basically taken logistic regression and repeating it twice.


## Neural Network Representation

[Video](https://youtu.be/Gk_xaB2Hg7U)


```{figure} images/3-3.png
---
height: 450px
name: 3-3
---
```

In this example, $a^{[1]}$ is a four dimensional vector, in `Python` it is the $4 \times 1$ matrix, or a $4$ column vector, which shows in the picture. And it's four dimensional, because in this case we have four nodes, or four units, or four hidden units in this hidden layer.

The hidden layer will have associated with it parameters $w$ and $b$. 
- $W^{[1]}, b^{[1]}$, superscripts square bracket 1 to indicate that these are parameters associated with layer one with the hidden layer. 
- We'll see later that $W^{[1]}$ will be a $4 \times 3$ matrix and $b^{[1]}$ will be a $4 \times 1$ vector in this example. 
- Where the first coordinate four comes from the fact that we have four nodes of our hidden units and a layer, and three comes from the fact that we have three input features. 
- In some of the output layers has associated with it also, parameters $W^{[2]}$ and $b^{[2]}$. And it turns out the dimensions of these are $1 \times 4$ and $1 \times 1$. And this $1 \times 4$ is because the hidden layer has four hidden units, the output layer has just one unit. 

We'll talk about the dimensions of these matrices in next section. 


```{admonition} What the word **hidden** means?
:class: hint
In a neural network that you train with supervised learning, the training set contains values of the inputs $x$ as well as the target outputs $y$. So the term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is, you don't see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden layer; just because you don't see it in the training set.
```

## Computing a Neural Network's Output

[Video](https://youtu.be/cptdfvi5wK0)

In the last section, you learned what a single hidden layer neural network looks like. In this section, let's go through the details of exactly how this neural network computes these outputs. What you see is that it is like logistic regression, but repeated a lot of times.

```{figure} images/3-4.png
---
height: 360px
name: 3-4
---
```

Now, we've said before that logistic regression, the circle in logistic regression, really represents two steps of computation rows. You compute $z$ first and then compute the activation as a sigmoid function of $z$. So, a neural network just does this a lot more times, like shows in the left. Let's start by focusing on just one of the nodes in the hidden layer. Let's look at the first node in the hidden layer.

I've grayed out the other nodes for now. So, similar to logistic regression on the left, this nodes in the hidden layer does two steps of computation. 

```{figure} images/3-5.jpg
---
height: 380px
name: 3-5
---
```

The first step and think of as the left half of this node, it computes $z_1^{[1]} = w_1^{[1]T} x + b_1^{[1]}$, and the notation we'll use is, these are all quantities associated with the first hidden layer. So, that's why we have a bunch of square brackets there. This is the first node in the hidden layer. So, that's why we have the subscript one over there. So first, it does that, and then the second step, is it computes $a_1^{[1]} = \sigma (z_1^{[1]} )$, like so. So, for both $z$ and $a$, the notational convention is that the $[l]$ here in superscript square brackets, refers to the layer number, and the $[i]$ subscript here, refers to the nodes in that layer. So, the node we'll be looking at is **layer one**, that is a hidden layer **node one**. 

That little circle, that first node in the neural network, represents carrying out these two steps of computation. Now, let's look at the second node in the neural network, or the second node in the hidden layer of the neural network. Similar to the logistic regression unit on the left, this little circle represents two steps of computation. The first step is it computes $z$. This is still layer one, but now as a second node $z_2^{[1]} = w_2^{[1]T} x + b_2^{[1]}$. And then $a_2^{[1]} = \sigma (z_2^{[1]} )$.

So, we've talked through the first two hidden units in a neural network, having units three and four also represents some computations. So now, if you then go through and write out the corresponding equations for the third and fourth hidden units, you get the following. Now, if you're actually implementing a neural network, doing this with a `for` loop, seems really inefficient. So, what we're going to do, is take these four equations and vectorize. So, we're going to start by showing how to compute $z$ as a vector, it turns out you could do it as follows.

- stack $w_i^{[1]T}$ into a matrix. By stacking those four $w$ vectors together, you end up with a matrix. So, another way to think of this is that we have four logistic regression units there, and each of the logistic regression units, has a corresponding parameter vector, $w$. By stacking those four vectors together, you end up with this $4 \times 3$ matrix. This matrix here which we obtained by stacking the lowercase $w_1^{[1]}$ through $w_4^{[1]}$, we're going to call this matrix capital $W^{[1]}$.

- Computing vector $z^{[1]}$, which is taken by stacking up these individuals of $z$'s into a column vector. When we're vectorizing, one of the rules of thumb that might help you navigate this, is that while we have different nodes in the layer, we'll stack them vertically. So, that's why we have $z_1^{[1]}$ through $z_4^{[1]}$, those corresponded to four different nodes in the hidden layer, and so we stacked these four numbers vertically to form the vector $z^{[1]}$. 

- Computing vector $a^{[1]}$. So, prior won't surprise you to see that we're going to define $a^{[1]}$, as just stacking together, those activation values, $a_1^{[1]}$ through $a_4^{[1]}$. So, just take these four values and stack them together in a vector called $a^{[1]}$. This is going to be a sigmoid of $z^{[1]}$, where this now has been implementation of the sigmoid function that takes in the four elements of $z$, and applies the sigmoid function element-wise to it. 


```{figure} images/3-6.png
---
height: 460px
name: 3-6
---
```

To sumarrise the first layer of the neural network given an input $x$: 

```{figure} images/3-7.png
---
height: 300px
name: 3-7
---
```

Remember, that we said $x$ is equal to $a^{[0]}$. Just say $\hat{y}$ is also equal to $a^{[2]}$. If you want, you can actually take the $x$ in the first equation and replace it with $a^{[0]}$, since $a^{[0]}$ is if you want as an alias for the vector of input features, $x$.


For logistic regression, to implement the output or to implement prediction, you compute $\mathbf{z} = \mathbf{w}^T \mathbf{x} + \mathbf{b}$, and $\hat{y} = a = \sigma{(z)}$. When you have a neural network with one hidden layer, what you need to implement, is to computer this output is just the four equations shows in the figure on the left. 

You can think of this as a vectorized implementation of computing the output of first four logistic regression units in the hidden layer: 

$$
\begin{aligned}
\mathbf{z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{a}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{a}^{[1]} &= \sigma({\mathbf{z}^{[1]}})
\end{aligned}
$$ 

and then the logistic regression in the output layer which is what this does:

$$
\begin{aligned}
\mathbf{z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{a}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{a}^{[2]} &= \sigma({\mathbf{z}^{[2]}})
\end{aligned}
$$ 


## Vectorizing Across Multiple Examples

[Video](https://youtu.be/WSu85RCigi0)

In the last section, you have learned how to compute the prediction on a neural network, given a single training example. In this section, you will learn how to vectorize across multiple training examples. And the outcome will be quite similar to what you learned for logistic regression. Whereby stacking up different training examples in different columns of the matrix, you'd be able to take the equations you had from the previous section. And with very little modification, change them to make the neural network compute the outputs on all the examples on pretty much all at the same time. So let's see the details on how to do that.

The four equations we have from the previous section of how you compute $\mathbf{z}^{[1]}$, $\mathbf{a}^{[1]}$, $\mathbf{z}^{[2]}$ and $\mathbf{a}^{[2]}$. And they tell you how, given an input feature back to $\mathbf{x}$, you can use them to generate $\mathbf{a}^{[2]} = \hat{\mathbf{y}}$ hat for a single training example.

$$\mathbf{x} \longrightarrow \mathbf{a}^{[2]} = \hat{\mathbf{y}}$$ 

Now if you have $m$ training examples, you need to repeat this process for:

$$
\begin{aligned}
\mathbf{x}^{(1)} &\longrightarrow \mathbf{a}^{[2](1)} = \hat{\mathbf{y}}^{(1)} \\
\mathbf{x}^{(2)} &\longrightarrow \mathbf{a}^{[2](2)} = \hat{\mathbf{y}}^{(2)}  \\
\vdots \\
\mathbf{x}^{(m)} &\longrightarrow \mathbf{a}^{[2](m)} = \hat{\mathbf{y}}^{(m)}
\end{aligned}
$$ 

So the notation $\mathbf{a}^{[2](i)}$:

- the round bracket $i$, $(i)$, refers to training example $i$
- the square bracket 2, $[2]$, refers to layer 2.

And so to suggest that if you have an unvectorized implementation and want to compute the predictions of all your training examples, you need to do a `for` loop from `i = 1 to m`. Then basically implement these four equations. What we like to do is vectorize this whole computation, so as to get rid of this `for` loop.


```{figure} images/3-8.png
---
height: 380px
name: 3-8
---
```

They are also obtained by taking these vectors and stacking them horizontally. And taking these vectors and stacking them horizontally, in order to get $\mathbf{Z}^{[2]}$, and $\mathbf{A}^{[2]}$. One of the property of this notation that might help you to think about it is that this matrixes say $\mathbf{Z}$, and $\mathbf{A}$, horizontally we're going to index across training examples. So that's why the horizontal index corresponds to different training example, when you sweep from left to right you're scanning through the training cells. And vertically this vertical index corresponds to different nodes in the neural network.

-  horizontally the matrix $\mathbf{A}$ goes over different training examples.

-  vertically the different indices in the matrix $\mathbf{A}$ corresponds to different hidden units. As you scan down this is your indexing to the hidden units number.

- $\mathbf{X}$ where horizontally corresponds to different training examples. And vertically it corresponds to different input features which are really different than those of the input layer of the neural network.



## Explanation for Vectorized Implementation

[Video](https://youtu.be/PzpiZKWtyRc)


```{figure} images/3-9.png
---
height: 380px
name: 3-9
---
```

I hope this gives a justification for why we had previously $\mathbf{W}^{[1]} \mathbf{x}^{(i)} = \mathbf{z}^{[1](i)} $ when we're looking at single training example at the time. When you took the different training examples and stacked them up in different columns, then the corresponding result is that you end up with the $\mathbf{z}$'s also stacked at the columns. 

So in this section, I've only justified that $\mathbf{Z}^{[1]} = \mathbf{W}^{[1]} \mathbf{X} + \mathbf{b}^{[1]}$ is a correct vectorization of the first step of the four steps we have in the previous section, but it turns out that a similar analysis allows you to show that the other steps also work on using a very similar logic where if you stack the inputs in columns then after the equation, you get the corresponding outputs also stacked up in columns. 


```{figure} images/3-10.png
---
height: 380px
name: 3-10
---
```

```{admonition} Reminder
:class: hint
Because $\mathbf{X}$ is also equal to $\mathbf{A}^{[0]}$, remember that the input feature vector $\mathbf{x}$ was equal to $\mathbf{a}^{[0]}$, so $\mathbf{x}^{(i)}$ equals \mathbf{a}^{[0](i)}. Then there's actually a certain symmetry to these equations where this first equation can also be written  $\mathbf{Z}^{[1]} = \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]}$. 

And so, you see that the first pair of equations and the second pair of equations actually look very similar but just of all of the indices advance by one. So this kind of shows that the different layers of a neural network are roughly doing the same thing or just doing the same computation over and over. And here we have two-layer neural network where we go to a much deeper neural network in next chapter. You see that even deeper neural networks are basically taking these two steps and just doing them even more times than you're seeing here. So that's how you can vectorize your neural network across multiple training examples. 
```

## Activation Functions

[Video](https://youtu.be/pFa8lh3Rqmo)

1. **sigmoid function**

$$g(z) = \dfrac{1}{1+e^{-z}}$$

- the sigmoid function goes between zero and one.

- I would say never use this except for the output layer if you're doing binary classification or maybe almost never use this. And the reason I almost never use this is because the tanh is pretty much strictly superior. 

2. **tanh function** (tangent function / hyperbolic tangent function)

$$g(z) = \dfrac{e^z - e^{-z}}{e^z + e^{-z}}$$

- the tanh function goes between $(-1, 1)$.

- This almost always works better than the sigmoid function because with values between plus one and minus one, the mean of the activations that come out of your hidden layer are closer to having a zero mean. And so just as sometimes when you train a learning algorithm, you might center the data and your data have zero mean using a **tanh** instead of a sigmoid function. Kind of has the effect of centering your data so that the mean of your data is close to 0 rather than 0.5. And this actually makes learning for the next layer a little bit easier.

- The one ***exception*** is for the **output layer** because if $y$ is either zero or one ($y \in (0,1)$), then it makes sense for $\hat{y}$ to be a number that you want to output that's between zero and one rather than between -1 and 1. So the one exception where I would use the sigmoid activation function is when you're using binary classification. In which case you might use the sigmoid activation function for the upper layer.

- One of the downsides of both the sigmoid function and the tanh function is that if $z$ is either very large or very small, then the gradient of the derivative of the slope of this function becomes very small. So if $z$ is very large or $z$ is very small, the slope of the function either ends up being close to zero and so this can slow down gradient descent. So one other choice that is very popular in machine learning is what's called the rectified linear unit. 

3. **ReLU function** (Rectified Linear unit)

$$g(z) = \text{max}(0, z)$$

- So the derivative is one so long as $z$ is positive and derivative or the slope is zero when $z$ is negative. If you're implementing this, technically the derivative when $z$ is exactly zero is not well defined. But when you implement this in the computer, the odds that you get exactly $z$ equals $0.000000000000000$ is very small. So you don't need to worry about it. In practice, you could pretend a derivative when $z$ is equal to zero, you can pretend is either one or zero. And you can work just fine. So the fact is not differentiable.

- One disadvantage of the ReLU is that the derivative is equal to zero when $z$ is negative. In practice this works just fine. But there is another version of the value called the Leaky ReLU.

4. **Leaky ReLU function**

$$g(z) = \text{max}(0.01z, z)$$

- instead of it being zero when $z$ is negative, it just takes a slight slope.

- And you might say, why is that constant $0.01$? Well, you can also make that an another parameter of the learning algorithm. And some people say that works even better, but how they see people do that. So, if you feel like trying it in your application, please feel free to do so. And you can just see how it works and how well it works, and stick with it if it gives you a good result. 

- This usually works better than the ReLU activation function. Although, it's just not used as much in practice. Either one should be fine. Although, if you had to pick one, I usually just use the ReLU.

- The advantage of both the ReLU and the Leaky ReLU is that for a lot of the space of $z$, the derivative of the activation function, the slope of the activation function is very different from zero. And so in practice, using the value activation function, your neural network will often learn much faster than when using the tanh or the sigmoid activation function. And the main reason is that there is less of this effect of the slope of the function going to zero, which slows down learning. And I know that for half of the range of $z$ in ReLU, the slope for value is zero. But in practice, enough of your hidden units will have $z$ greater than zero. So learning can still be quite fast for most training examples.


```{figure} images/3-11.png
---
height: 360px
name: 3-11
---
```

- The activation functions can be different for different layers. And sometimes to denote that the activation functions are different for different layers, we might use these square brackets superscripts as well to indicate that $g^{[1]}(z)$ may be different than $g^{[2]}(z)$. 


```{admonition} ***Rules of thumb for choosing activation functions***
:class: hint
If your output is zero one value, if you're using binary classification, then the sigmoid activation function is very natural choice for the output layer. And then for all other units, the ReLU or the rectified linear unit is increasingly the default choice of activation function. So if you're not sure what to use for your hidden layer, I would just use the ReLU activation function, is what you see most people using these days. Although sometimes people also use the tanh activation function. 

One of the things we will see in deep learning is that you often have a lot of different choices in how you build your neural network. Ranging from a number of hidden units to the choices activation function, to how you initialize the ways. A lot of choices like that. And it turns out that it is sometimes difficult to get good guidelines for exactly what will work best for your problem. So throughout these courses, I'll keep on giving you a sense of what I see in the industry in terms of what's more or less popular. But for your application, with your applications, idiosyncrasies is actually very difficult to know in advance exactly what will work best. So common piece of advice would be, if you're not sure which one of these activation functions work best, try them all. And evaluate on a holdout validation set or a development set. And see which one works better and then go of that. And I think that by testing these different choices for your application, you would be better at future proofing your neural network architecture against the idiosyncracies problems. As well as evolutions of the algorithms rather than, if I were to tell you always use a value activation and don't use anything else. That just may or may not apply for whatever problem you end up working on. 
```



## Why do you need Non-Linear Activation Functions?

[Video](https://youtu.be/cJmupX30PCs)



## Derivatives of Activation Functions

[Video](https://youtu.be/WLgjYfNifGk)

1. **sigmoid function**

$$a = g(z) = \dfrac{1}{1+e^{-z}}$$

$$
\begin{aligned}
g'(z) 
&= \dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \\
&= \dfrac{-e^{-z}}{(1+e^{-z})^2} \\
&= \dfrac{1}{1+e^{-z}} \cdot \dfrac{-e^{-z}}{1+e^{-z}} \\
&= \dfrac{1}{1+e^{-z}} \cdot \Big( 1 - \dfrac{1}{1+e^{-z}} \Big) \\
&= g(z) \Big(1-g(z) \Big) \\
&= a(1-a)
\end{aligned}
$$ 

- If $z = 10$, $g(z) \approx 1$, $\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 1\times(1-1) \approx 0$
- If $z = -10$, $g(z) \approx 0$, $\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 0\times(1-0) \approx 0$
- If $z = 0$, $g(z) = \dfrac{1}{2}$, $\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) = \dfrac{1}{2}\times(1-\dfrac{1}{2}) = \dfrac{1}{4}$

2. **tanh function**

$$a = g(z) = \dfrac{e^z - e^{-z}}{e^z + e^{-z}}$$

$$
\begin{aligned}
g'(z) 
&= \dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \\
&= \dfrac{(e^z + e^{-z})^2 - (e^z - e^{-z})^2}{(e^z + e^{-z})^2} \\
&= 1- \dfrac{(e^z - e^{-z})^2}{(e^z + e^{-z})^2} \\
&= 1- \Big(g(z) \Big)^2 \\
&= 1-a^2
\end{aligned}
$$ 

- If $z = 10$, $g(z) \approx 1$, $\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 0$
- If $z = -10$, $g(z) \approx -1$, $\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) \approx 0$
- If $z = 0$, $g(z) = 0$, $\dfrac{\mathrm{d}}{\mathrm{d}z}g(z) = 1$

3. **ReLU function** and **Leaky ReLU function**

$$g(z) = \text{max}(0, z)$$

$$
g'(z) =
\begin{cases}
0,\quad \mbox{if} z < 0\\
1,\quad \mbox{if} z \geq 0
\end{cases}
$$

$$g(z) = \text{max}(0.01z, z)$$

$$
g'(z) =
\begin{cases}
0.01,\quad &\mbox{if} z < 0\\
1,\quad &\mbox{if} z \geq 0
\end{cases}
$$

## Gradient Descent for Neural Networks

[Video](https://youtu.be/CT21tIhFP-E)


Parameters: $\mathbf{W}^{[1]}, \ \mathbf{b}^{[1]}, \ \mathbf{W}^{[2]}, \ \mathbf{b}^{[2]}$

Dimensions: $n_x = n^{[0]}$, $n^{[1]}$, $n^{[2]}$; 

- $\mathbf{W}^{[1]}$: $(n^{[1]}, n^{[0]})$
- $\mathbf{b}^{[1]}$: $(n^{[1]}, 1)$
- $\mathbf{W}^{[2]}$: $(n^{[2]}, n^{[1]})$
- $\mathbf{b}^{[2]}$: $(n^{[2]}, 1)$

Cost function: $\boldsymbol{J}(\mathbf{W}^{[1]}, \ \mathbf{b}^{[1]}, \ \mathbf{W}^{[2]}, \ \mathbf{b}^{[2]}) = \dfrac{1}{m} \sum_{i=1}^{m}  \mathcal{L}(\hat{y}, y) $



**Forward propogation**:

$$
\begin{aligned}
\mathbf{Z}^{[1]} &= \mathbf{W}^{[1]} \mathbf{A}^{[0]} + \mathbf{b}^{[1]} \\
\mathbf{A}^{[1]} &= g^{[1]}({\mathbf{Z}^{[1]}}) \\
\mathbf{Z}^{[2]} &= \mathbf{W}^{[2]} \mathbf{A}^{[1]} + \mathbf{b}^{[2]} \\
\mathbf{A}^{[2]} &= g^{[2]}({\mathbf{Z}^{[2]}}) = \sigma({\mathbf{Z}^{[2]}})
\end{aligned}
$$ 


**Backward propogation**:

```{figure} images/3-12.png
---
height: 300px
name: 3-12
---
```




## Backpropagation Intuition (Optional)

[Video](https://youtu.be/MXLJQ0qIRxM)




## Random Initialization

[Video](https://youtu.be/BSAWBJRTjuM)


When you change your neural network, it's important to initialize the weights randomly. For logistic regression, it was okay to initialize the weights to zero. But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent, it won't work. 


```python
W1 = np.random.randn(2,2) * 0.01
b1 = np.zeros((2,1))
W2 = np.random.randn(2,2) * 0.01
b2 = np.zeros((2,1))
```
```{admonition} Where did the constant $0.01$ comes from and why is it? Why not put the number $100$ or $1000$? 
:class: hint

If $\mathbf{W}$ is too large, you're more likely to end up even at the very start of training, with very large values of $\mathbf{z}$. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. 

- Turns out that we usually prefer to initialize the weights to very small random values. Because if you are using a tanh or sigmoid activation function, or the other sigmoid, even just at the output layer. If the weights are too large, then when you compute the activation values, remember that $\mathbf{z}^{[1]} = \mathbf{W}^{[1]} \mathbf{a}^{[0]} + \mathbf{b}^{[1]}$. And then $\mathbf{a}^{[1]} $ is the activation function applied to $\mathbf{z}^{[1]}$. So if $\mathbf{W}$ is very big, $\mathbf{z}$ will be very, or at least some values of $\mathbf{z}$ will be either very large or very small. And so in that case, you're more likely to end up at these **flat parts** of the tanh function or the sigmoid function, where the slope or the gradient is very small. Meaning that gradient descent will be very slow. So learning was very slow. 

If you don't have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue. But if you're doing binary classification, and your output unit is a sigmoid function, then you just don't want the initial parameters to be too large. So that's why multiplying by 0.01 would be something reasonable to try, or any other small number. 

```{figure} images/3-13.png
---
height: 210px
name: 3-13
---
```


When you're training a neural network with just one hidden layer, it is a relatively shallow neural network, without too many hidden layers. Set it to 0.01 will probably work okay. But when you're training a very very deep neural network, then you might want to pick a different constant than 0.01. And in next chapter material, we'll talk a little bit about how and when you might want to choose a different constant than 0.01. But either way, it will usually end up being a relatively small number.




## Quiz

1. Which of the following are true? (Check all that apply.)

    A. $W^{[1]}$ is a matrix with rows equal to the transpose of the parameter vectors of the first layer.

    B. $W^{[1]}$ is a matrix with rows equal to the parameter vectors of the first layer.

    C. $W_1$ is a matrix with rows equal to the parameter vectors of the first layer.

    D. $w^{[4]}_3$ is the column vector of parameters of the fourth layer and third neuron.
    
    E. $w^{[4]}_3$ is the column vector of parameters of the third layer and fourth neuron.
    
    F. $w^{[4]}_3$ is the row vector of parameters of the fourth layer and third neuron.


2.  **True/False** The sigmoid function is only mentioned as an activation function for historical reasons. The tanh is always preferred without exceptions in all the layers of a Neural Network.   __________

  
3.  Which of these is a correct vectorized implementation of forward propagation for layer $l$, where $1 \le l \le L$?

    A. $\begin{aligned}
Z^{[l]} &= W^{[l-1]}A^{[l]} + b^{[l-1]} \\
A^{[l]} &= g^{[l]}(Z^{[l]})
\end{aligned}$

    B. $\begin{aligned}
Z^{[l]} &= W^{[l]}A^{[l-1]} + b^{[l]} \\
A^{[l]} &= g^{[l]}(Z^{[l]})
\end{aligned}$

    C. $\begin{aligned}
Z^{[l]} &= W^{[l]}A^{[l]} + b^{[l]} \\
A^{[l+1]} &= g^{[l]}(Z^{[l]})
\end{aligned}$

    D. $\begin{aligned}
Z^{[l]} &= W^{[l]}A^{[l]} + b^{[l]} \\
A^{[l+1]} &= g^{[l+1]}(Z^{[l]})
\end{aligned}$

4. **True/False** The use of the ReLU activation function is becoming more rare because the ReLU function has no derivative for $c = 0$.  __________

5.  Consider the following code:
    ```python
    x = np.random.randn(4, 5) 
    y = np.sum(x, axis =1 ) 
    ```
    What will be the $y.shape$?

    A. `(4, 1)`

    B. `(4, )`

    C. `(1, 5)`

    D. `(5, )`

6.  Suppose you have built a neural network with one hidden layer and tanh as activation function for the hidden layers. Which of the following is a best option to initialize the weights?

    A. Initialize all weights to a single number chosen randomly.

    B. Initialize all weights to 0.

    C. Initialize the weights to large random numbers.

    D. Initialize the weights to small random numbers.

7.  **True/False** A single output and single layer neural network that uses the sigmoid function as activation is equivalent to the logistic regression.   __________
 
8.  You have built a network using the tanh activation for all the hidden units. You initialize the weights to relatively large values, using `np.random.randn(.., ..)*1000`. What will happen?

    A. This will cause the inputs of the tanh to also be very large, thus causing gradients to be close to zero. The optimization algorithm will thus become slow.

    B. This will cause the inputs of the tanh to also be very large, causing the units to be “highly activated” and thus speed up learning compared to if the weights had to start from small values.

    C. So long as you initialize the weights randomly gradient descent is not affected by whether the weights are large or small.

    D. This will cause the inputs of the tanh to also be very large, thus causing gradients to also become large. You therefore have to set $\alpha$ to a very small value to prevent divergence; this will slow down learning.
    
9. Consider the following 1 hidden layer neural network:
    
    ```{figure} images/3-q9.png
    ---
    height: 200px
    name: 3-q9
    ---
    ```
    
    Which of the following statements are True? (Check all that apply).
    
    A. $b^{[2]}$ will have shape $(4,1)$

    B. $b^{[1]}$ will have shape $(2,1)$

    C. $b^{[1]}$ will have shape $(4,1)$

    D. $W^{[1]}$ will have shape $(2,4)$  
    
    E. $W^{[2]}$ will have shape $(1,4)$  
    
    F. $W^{[1]}$ will have shape $(4,2)$ 
    
    G. $b^{[2]}$ will have shape $(1,1)$
    
    H. $W^{[2]}$ will have shape $(4,1)$ 
    
    
10. Consider the following 1 hidden layer neural network:
      ```{figure} images/3-q10.png
      ---
      height: 460px
      name: 3-q10
      ---
      ```    

      What are the dimensions of $Z^{[1]}$  and $A^{[1]}$?

      A. $Z^{[1]}$ and $A^{[1]}$ are $(2,m)$

      B. $Z^{[1]}$ and $A^{[1]}$ are $(4,1)$

      C. $Z^{[1]}$ and $A^{[1]}$ are $(4,m)$

      D. $Z^{[1]}$ and $A^{[1]}$ are $(2,1)$ 



:::{admonition} Click here for answers!
:class: tip, dropdown

1. AD </br>

    A. We construct $W^{[1]}$ stacking the parameter vectors $w^{[1]_j}$ of all the neurons of the first layer. </br>
    
    D. The vector $w^{[i]_j}$ is the column vector of parameters of the i-th layer and j-th neuron of that layer. </br>

2. False </br>

    Although the tanh almost always works better than the sigmoid function when used in hidden layers, thus is always proffered as activation function, the exception is for the output layer in classification problems. </br>

3. B </br>

4. False </br>

    Although the ReLU function has no derivative at $c = 0$ this rarely causes any problems in practice. Moreover it has become the default activation function in many cases, as explained in the lectures.  </br>

5. B </br>

    B. Yes. By using `axis=1` the sum is computed over each row of the array, thus the resulting array is a column vector with 4 entries. Since the option `keepdims` was not used the array doesn't keep the second dimension. </br>
    
6. D </br>

    D. The use of random numbers helps to "break the symmetry" between all the neurons allowing them to compute different functions. When using small random numbers the values $z^{[k]}$ will be close to zero thus the activation values will have a larger gradient speeding up the training process. </br>
    
7. True </br>

    The logistic regression model can be expressed by $\hat{y} = \sigma(Wx+b)$. This isthe same as $a^{[1]}=\sigma(W^{[1]}X+b)$. </br>

8. A </br>

    A. tanh becomes flat for large values; this leads its gradient to be close to zero. This slows down the optimization algorithm. </br>
    
9. CEFG </br>

10. A </br>

    A. Yes. The $Z^{[1]}$ and $A^{[1]}$ are calculated over a batch of training examples. The number of columns in $Z^{[1]}$  and $A^{[1]}$ is equal to the number of examples in the batch, $m$. And the number of rows in $Z^{[1]}$  and $A^{[1]}$ is equal to the number of neurons in the first layer. </br>

:::