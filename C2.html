
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2 Neural Networks Basics &#8212; Deep Learning Specialization</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'C2';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pre-Practical: Python Basics with Numpy (optional assignment)" href="C2_Practical.html" />
    <link rel="prev" title="1 Introduction to Deep Learning" href="C1.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Deep Learning Specialization - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Deep Learning Specialization - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Deep Learning Specialization
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="C1.html">1 Introduction to Deep Learning</a></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">2 Neural Networks Basics</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C2_Practical.html">Pre-Practical: Python Basics with Numpy (optional assignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="C2_Practical_Test.html">Practicel 1: Logistic Regression with a Neural Network mindset</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C3.html">3 Shallow Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C3_Practical_Test.html">Practical 2: Planar data classification with one hidden layer</a></li>


</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C4.html">4 Deep Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C4_Practical_Test_P1.html">Practical 3: Building your Deep Neural Network: Step by Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="C4_Practical_Test_P2.html">Practical 4: Deep Neural Network for Image Classification: Application</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C5.html">5 Practical Aspects of Deep Learning</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C5_Initialization.html">Practical 5: Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="C5_Regularization.html">Practical 6: Regularization</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FC2.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/C2.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>2 Neural Networks Basics</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regreesion-as-a-neural-network">Logistic Regreesion as a Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification">Binary Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-cost-function">Logistic Regression Cost Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-logisitic-regression-cost-function-optional">Explanation of Logisitic Regression Cost Function (Optional)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#logisitc-regression-cost-function">Logisitc regression cost function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-on-m-example">Cost on m example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives">Derivatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-derivative-examples">More Derivative Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graph">Computation Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-gradient-descent">Logistic Regression Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-on-m-examples">Gradient Descent on m Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-and-vectoraization">Python and Vectoraization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorization">Vectorization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-vectorization-examples">More Vectorization Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-and-matrix-valued-functions">Vectors and matrix valued functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-implementation">Gradient descent implementation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorizting-logistic-regression">Vectorizting Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorizing-logistic-regresion-s-gradient-output">Vectorizing Logistic Regresion’s Gradient Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting-in-python">Broadcasting in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting-examples">Broadcasting examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#general-principle">General Principle</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-note-on-python-numpy-vectors">A Note on Python/Numpy Vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quiz">Quiz</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="neural-networks-basics">
<span id="nnb"></span><h1>2 Neural Networks Basics<a class="headerlink" href="#neural-networks-basics" title="Link to this heading">#</a></h1>
<p>Set up a machine learning problem with a neural network mindset and use vectorization to speed up your models.</p>
<p><strong>Learning Objectives</strong></p>
<ul class="simple">
<li><p>Build a logistic regression model structured as a shallow neural network.</p></li>
<li><p>Build the general architecture of a learning algorithm, including parameter initialization, cost function and gradient calculation, and optimization implementation (gradient descent).</p></li>
<li><p>Implement computationally efficient and highly vectorized versions of models.</p></li>
<li><p>Compute derivatives for logistic regression, using a backpropagation mindset.</p></li>
<li><p>Use Numpy functions and <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> matrix/vector operations.</p></li>
<li><p>Work with <code class="docutils literal notranslate"><span class="pre">iPython</span></code> Notebooks.</p></li>
<li><p>Implement vectorization across multiple training examples.</p></li>
<li><p>Explain the concept of broadcasting.</p></li>
</ul>
<hr class="docutils" />
<section id="logistic-regreesion-as-a-neural-network">
<h2>Logistic Regreesion as a Neural Network<a class="headerlink" href="#logistic-regreesion-as-a-neural-network" title="Link to this heading">#</a></h2>
<section id="binary-classification">
<h3>Binary Classification<a class="headerlink" href="#binary-classification" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/kbrSXl43iPM">Video</a></p>
<p>Logistic regression is an algorithm for binary classification. So let’s start by setting up the problem.</p>
<p>Here’s an example of a binary classification problem. You might have an input of an image, and want to output a label to recognize this image as either being a cat, in which case you output 1, or not-cat in which case you output 0, and we’re going to use <span class="math notranslate nohighlight">\(y\)</span> to denote the output label. Let’s look at how an image is represented in a computer.</p>
<p>To store an image your computer stores three separate matrices corresponding to the red, green, and blue color channels of this image.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="_images/2-1.png"><img alt="_images/2-1.png" src="_images/2-1.png" style="height: 300px;" /></a>
</figure>
<p>So if your input image is 64 pixels by 64 pixels (<span class="math notranslate nohighlight">\(64\times64\)</span>), then you would have <strong>three</strong> 64 by 64 matrices corresponding to the red, green and blue pixel intensity values for your images, which can be presented as <span class="math notranslate nohighlight">\(64\times64\times3\)</span>. Although to make this as an small example here, I drew these as much smaller matrices, so these are actually 5 by 4 matrices rather than 64 by 64 (<span class="math notranslate nohighlight">\(5\times4\times3\)</span>).</p>
<p>So to turn these pixel intensity values into a feature vector, what we’re going to do is <em><strong>unroll</strong></em> all of these pixel values into an input feature vector <span class="math notranslate nohighlight">\(x\)</span>. So to unroll all these pixel intensity values into a feature vector, what we’re going to do is define a feature vector <span class="math notranslate nohighlight">\(x\)</span> corresponding to this image as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
 x = \left[
\begin{matrix}
255 \\
231 \\
42 \\
\vdots \\
124 \\
255 \\
134 \\
202 \\
\vdots \\
94 \\
255 \\
134 \\
93 \\
\vdots \\
142
\end{matrix}
\right], \quad y = 1
\end{split}\]</div>
<ul class="simple">
<li><p>We’re just going to take all the pixel values 255, 231, and so on until we’ve listed all the red pixels.</p></li>
<li><p>And then eventually 255, 134 and so on until we get a long feature vector listing out all the red, green and blue pixel intensity values of this image.</p></li>
</ul>
<p>If this image is a 64 by 64 image, the total dimension of this vector <span class="math notranslate nohighlight">\(x\)</span> will be <span class="math notranslate nohighlight">\(64\times64\times3\)</span> because that’s the total numbers we have in all of these matrixes. Which in this case, turns out to be 12,288, that’s what you get if you multiply all those numbers. Using <span class="math notranslate nohighlight">\(n_x = 12,288\)</span> to represent the dimension of the input features <span class="math notranslate nohighlight">\(x\)</span>. And sometimes for brevity, I will also just use lowercase <span class="math notranslate nohighlight">\(n\)</span> to represent the dimension of this input feature vector.</p>
<p>So in binary classification, our goal is to learn a classifier that can input an image represented by this feature vector <span class="math notranslate nohighlight">\(x\)</span>. And predict whether the corresponding label <span class="math notranslate nohighlight">\(y\)</span> is 1 or 0, that is, whether this is a cat image or a non-cat image.</p>
<p>Let’s now lay out some of the notation that we’ll use throughout the rest of this book.</p>
<ul class="simple">
<li><p>A single training example is represented by a pair <span class="math notranslate nohighlight">\((x,y)\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is an <span class="math notranslate nohighlight">\(x\)</span>-dimensional feature vector <span class="math notranslate nohighlight">\((x \in  \mathbb{R}^{n_x})\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, the label, is either 0 or 1 <span class="math notranslate nohighlight">\((y \in \{0,1\})\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(m\)</span> training examples:  <span class="math notranslate nohighlight">\(\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)}) \}\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(m_{\text{train}}\)</span> denotes the number of training samples; <span class="math notranslate nohighlight">\(m_{\text{test}}\)</span> denotes the number of test samples.</p></li>
<li><p><span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n_x \times m}\)</span> is the input matrix. In python, <code class="docutils literal notranslate"><span class="pre">X.shape</span></code> is <span class="math notranslate nohighlight">\((n_x,m)\)</span>. <span class="math notranslate nohighlight">\(n_x\)</span> rows and m columns.</p></li>
<li><p><span class="math notranslate nohighlight">\(x^{(i)} \in \mathbb{R}^{n_x}\)</span> is the <span class="math notranslate nohighlight">\(i^{th}\)</span> example represented as a column vector.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}X= \left[
\begin{matrix}
\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \\
\vdots &amp; \vdots &amp; \cdots &amp; \vdots 
\end{matrix}
\right]
\end{split}\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y \in \mathbb{R}^{1 \times m}\)</span> is the label matrix. In python, <code class="docutils literal notranslate"><span class="pre">Y.shape</span></code> is <span class="math notranslate nohighlight">\((1,m)\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[Y=[y^{(1)}, y^{(2)}, \cdots, y^{(m)}]\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(y^{(i)}\)</span> is the output label for the <span class="math notranslate nohighlight">\(i^{th}\)</span> example.</p></li>
</ul>
</section>
<section id="logistic-regression">
<h3>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">#</a></h3>
<p>Logistic regression is a learning algorithm used in a supervised learning problem when the output <span class="math notranslate nohighlight">\(y\)</span> are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data.</p>
<p><a class="reference external" href="https://youtu.be/4u0_TJhNGY8">Video</a></p>
<p>In this section, we will go over logistic regression. This is a learning algorithm that you use when the output labels <span class="math notranslate nohighlight">\(Y\)</span> in a supervised learning problem are all either zero or one, so for binary classification problems.</p>
<p>Example: Cat vs No-cat</p>
<ul class="simple">
<li><p>Given an input feature vector <span class="math notranslate nohighlight">\(x\)</span> maybe corresponding to an image that you want to recognize as either a cat picture or not a cat picture, you want an algorithm that can output a prediction, which we will call y hat <span class="math notranslate nohighlight">\((\hat{y})\)</span>, which is your estimate of <span class="math notranslate nohighlight">\(y\)</span>.  More formally, you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be the <em><strong>probability of the chance</strong></em> that, <span class="math notranslate nohighlight">\(\hat{y} = \mathrm{P}(y=1\mid x)\)</span> (<span class="math notranslate nohighlight">\(y\)</span> is equal to one given the input features <span class="math notranslate nohighlight">\(x\)</span>).  So in other words, if <span class="math notranslate nohighlight">\(x\)</span> is a picture, you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to tell you, what is the chance that this is a cat picture.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Given } x, \quad \hat{y} = \mathrm{P}(y=1\mid x), \quad \text{where } 0 \leq \hat{y} \leq 1\]</div>
<p>The parameters used in Logistic regression are:</p>
<ul class="simple">
<li><p>The input features vector: <span class="math notranslate nohighlight">\(x \in \mathbb{R}^{n_x}\)</span>, where <span class="math notranslate nohighlight">\(n_x\)</span> is the number of features. That is, <span class="math notranslate nohighlight">\(x\)</span> is an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector.</p></li>
<li><p>The training label: <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span>.</p></li>
<li><p>The weights: <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}\)</span>. <span class="math notranslate nohighlight">\(w\)</span> is also an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector.</p></li>
<li><p>The threshold: <span class="math notranslate nohighlight">\(b \in \mathbb{R}\)</span></p></li>
<li><p>Summary: given that the parameters of logistic regression will be <span class="math notranslate nohighlight">\(w\)</span> which is also an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector <span class="math notranslate nohighlight">\((w \in \mathbb{R}^{n_x})\)</span>, together with <span class="math notranslate nohighlight">\(b\)</span> which is just a real number <span class="math notranslate nohighlight">\((b \in \mathbb{R})\)</span>.</p></li>
<li><p>So given an input <span class="math notranslate nohighlight">\(x\)</span> and the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, how do we generate the output <span class="math notranslate nohighlight">\(\hat{y}\)</span>?</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-2-1">
<span class="eqno">(1)<a class="headerlink" href="#equation-2-1" title="Link to this equation">#</a></span>\[\hat{y} = \text{sigmoid} (z) = \sigma(z) = \sigma(w^T x + b)\]</div>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="_images/2-2-sigmoid.png"><img alt="_images/2-2-sigmoid.png" src="_images/2-2-sigmoid.png" style="height: 200px;" /></a>
</figure>
<div class="important admonition">
<p class="admonition-title">Why we are using the sigmoid function here?</p>
<p><span class="math notranslate nohighlight">\((w^T x + b)\)</span> is a linear function <span class="math notranslate nohighlight">\((ax+b)\)</span>, but since we are looking for a probability constraint between <span class="math notranslate nohighlight">\([0,1]\)</span>, the sigmoid function is used. The function is bounded between <span class="math notranslate nohighlight">\([0,1]\)</span> as shown in the graph above.</p>
</div>
<p>This is what the sigmoid function looks like.</p>
<div class="math notranslate nohighlight" id="equation-2-2">
<span class="eqno">(2)<a class="headerlink" href="#equation-2-2" title="Link to this equation">#</a></span>\[\sigma(z) = \dfrac{1}{1+e^{(-z)}}\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(z\)</span> is a very large positive number, then <span class="math notranslate nohighlight">\(\sigma(z)\)</span> will be close to 1.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(z\)</span> is a very large negative number, then <span class="math notranslate nohighlight">\(\sigma(z)\)</span> will be close to 0.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(z = 0\)</span>, then  <span class="math notranslate nohighlight">\(\sigma(z) = 0.5\)</span></p></li>
</ul>
<p>So when you implement logistic regression, your job is to try to learn parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> so that <span class="math notranslate nohighlight">\(\hat{y}\)</span> becomes a good estimate of the chance of <span class="math notranslate nohighlight">\(y\)</span> being equal to one.</p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p>Q: <strong>What are the parameters of logistic regression?</strong>  </br>
A. <span class="math notranslate nohighlight">\(w\)</span>, an identity vector, and <span class="math notranslate nohighlight">\(b\)</span>, a real number.  </br>
B. <span class="math notranslate nohighlight">\(w\)</span>, an <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector, and <span class="math notranslate nohighlight">\(b\)</span>, a real number.  </br>
C. <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>,  both <span class="math notranslate nohighlight">\(n_x\)</span> dimensional vector.  </br>
D. <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>,  both real number.</p>
</div>
</section>
<section id="logistic-regression-cost-function">
<h3>Logistic Regression Cost Function<a class="headerlink" href="#logistic-regression-cost-function" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/9K62xu8MKMk">Video</a></p>
<p>In the previous section, you saw the logistic regression model to train the parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, of logistic regression model. You need to define a cost function, let’s take a look at the cost function.</p>
<p>To learn parameters for your model, you’re given a training set of training examples and it seems natural that you want to find parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.  So that at least on the training set, the outputs you have the predictions you have on the training set, which that the preicition values will be close to the true labels y that you got in the training set.</p>
<ul class="simple">
<li><p>Given <span class="math notranslate nohighlight">\(m\)</span> examples: <span class="math notranslate nohighlight">\(\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)}) \}\)</span>, want <span class="math notranslate nohighlight">\(\hat{y}^{(i)} \approx y^{(i)}\)</span></p></li>
<li><p>And of course for each training example, we’re using these superscripts with round brackets with parentheses to index into different train examples. Your prediction on training example <span class="math notranslate nohighlight">\(i\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-2-3">
<span class="eqno">(3)<a class="headerlink" href="#equation-2-3" title="Link to this equation">#</a></span>\[\hat{y}^{(i)} = \sigma(w^T x^{(i)} + b), \quad \text{where } \sigma(z^{(i)}) = \dfrac{1}{1+e^{(-z^{(i)})}}\]</div>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\(i\)</span>-th example: <span class="math notranslate nohighlight">\(x^{(i)}\)</span>, <span class="math notranslate nohighlight">\(y^{(i)}\)</span>, <span class="math notranslate nohighlight">\(z^{(i)}\)</span>.</p></li>
</ul>
<p>Now let’s see what loss function or an error function we can use to measure how well our algorithm is doing.</p>
<p><strong>Loss (error) function:</strong></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = - \Big( y^{(i)} \  \text{log}(\hat{y}^{(i)}) + (1-y^{(i)}) \ \text{log}(1-\hat{y}^{(i)}) \Big)\]</div>
<div class="math notranslate nohighlight" id="equation-2-4">
<span class="eqno">(4)<a class="headerlink" href="#equation-2-4" title="Link to this equation">#</a></span>\[\mathcal{L}(\hat{y}, y) = - \Big( y \  \text{log}(\hat{y}) + (1-y) \ \text{log}(1-\hat{y}) \Big)\]</div>
<p>This function <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> is called the <em><strong>loss function</strong></em> is a function will need to define to measure how good our output <span class="math notranslate nohighlight">\(\hat{y}\)</span> is when the true label is <span class="math notranslate nohighlight">\(y\)</span>. The loss function measures the discrepancy between the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> and the desired output <span class="math notranslate nohighlight">\(y\)</span>. In other words, the loss function computes the error for a single training example.</p>
<p>Keep in mind that if we are using squared error then you want to square error to be as small as possible. And with this logistic regression, lost function will also want this to be as small as possible. To understand why this makes sense, let’s look at the two cases:</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y = 1\)</span>, then <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y) = - \text{log}(\hat{y})\)</span>. So you want <span class="math notranslate nohighlight">\(- \text{log}(\hat{y})\)</span> to be as small as possible, that means you want <span class="math notranslate nohighlight">\(\text{log}(\hat{y})\)</span> to be as big as possible, and that means you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be large. But because <span class="math notranslate nohighlight">\(\hat{y}\)</span> is you know the sigmoid function, it can never be bigger than one. So this is saying that if <span class="math notranslate nohighlight">\(y = 1\)</span>, you want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be as big as possible, but it can’t ever be bigger than one. So saying you want, <span class="math notranslate nohighlight">\(\hat{y}\)</span> to be <strong>close to one</strong> as well.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(y = 0\)</span>, then <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y) = - \text{log}(1-\hat{y})\)</span>. So if in your learning procedure you try to make the loss function small. What this means is that you want, <span class="math notranslate nohighlight">\(\text{log}(1-\hat{y})\)</span> to be large. And then through a similar piece of reasoning, you can conclude that this loss function is trying to make <span class="math notranslate nohighlight">\(\hat{y}\)</span> as small as possible, and again, because <span class="math notranslate nohighlight">\(\hat{y} \in \{0,1\}\)</span> . This is saying that if <span class="math notranslate nohighlight">\(y = 0\)</span> then your loss function will push the parameters to make <span class="math notranslate nohighlight">\(\hat{y}\)</span> as <strong>close to zero</strong> as possible.</p></li>
</ul>
<p>We just gave here a somewhat informal justification for this particular loss function, we will provide more details later to give a more formal justification for <span class="math notranslate nohighlight">\(y\)</span>. In logistic regression, we like to use the loss function with this particular form.</p>
<div class="important admonition">
<p class="admonition-title">Why DO NOT use the squared error, <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y)= \dfrac{1}{2}(\hat{y}-y)^2\)</span>, in the loss function?</p>
<p>It turns out that you could do this, but in logistic regression people don’t usually do this. Because when you come to learn the parameters, you find that the optimization problem, which becomes non-convex. So you end up with optimization problem, you are with multiple local optima. So gradient descent, may not find a global optimum.</p>
<p>Squared eror seems like it might be a reasonable choice except that it makes great in descent not work well. So in logistic regression were actually define a different loss function that plays a similar role as squared error but will give us an optimization problem that is convex.</p>
</div>
<p>The loss function was defined with respect to a single training example. It measures how well you’re doing on a single training example, I’m now going to define something called the cost function, which measures how are you doing on the entire training set.</p>
<p><strong>Cost function:</strong></p>
<p>The cost function is the average of the loss function of the entire training set. We are going to find the parameters <span class="math notranslate nohighlight">\(w\)</span> 𝑎𝑛𝑑 <span class="math notranslate nohighlight">\(b\)</span> that minimize the overall cost function.</p>
<div class="math notranslate nohighlight" id="equation-2-5">
<span class="eqno">(5)<a class="headerlink" href="#equation-2-5" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\boldsymbol{J}(w,b) 
&amp;= \dfrac{1}{m} \sum_{i=1}^{m}  \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) \\
&amp;= - \dfrac{1}{m} \sum_{i=1}^{m} \Big[ \Big( y^{(i)} \  \text{log}(\hat{y}^{(i)}) + (1-y^{(i)}) \ \text{log}(1-\hat{y}^{(i)}) \Big) \Big]
\end{aligned}\end{split}\]</div>
<p>So the cost function <span class="math notranslate nohighlight">\(\boldsymbol{J}\)</span>, which is applied to your parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, is going to be the average, one of the <span class="math notranslate nohighlight">\(m\)</span> of the sum of the loss function apply to each of the training examples in turn.</p>
<p>The terminology I’m going to use is that the loss function is applied to just a single training example, check out equation <a class="reference internal" href="#equation-2-4">(4)</a>. And the cost function is the cost of your parameters, so in training your logistic regression model, we’re going to try to find parameters <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>. That minimize the overall cost function <span class="math notranslate nohighlight">\(\boldsymbol{J}\)</span>, written at the equation <a class="reference internal" href="#equation-2-5">(5)</a>.</p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p>Q: <strong>What is the difference between the cost function and the loss function for logistic regression?</strong>  </br>
A. The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.  </br>
B. The cost function computes the error for a single training example; the loss function is the average of the cost functions of the entire training set.  </br>
C. They are different names for the same function.  </br></p>
</div>
</section>
<section id="explanation-of-logisitic-regression-cost-function-optional">
<h3>Explanation of Logisitic Regression Cost Function (Optional)<a class="headerlink" href="#explanation-of-logisitic-regression-cost-function-optional" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/2vzqUcivE0A">Video</a></p>
<p>This is an optional section. In this part, we will have a quick justification for why we like to use that cost function for logistic regression.</p>
<section id="logisitc-regression-cost-function">
<h4>Logisitc regression cost function<a class="headerlink" href="#logisitc-regression-cost-function" title="Link to this heading">#</a></h4>
<p>In logistic regression,</p>
<div class="math notranslate nohighlight">
\[\hat{y} = \sigma(w^T + b), \quad \text{where } \sigma (z) = \dfrac{1}{1+e^{-z}}\]</div>
<p>The interpretation is <span class="math notranslate nohighlight">\(\hat{y} = \mathrm{P}(y=1\mid x)\)</span>. So we want our algorithm to output <span class="math notranslate nohighlight">\(\hat{y}\)</span> has the chance that <span class="math notranslate nohighlight">\(y = 1\)</span> for a given set of input features <span class="math notranslate nohighlight">\(x\)</span>. So another way to say this is that if <span class="math notranslate nohighlight">\(y\)</span> is equal to 1 then the chance of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span> is equal to <span class="math notranslate nohighlight">\(\hat{y}\)</span>,  and conversely if <span class="math notranslate nohighlight">\(y\)</span> is equal to 0 then the chance that y was 0 was <span class="math notranslate nohighlight">\(1-\hat{y}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{If } y = 1: \qquad \mathrm{P}(y\mid x) &amp;= \hat{y} \\
\text{If } y = 0: \qquad \mathrm{P}(y\mid x) &amp;= 1-\hat{y} 
\end{aligned}
\end{split}\]</div>
<p>So if <span class="math notranslate nohighlight">\(\hat{y}\)</span> was a chance that <span class="math notranslate nohighlight">\(y = 1\)</span>, then <span class="math notranslate nohighlight">\(1-\hat{y}\)</span> is the chance that <span class="math notranslate nohighlight">\(y = 0\)</span>.</p>
<p>So what I’m going to do is take these two equations which basically define <span class="math notranslate nohighlight">\(\mathrm{P}(y \mid x)\)</span> for the two cases of <span class="math notranslate nohighlight">\(y = 0\)</span> or <span class="math notranslate nohighlight">\(y = 1\)</span>. And then take these two equations and summarize them into a single equation. And just to point out <span class="math notranslate nohighlight">\(y\)</span> has to be either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span> because in binary cost equations, <span class="math notranslate nohighlight">\(y = 0\)</span> or <span class="math notranslate nohighlight">\(y = 1\)</span> are the only two possible cases. When someone take these two equations and summarize them as follows:</p>
<div class="math notranslate nohighlight">
\[\mathrm{P}(y \mid x) = \hat{y}^y \  (1-\hat{y})^{(1-y)}\]</div>
<p>Now, because the <span class="math notranslate nohighlight">\(\text{log}\)</span> function is a strictly monotonically increasing function, your maximizing <span class="math notranslate nohighlight">\(\text{log} \ \mathrm{P}(y \mid x)\)</span> should give you a similar result as optimizing <span class="math notranslate nohighlight">\(\mathrm{P}(y \mid x)\)</span>. So:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{log} \ \mathrm{P}(y \mid x)
&amp;=  y \  \text{log}(\hat{y}) + (1-y) \ \text{log}(1-\hat{y})  \\
&amp;=  - \mathcal{L}(\hat{y}, y)
\end{aligned}
\end{split}\]</div>
<p>So this is actually negative of the loss function that we had to find previously. And there is a negative sign there because usually if you are training a learning algorithm, you want to make probabilities large. Whereas in logistic regression, we want to minimize the loss function. So minimizing the loss corresponds to maximizing the log of the probability. So this is what the loss function on a single example looks like.</p>
<p>So this is what the loss function on a single example looks like. How about the cost function, the overall cost function on the entire training set on m examples? Let’s figure that out.</p>
</section>
<section id="cost-on-m-example">
<h4>Cost on m example<a class="headerlink" href="#cost-on-m-example" title="Link to this heading">#</a></h4>
<p>So, the probability of all the labels In the training set. If you assume that the training examples I’ve drawn independently or drawn IID, identically independently distributed, then the probability of the example is the product of probabilities.</p>
<div class="math notranslate nohighlight">
\[\mathrm{P}(\text{labels in training set}) = \prod_{i=1}^{m} \mathrm{P}(y^{(i)} \mid x^{(i)})\]</div>
<p>And so if you want to carry out maximum likelihood estimation, then you want to find the parameters that maximizes the chance of your observations at training set. But maximizing this is the same as maximizing the log, so we just put logs on both sides:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\text{log} \ \mathrm{P}
&amp;=  \sum_{i=1}^{m} \text{log} \  \mathrm{P}(y^{(i)} \mid x^{(i)})\\
&amp;=  - \sum_{i=1}^{m} \ \mathcal{L}(\hat{y}^{(i)}, y^{(i)})
\end{aligned}
\end{split}\]</div>
<p>And so in statistics, there’s a principle called the principle of maximum likelihood estimation, which just means to choose the parameters that maximizes <span class="math notranslate nohighlight">\(\text{log} \ \mathrm{P}\)</span>. Or in other words, that maximizes this <span class="math notranslate nohighlight">\(\text{log} \ \mathrm{P}\)</span>.</p>
<p>So this justifies the cost we had for logistic regression which is <span class="math notranslate nohighlight">\(\boldsymbol{J}(w,b) \)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{J}(w,b) = \dfrac{1}{m} \sum_{i=1}^{m}  \mathcal{L}(\hat{y}^{(i)}, y^{(i)}) = - \dfrac{1}{m} \text{log} \ \mathrm{P}\]</div>
<p>Because we now want to minimize the cost instead of maximizing likelihood, we’ve got to rid of the minus sign. And then finally for convenience, to make sure that our quantities are better scale, we just add a 1 over <span class="math notranslate nohighlight">\(m\)</span> extra scaling factor there. But so to summarize, by minimizing this cost function <span class="math notranslate nohighlight">\(\boldsymbol{J}(w,b) \)</span> we’re really carrying out maximum likelihood estimation with the logistic regression model. Under the assumption that our training examples were IID, or identically independently distributed.</p>
<p>I hope this gives you a sense of why we use the cost function we do for logistic regression.</p>
</section>
</section>
<section id="gradient-descent">
<h3>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/Nuxx65l5pUI">Video</a></p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p><strong>True or False</strong>: A convex function always has multiple local optima.  __________  </br></p>
</div>
</section>
<section id="derivatives">
<h3>Derivatives<a class="headerlink" href="#derivatives" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/Mz4VYF4CxGg">Video</a></p>
</section>
<section id="more-derivative-examples">
<h3>More Derivative Examples<a class="headerlink" href="#more-derivative-examples" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/6PSzTakWAO0">Video</a></p>
<ol class="arabic simple">
<li><p><strong>Derivative just means slope of a line.</strong></p>
<ul class="simple">
<li><p>The derivative of the function just means the slope of a function. The slope of a function can be different at different points on the function.</p></li>
<li><p>In our first example where <span class="math notranslate nohighlight">\(f(a) = 3a\)</span> is a straight line. The derivative was the same everywhere, it was three everywhere. For other functions like <span class="math notranslate nohighlight">\(f(a) = a^2\)</span> or <span class="math notranslate nohighlight">\(f(a) = \text{log}(a)\)</span>, the slope of the line varies. So, the slope or the derivative can be different at different points on the curve.</p></li>
</ul>
</li>
<li><p>If you want to look up the derivative of a function, you can flip open your calculus textbook or look up Wikipedia and often get a formula for the slope of these functions at different points.</p></li>
</ol>
</section>
<section id="computation-graph">
<h3>Computation Graph<a class="headerlink" href="#computation-graph" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/mS-SIq7ENuQ">Video</a></p>
<div class="tip admonition">
<p class="admonition-title">Practice Quiz</p>
<p>Q: <strong>One step of ________ propagation on a computation graph yields derivative of final output variable.</strong>  </br>
A. Backward  </br>
B. Forward </br></p>
</div>
</section>
<section id="derivatives-with-a-computation-graph">
<h3>Derivatives with a Computation Graph<a class="headerlink" href="#derivatives-with-a-computation-graph" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/VTlwSyuVIXU">Video</a></p>
</section>
<section id="logistic-regression-gradient-descent">
<h3>Logistic Regression Gradient Descent<a class="headerlink" href="#logistic-regression-gradient-descent" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/hZqWRCRu9N0">Video</a></p>
<div class="math notranslate nohighlight" id="equation-2-6">
<span class="eqno">(6)<a class="headerlink" href="#equation-2-6" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
z &amp;= w^T x + b \\
\hat{y} &amp;= a = \sigma(z) = \dfrac{1}{1+e^{-z}} \\
\mathcal{L}(a, y) &amp;= - \Big( y \  \text{log}(a) + (1-y) \ \text{log}(1-a) \Big)
\end{aligned}\end{split}\]</div>
<figure class="align-default" id="id3">
<a class="reference internal image-reference" href="_images/2-3.png"><img alt="_images/2-3.png" src="_images/2-3.png" style="height: 120px;" /></a>
</figure>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Python coding convention <code class="docutils literal notranslate"><span class="pre">dvar</span></code> represents:
The derivative of a final output variable with respect to various intermediate quantities.</p>
</div>
<p><strong>Derivatives:</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">da</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} = -\dfrac{y}{a} + \dfrac{1-y}{1-a}\)</span></p>
<p>Using the chain rule to calculate <code class="docutils literal notranslate"><span class="pre">dz</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">dz</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}z} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times  \dfrac{\mathrm{d}a}{\mathrm{d}z}\)</span></p>
<div class="math notranslate nohighlight" id="equation-2-7">
<span class="eqno">(7)<a class="headerlink" href="#equation-2-7" title="Link to this equation">#</a></span>\[\begin{split}
\begin{aligned}
\dfrac{\mathrm{d}a}{\mathrm{d}z} &amp;=  \dfrac{\mathrm{d}}{\mathrm{d}z} \sigma(z)  \\
&amp;= \dfrac{\mathrm{d}}{\mathrm{d}z} \Bigg( \dfrac{1}{1+e^{-z}} \Bigg) \\
&amp;= \dfrac{e^{-z}}{(1+e^{-z})^2}
\end{aligned}\end{split}\]</div>
<p>From equation <span class="math notranslate nohighlight">\(a = \dfrac{1}{1+e^{-z}}\)</span>, we can derive:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\dfrac{\mathrm{d}a}{\mathrm{d}z} &amp;= \dfrac{e^{-z}}{(1+e^{-z})^2} \\
&amp;= \dfrac{1}{1+e^{-z}} \cdot \Big(1- \dfrac{1}{1+e^{-z}} \Big) \\
&amp;= a(1-a)
\end{aligned}\end{split}\]</div>
<p><code class="docutils literal notranslate"><span class="pre">dz</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times  \dfrac{\mathrm{d}a}{\mathrm{d}z} = \Bigg( -\dfrac{y}{a} + \dfrac{1-y}{1-a}\Bigg) \times a(1-a) = a-y\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">dw1</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}w_1} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}w_1} = (a-y)x_1\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">dw2</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}w_2} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}w_2} = (a-y)x_2\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">db</span></code> <span class="math notranslate nohighlight">\( = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}b} = \dfrac{\mathrm{d}\mathcal{L}}{\mathrm{d}a} \times \dfrac{\mathrm{d}a}{\mathrm{d}z} \times \dfrac{\mathrm{d}z}{\mathrm{d}b} = a-y\)</span></p>
</section>
<section id="gradient-descent-on-m-examples">
<h3>Gradient Descent on m Examples<a class="headerlink" href="#gradient-descent-on-m-examples" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/4ckUNzRGgDI">Video</a></p>
</section>
</section>
<section id="python-and-vectoraization">
<h2>Python and Vectoraization<a class="headerlink" href="#python-and-vectoraization" title="Link to this heading">#</a></h2>
<section id="vectorization">
<h3>Vectorization<a class="headerlink" href="#vectorization" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/vMFJQbMIaQc">Video</a></p>
<p>Vectorization is basically the art of getting rid of explicit for loops in your code. In the deep learning era, especially in deep learning in practice, you often find yourself training on relatively large data sets, because that’s when deep learning algorithms tend to shine. And so, it’s important that your code very quickly because otherwise, if it’s training a big data set, your code might take a long time to run then you just find yourself waiting a very long time to get the result. So in the deep learning era, I think the ability to perform vectorization has become a key skill. Let’s start with an example.</p>
<p>In logistic regreesion, you need to solve this kind of problem:</p>
<div class="math notranslate nohighlight">
\[z = w^T x + b \quad \text{where } w \in \mathbb{R}^{n_x},  x \in  \mathbb{R}^{n_x}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\)</span> was this column vector and <span class="math notranslate nohighlight">\(x\)</span> is also this vector. Maybe they are very large vectors if you have a lot of features. So, <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(x\)</span> were both <span class="math notranslate nohighlight">\(\mathbb{R}^{n_x}\)</span> dimensional vectors.</p>
<p>So, to compute <span class="math notranslate nohighlight">\(w\)</span> transpose <span class="math notranslate nohighlight">\(x\)</span>, if you had a <em><strong>non-vectorized</strong></em> implementation, you would do something like <code class="docutils literal notranslate"><span class="pre">for</span></code> loop:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">n_x</span><span class="p">):</span>
	<span class="n">z</span> <span class="o">+=</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">z</span> <span class="o">+=</span> <span class="n">b</span>
</pre></div>
</div>
<p>That’s a non-vectorized implementation. Then you find that that’s going to be really slow. In contrast, a <em><strong>vectorized</strong></em> implementation would just compute <span class="math notranslate nohighlight">\(w\)</span> transpose <span class="math notranslate nohighlight">\(x\)</span> directly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>And you find that this is much faster. Let’s actually illustrate this with a little demo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1 2 3 4]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1000000</span><span class="p">)</span>

<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Vectorized version: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">tic</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000000</span><span class="p">):</span>
	<span class="n">c</span> <span class="o">+=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">toc</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;c = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">c</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Non-Vectorized version: &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="mi">1000</span><span class="o">*</span><span class="p">(</span><span class="n">toc</span><span class="o">-</span><span class="n">tic</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot;ms&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c = 249881.3198297877
Vectorized version: 1.0280609130859375ms
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>c = 249881.3198297821
Non-Vectorized version: 211.3649845123291ms
</pre></div>
</div>
</div>
</div>
<p>In both cases, the vectorize version and the non-vectorize version computed the same values, 249979, so on. The vectorize version took 0.4 milliseconds. The explicit for loop and non-vectorize version took about 255, almost 260 milliseconds. The non-vectorize version took something like 600 times longer than the vectorize version. With this example you see that if only you remember to vectorize your code, your code actually runs over 600 times faster.</p>
<p>If the engine slows down, it’s the difference between your code taking maybe one minute to run versus taking say five hours to run. And when you are implementing deep learning algorithms, you can really get a result back faster. It will be much faster if you vectorize your code.</p>
<p>Some of you might have heard that a lot of scaleable deep learning implementations are done on a GPU or a graphics processing unit. But all the demos I did just now in the Jupiter notebook where actually on the CPU. And it turns out that both GPU and CPU have parallelization instructions. They’re sometimes called SIMD instructions. This stands for a single instruction multiple data. But what this basically means is that, if you use built-in functions such as this <code class="docutils literal notranslate"><span class="pre">np.dot()</span></code> function or other functions that don’t require you explicitly implementing a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. It enables Phyton <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> to take much better advantage of parallelism to do your computations much faster.  And this is true both computations on CPUs and computations on GPUs. It’s just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that. Maybe just not as good as GPUs.</p>
<p>You’re seeing how vectorization can significantly speed up your code. The rule of thumb to remember is <em><strong>whenever possible, avoid using explicit for loops</strong></em>.</p>
</section>
<section id="more-vectorization-examples">
<h3>More Vectorization Examples<a class="headerlink" href="#more-vectorization-examples" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/QLyqDRZrNm0">Video</a></p>
<div class="hint admonition">
<p class="admonition-title">Neural Network programming guideline</p>
<p>Whenever possible, avoid explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
</div>
<p>And it’s not always possible to never use a for-loop, but when you can use a built in function or find some other way to compute whatever you need, you’ll often go faster than if you have an explicit for-loop.</p>
<p>Here is an another example, if ever you want to compute a vector <span class="math notranslate nohighlight">\(u\)</span> as the product of the matrix <span class="math notranslate nohighlight">\(A\)</span>, and another vector <span class="math notranslate nohighlight">\(v\)</span>, then the definition of our matrix multiply is that:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
u &amp;= Av \\
u_i &amp;= \sum_j A_{ij} v_j
\end{aligned}\end{split}\]</div>
<p>Non-vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="o">...</span> <span class="p">:</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="o">...</span> <span class="p">:</span> 
	 <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
</pre></div>
</div>
<p>Vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<section id="vectors-and-matrix-valued-functions">
<h4>Vectors and matrix valued functions<a class="headerlink" href="#vectors-and-matrix-valued-functions" title="Link to this heading">#</a></h4>
<p>Say you need to apply the exponential operation on every element of matrix/vector.</p>
<div class="math notranslate nohighlight">
\[\begin{split}v = \left[
\begin{matrix}
v_1 \\
\vdots \\
v_n
\end{matrix}
\right], \quad
u = \left[
\begin{matrix}
e^{v_1} \\
\vdots \\
e^{v_n} 
\end{matrix}
\right]
\end{split}\]</div>
<p>Non-vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="p">:</span>
     <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</pre></div>
</div>
<p>Vectorized in coding:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
<p>And so, notice that, whereas previously you had that explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, with just one line of code here, just <span class="math notranslate nohighlight">\(v\)</span> as an input vector <span class="math notranslate nohighlight">\(u\)</span> as an output vector, you’ve gotten rid of the explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop, and the implementation on the right will be much faster that the one needing an explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
<p>In fact, the NumPy library has many of the vector value functions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>   <span class="c1">#compute the element-wise log</span>
<span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">v</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">v</span><span class="o">**</span><span class="mi">2</span>
<span class="mi">1</span><span class="o">/</span><span class="n">v</span>
</pre></div>
</div>
<p>So, whenever you are tempted to write a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop take a look, and see if there’s a way to call a <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> built-in function to do it without that <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
</section>
<section id="gradient-descent-implementation">
<h4>Gradient descent implementation<a class="headerlink" href="#gradient-descent-implementation" title="Link to this heading">#</a></h4>
<p>So, let’s take all of these learnings and apply it to our logistic regression gradient descent implementation, and see if we can at least get rid of one of the two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops we had. So here’s our code for computing the derivatives for logistic regression, and we had two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">m</span> <span class="p">:</span>
	<span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
	<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
	<span class="c1"># in this example we only have 2 features, if yo had more features, see below</span>
	<span class="n">dw1</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">dw2</span> <span class="o">+=</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
	<span class="c1"># more features</span>
	<span class="k">for</span> <span class="n">j</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">nx</span> <span class="p">:</span>
		<span class="n">dw</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
</pre></div>
</div>
<p>So the way we’ll do so is that instead of explicitly initializing <code class="docutils literal notranslate"><span class="pre">dw1</span></code>, <code class="docutils literal notranslate"><span class="pre">dw2</span></code>, and so on to zeros, we’re going to get rid of this and instead make dw a vector.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">dw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_x</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">m</span> <span class="p">:</span>
	<span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
	<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
	<span class="n">dw</span> <span class="o">+=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	
<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
</pre></div>
</div>
<p>So now we’ve gone from having two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops to just one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. We still have this one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop that loops over the individual training examples.</p>
<p>So I hope this section gives you a sense of vectorization. And by getting rid of one for-loop your code will already run faster. But it turns out we could do even better. So the next section will talk about how to vectorize logistic aggression even further. And you see a pretty surprising result, that without using any for-loops, without needing a for-loop over the training examples, you could write code to process the entire training sets. So, pretty much all at the same time.</p>
</section>
</section>
<section id="vectorizting-logistic-regression">
<h3>Vectorizting Logistic Regression<a class="headerlink" href="#vectorizting-logistic-regression" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/A9Ag0PtZDLA">Video</a></p>
<p>To compute these predictions on our <span class="math notranslate nohighlight">\(m\)</span> training examples, there is a way to do so, without needing an explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loop.</p>
<ul class="simple">
<li><p>First, remember that we defined a matrix capital X to be your training inputs, stacked together in different columns like this.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}X= \left[
\begin{matrix}
\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \\
\vdots &amp; \vdots &amp; \cdots &amp; \vdots 
\end{matrix}
\right]
\end{split}\]</div>
<p>So, this is a matrix, that is a <span class="math notranslate nohighlight">\(n_x \times m\)</span> dimensional matrix. Now, the first thing I want to do is show how you can compute <span class="math notranslate nohighlight">\(z^{(1)}\)</span>, <span class="math notranslate nohighlight">\(z^{(2)}\)</span>, <span class="math notranslate nohighlight">\(z^{(3)}\)</span> and so on, all in one step, in fact, with one line of code. So, I’m going to construct a <span class="math notranslate nohighlight">\(1 \times m\)</span> matrix that’s really a row vector while I’m going to compute <span class="math notranslate nohighlight">\(z^{(1)}\)</span>, <span class="math notranslate nohighlight">\(z^{(2)}\)</span>, and so on, down to <span class="math notranslate nohighlight">\(z^{(m)}\)</span>, all at the same time.</p>
<div class="math notranslate nohighlight">
\[Z = [z^{(1)}, z^{(2)}, \cdots, z^{(m)}] = w^T X+ [b, b, \cdots, b] = [w^T x^{(1)} + b, w^T x^{(2)} + b, \cdots, w^T x^{(m)} + b]\]</div>
<p>The <span class="math notranslate nohighlight">\(w^T\)</span> will be a row vector. <span class="math notranslate nohighlight">\([b, b, \cdots, b]\)</span> is a <span class="math notranslate nohighlight">\(1 \times m\)</span> row vector. So you end up with another <span class="math notranslate nohighlight">\(1 \times m\)</span> vector.</p>
<p>So just as <span class="math notranslate nohighlight">\(X\)</span> was once obtained, when you took your training examples and stacked them next to each other, stacked them horizontally. I’m going to define capital <span class="math notranslate nohighlight">\(Z\)</span> to be this where you take the lowercase <span class="math notranslate nohighlight">\(z\)</span>’s and stack them horizontally. So when you stack the lower case <span class="math notranslate nohighlight">\(x\)</span>’s corresponding to a different training examples, horizontally you get this variable capital <span class="math notranslate nohighlight">\(X\)</span> and the same way when you take these lowercase <span class="math notranslate nohighlight">\(z\)</span> variables, and stack them horizontally, you get this variable capital <span class="math notranslate nohighlight">\(Z\)</span>.</p>
<p>And it turns out, that in order to implement this, the <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> command is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>Now there is a subtlety in Python, which is at here <code class="docutils literal notranslate"><span class="pre">b</span></code> is a real number or if you want to say you know <span class="math notranslate nohighlight">\(1 \times 1\)</span> matrix, is just a normal real number. But, when you add this vector to a real number, Python automatically takes this real number <code class="docutils literal notranslate"><span class="pre">b</span></code> and expands it out to this  <span class="math notranslate nohighlight">\(1 \times m\)</span> row vector (i.e. <span class="math notranslate nohighlight">\([b, b, \cdots, b]\)</span>). So in case this operation seems a little bit mysterious, this is called <strong>broadcasting</strong> in Python.</p>
<ul class="simple">
<li><p>Second, what we would like to do next, is find a way to compute <span class="math notranslate nohighlight">\(a^{(1)}\)</span>, <span class="math notranslate nohighlight">\(a^{(2)}\)</span> and so on to <span class="math notranslate nohighlight">\(a^{(m)}\)</span>, all at the same time, and just as stacking lowercase <span class="math notranslate nohighlight">\(x\)</span>’s resulted in capital <span class="math notranslate nohighlight">\(X\)</span> and stacking horizontally lowercase <span class="math notranslate nohighlight">\(z\)</span>’s resulted in capital <span class="math notranslate nohighlight">\(Z\)</span>, stacking lower case <span class="math notranslate nohighlight">\(a\)</span>, is going to result in a new variable, which we are going to define as capital <span class="math notranslate nohighlight">\(A\)</span>.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[A = [a^{(1)}, a^{(2)}, \cdots, a^{(m)}] = \sigma(Z)\]</div>
<p>And in the program assignment, you see how to implement a vector valued sigmoid function, so that the sigmoid function, inputs this capital <span class="math notranslate nohighlight">\(Z\)</span> as a variable and very efficiently outputs capital <span class="math notranslate nohighlight">\(A\)</span>. So you see the details of that in the programming assignment.</p>
<p>What we’ve seen in this section is that instead of needing to loop over <span class="math notranslate nohighlight">\(m\)</span> training examples to compute lowercase <span class="math notranslate nohighlight">\(z\)</span> and lowercase <span class="math notranslate nohighlight">\(a\)</span>, one of the time, you can implement this one line of code, to compute all these <span class="math notranslate nohighlight">\(z\)</span>’s at the same time. And then, this one line of code, with appropriate implementation of lowercase Sigma to compute all the lowercase <span class="math notranslate nohighlight">\(a\)</span>’s all at the same time. So this is how you implement a vectorize implementation of the forward propagation for all <span class="math notranslate nohighlight">\(m\)</span> training examples at the same time.</p>
<p>So to summarize, you’ve just seen how you can use vectorization to very efficiently compute all of the activations, all the lowercase <span class="math notranslate nohighlight">\(a\)</span>’s at the same time. Next, it turns out, you can also use vectorization very efficiently to compute the backward propagation, to compute the gradients.</p>
</section>
<section id="vectorizing-logistic-regresion-s-gradient-output">
<h3>Vectorizing Logistic Regresion’s Gradient Output<a class="headerlink" href="#vectorizing-logistic-regresion-s-gradient-output" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/Y5LsQZY-0QM">Video</a></p>
<p>In the previous section, you learned how you can use vectorization to compute their predictions. In this section, you will learn how you can use vectorization to also perform the gradient computations for all <span class="math notranslate nohighlight">\(m\)</span> training samples. Again, all sort of at the same time. And then at the end of this part, we will put it all together and show how you can derive a very efficient implementation of logistic regression.</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}Z = [\mathrm{d}z^{(1)}, \mathrm{d}z^{(2)}, \cdots, \mathrm{d}z^{(m)}] = [a^{(1)} - y^{(1)}, a^{(2)} - y^{(2)}, \cdots, a^{(m)} - y^{(m)}]\]</div>
<p>It’s <span class="math notranslate nohighlight">\(1 \times m\)</span> matrix or alternatively a <span class="math notranslate nohighlight">\(m\)</span> dimensional row vector.</p>
<div class="math notranslate nohighlight">
\[A = [a^{(1)}, a^{(2)}, \cdots, a^{(m)}]\]</div>
<div class="math notranslate nohighlight">
\[Y = [y^{(1)}, y^{(2)}, \cdots, y^{(m)}]\]</div>
<p>So, based on these definitions, maybe you can see for yourself that <span class="math notranslate nohighlight">\(\mathrm{d}Z\)</span> can be computed as just <span class="math notranslate nohighlight">\(A-Y\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}Z = A-Y = [a^{(1)} - y^{(1)}, a^{(2)} - y^{(2)}, \cdots, a^{(m)} - y^{(m)}]\]</div>
<p>So, with just one line of code, you can compute all of this at the same time.</p>
<p>Now, in the previous implementation, we’ve gotten rid of one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop already but we still had this second <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over training examples.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}w &amp;= 0 \\
\mathrm{d}w &amp;\ += x^{(1)}\mathrm{d}z^{(1)} \\
\mathrm{d}w &amp;\ += x^{(2)}\mathrm{d}z^{(2)} \\
&amp; \vdots \\
\mathrm{d}w  &amp;\ /= m
\end{aligned} \ \qquad  \qquad 
\begin{aligned}
\mathrm{d}b &amp;= 0 \\
\mathrm{d}b &amp;\ += \mathrm{d}z^{(1)} \\
\mathrm{d}b &amp;\ += \mathrm{d}z^{(2)} \\
&amp; \vdots \\
\mathrm{d}b  &amp;\ /= m
\end{aligned}\end{split}\]</div>
<p>So that’s what we had in the previous implementation. We’d already got rid of one <code class="docutils literal notranslate"><span class="pre">for</span></code> loop. So, at least now <code class="docutils literal notranslate"><span class="pre">dw</span></code> is a vector and we went separately updating <code class="docutils literal notranslate"><span class="pre">dw1</span></code>, <code class="docutils literal notranslate"><span class="pre">dw2</span></code> and so on. So, we got rid of that already but we still had the <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over the <span class="math notranslate nohighlight">\(m\)</span> examples in the training set. So, let’s take these operations and vectorize them.</p>
<div class="math notranslate nohighlight">
\[\mathrm{d}b = \dfrac{1}{m} \sum_{i=1}^{m} \mathrm{d}z^{(i)}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">)</span>
</pre></div>
</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathrm{d}w 
&amp;= \dfrac{1}{m} X (\mathrm{d}Z)^T \\
&amp;= \dfrac{1}{m} \left[
\begin{matrix}
\vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
x^{(1)} &amp; x^{(2)} &amp; \cdots &amp; x^{(m)} \\
\vdots &amp; \vdots &amp; \cdots &amp; \vdots 
\end{matrix}
\right] \left[
\begin{matrix}
\mathrm{d}z^{(1)} \\
\vdots \\
\mathrm{d}z^{(m)}
\end{matrix}
\right] \\
&amp;= \dfrac{1}{m} [x^{(1)}\mathrm{d}z^{(1)} + \cdots + x^{(m)} \mathrm{d}z^{(m)}]
\end{aligned}\end{split}\]</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dw</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dZ</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
<p>So now, let’s put all together into how you would actually implement logistic regression. This is our original, highly inefficient non vectorize implementation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">J</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="mi">0</span>

<span class="k">for</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">1</span> <span class="n">to</span> <span class="n">m</span> <span class="p">:</span>
	<span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span>
	<span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">J</span> <span class="o">+=</span> <span class="o">-</span> <span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
	<span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">dw1</span> <span class="o">+=</span> <span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">dw2</span> <span class="o">+=</span> <span class="n">x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
	<span class="n">db</span> <span class="o">+=</span> <span class="n">dz</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">J</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw1</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">dw2</span> <span class="o">/=</span> <span class="n">m</span><span class="p">,</span> <span class="n">db</span> <span class="o">/=</span> <span class="n">m</span>
</pre></div>
</div>
<p>But now, we will see that the whole vectorization process without expilcit two <code class="docutils literal notranslate"><span class="pre">for</span></code> loops.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
<span class="n">dZ</span> <span class="o">=</span> <span class="n">A</span> <span class="o">-</span> <span class="n">Y</span>
<span class="n">dw</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dZ</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">)</span>
</pre></div>
</div>
<p>So, you’ve just done forward propagation and back propagation, really computing the predictions and computing the derivatives on all <span class="math notranslate nohighlight">\(m\)</span> training examples without using a for loop. And so the gradient descent update then would be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dw</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> \<span class="n">alpha</span> <span class="n">dw</span>
<span class="n">db</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> \<span class="n">alpha</span> <span class="n">db</span>
</pre></div>
</div>
<p>You have just implemented a <strong>single iteration</strong> of gradient descent for logistic regression.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Now, we talked about before we should get rid of explicit <code class="docutils literal notranslate"><span class="pre">for</span></code> loops whenever you can. However, if you want to implement multiple iterations as a gradient descent then you still need a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over the number of iterations. So, if you want to have a thousand iterations of gradient descent, you might still need a <code class="docutils literal notranslate"><span class="pre">for</span></code> loop over the iteration number. There is an outermost for loop like that then I don’t think there is any way to get rid of that for loop.</p>
<div class="python docutils">
<p>for iter in range(1000):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Z = np.dot(w.T, X) + b
A = sigmoid(Z)
dZ = A - Y
dw = 1/m * np.dot(X, dZ.T)
db = 1/m * np.sum(dZ)

dw = w - \alpha dw
db = b - \alpha db
</pre></div>
</div>
</div>
</div>
<p>So, that’s it you now have a highly vectorize and highly efficient implementation of gradient descent for logistic regression. There is just one more detail that I want to talk about in the section, which is in our description here I briefly alluded to this technique called broadcasting. Broadcasting turns out to be a technique that Python and numpy allows you to use to make certain parts of your code also much more efficient.</p>
</section>
<section id="broadcasting-in-python">
<h3>Broadcasting in Python<a class="headerlink" href="#broadcasting-in-python" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/hnBXZO5JHUc">Video</a></p>
<p>In the previous section, we noticed that broadcasting is another technique that you can use to make your Python code run faster. In this section, let’s delve into how broadcasting in Python actually works. Let’s explore broadcasting with an example.</p>
<p>In this matrix, it shows the number of calories from carbohydrates, proteins, and fats in 100 grams of four different foods:</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="_images/2-4.png"><img alt="_images/2-4.png" src="_images/2-4.png" style="height: 100px;" /></a>
</figure>
<p>So for example, a 100 grams of apples turns out, has 56 calories from carbs, and much less from proteins and fats. Whereas, in contrast, a 100 grams of beef has 104 calories from protein and 135 calories from fat.</p>
<p>Now, let’s say your goal is to calculate the percentage of calories from carbs, proteins and fats for each of the four foods. So, for example, if you look at this column and add up the numbers in that column you get that 100 grams of apple has 56 plus 1.2 plus 1.8 so that’s 59 calories. And so as a percentage the percentage of calories from carbohydrates in an apple would be 56 over 59, that’s about 94.9%. So most of the calories in an apple come from carbs, whereas in contrast, most of the calories of beef come from protein and fat and so on. So the calculation you want is really to sum up each of the four columns of this matrix to get the total number of calories in 100 grams of apples, beef, eggs, and potatoes. And then to divide throughout the matrix, so as to get the percentage of calories from carbs, proteins and fats for each of the four foods. So the question is, can you do this without an explicit for-loop?</p>
<p>What I’m going to do is show you how you can set, say this matrix equal to three by four matrix <span class="math notranslate nohighlight">\(A\)</span>.</p>
<ul class="simple">
<li><p>And then with one line of Python code we’re going to sum down the columns. So we’re going to get four numbers corresponding to the total number of calories in these four different types of foods, 100 grams of these four different types of foods.</p></li>
<li><p>Using a second line of Python code to divide each of the four columns by their corresponding sum.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">56.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">4.4</span><span class="p">,</span> <span class="mf">68.0</span><span class="p">],</span>
	      <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">104.0</span><span class="p">,</span> <span class="mf">52.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">],</span>
	      <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span> <span class="mf">135.0</span><span class="p">,</span> <span class="mf">99.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>
		     
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">cal</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cal</span><span class="p">)</span>

<span class="n">percentage</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">A</span> <span class="o">/</span> <span class="n">cal</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">percentage</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[ 56.    0.    4.4  68. ]
 [  1.2 104.   52.    8. ]
 [  1.8 135.   99.    0.9]]
[ 59.  239.  155.4  76.9]
[[94.91525424  0.          2.83140283 88.42652796]
 [ 2.03389831 43.51464435 33.46203346 10.40312094]
 [ 3.05084746 56.48535565 63.70656371  1.17035111]]
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>To add a bit of detail this parameter, <code class="docutils literal notranslate"><span class="pre">(axis</span> <span class="pre">=</span> <span class="pre">0)</span></code>, means that you want Python to sum vertically. So if this is axis 0 this means to sum vertically, where as the horizontal axis is axis 1. So be able to write axis 1 or sum horizontally instead of sum vertically.</p></li>
<li><p>So this is a three by four matrix and you divide it by a one by four matrix. And technically, after this first line of codes <code class="docutils literal notranslate"><span class="pre">cal</span></code>, the variable <code class="docutils literal notranslate"><span class="pre">cal</span></code>, is already a one by four matrix. So technically you don’t need to call reshape here again, so that’s actually a little bit redundant. But when I’m writing Python codes if I’m not entirely sure what matrix, whether the dimensions of a matrix I often would just call a reshape command just to make sure that it’s the right column vector or the row vector or whatever you want it to be. The reshape command is a constant time. It’s a order one operation that’s very cheap to call. So don’t be shy about using the reshape command to make sure that your matrices are the size you need it to be.</p></li>
</ul>
<section id="broadcasting-examples">
<h4>Broadcasting examples<a class="headerlink" href="#broadcasting-examples" title="Link to this heading">#</a></h4>
<p>Now, let’s explain in greater detail how this type of operation works. We had a three by four matrix and we divided it by a one by four matrix. So, how can you divide a three by four matrix by a one by four matrix? Or by one by four vector? Let’s go through a few more examples of broadcasting.</p>
<ol class="arabic simple">
<li><p>If you take a 4 by 1 vector and add it to a number, what Python will do is take this number and auto-expand it into a four by one vector as well, as follows. And so the vector [1, 2, 3, 4] plus the number 100 ends up with that vector on the right. You’re adding a 100 to every element, and in fact we use this form of broadcasting where that constant was the parameter <span class="math notranslate nohighlight">\(b\)</span> in an earlier section. And this type of broadcasting works with both column vectors and row vectors, and in fact we use a similar form of broadcasting earlier with the constant we’re adding to a vector being the parameter <span class="math notranslate nohighlight">\(b\)</span> in logistic regression.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1 \\
2 \\
3 \\
4
\end{matrix}
\right] + 100 = 
\left[
\begin{matrix}
1 \\
2 \\
3 \\
4
\end{matrix}
\right]  + 
\left[
\begin{matrix}
100 \\
100 \\
100 \\
100
\end{matrix}
\right] =
\left[
\begin{matrix}
101 \\
102 \\
103 \\
104
\end{matrix}
\right] \end{split}\]</div>
<ol class="arabic simple" start="2">
<li><p>Here’s another example. Let’s say you have a two by three matrix and you add it to this one by <span class="math notranslate nohighlight">\(n\)</span> matrix. So the general case would be if you have some <span class="math notranslate nohighlight">\((m,n)\)</span> matrix here and you add it to a <span class="math notranslate nohighlight">\((1,n)\)</span> matrix. What Python will do is copy the matrix <span class="math notranslate nohighlight">\(m\)</span> times to turn this into <span class="math notranslate nohighlight">\(m\)</span> by <span class="math notranslate nohighlight">\(n\)</span> matrix, so instead of this one by three matrix it’ll copy it twice in this example to turn it into this. Also, two by three matrix and we’ll add these so you’ll end up with the sum on the right. So you taken, you added 100 to the first column, added 200 to second column, added 300 to the third column. And this is basically what we did on the previous steps, except that we use a division operation instead of an addition operation.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right] + 
\left[
\begin{matrix}
100 &amp;  200  &amp; 300
\end{matrix}
\right] = 
\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right]  + 
\left[
\begin{matrix}
100 &amp;  200  &amp; 300 \\
100 &amp;  200  &amp; 300
\end{matrix}
\right]   =
\left[
\begin{matrix}
101 &amp;  202  &amp; 303 \\
104 &amp;  205  &amp; 306
\end{matrix}
\right] \end{split}\]</div>
<ol class="arabic simple" start="3">
<li><p>So one last example, whether you have a <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and you add this to a <span class="math notranslate nohighlight">\((m,1)\)</span> vector or <span class="math notranslate nohighlight">\((m,1)\)</span> matrix. Then just copy this <span class="math notranslate nohighlight">\(n\)</span> times horizontally. So you end up with an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix. So as you can imagine you copy it horizontally three times. And you add those. So when you add them you end up with this. So we’ve added 100 to the first row and added 200 to the second row.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right] + 
\left[
\begin{matrix}
100 \\ 
200
\end{matrix}
\right] = 
\left[
\begin{matrix}
1  &amp;  2  &amp; 3 \\
4  &amp;  5  &amp; 6
\end{matrix}
\right]  + 
\left[
\begin{matrix}
100 &amp;  100  &amp; 100 \\
200 &amp;  200  &amp; 200
\end{matrix}
\right]   =
\left[
\begin{matrix}
101 &amp;  102  &amp; 103 \\
204 &amp;  205  &amp; 206
\end{matrix}
\right] \end{split}\]</div>
</section>
<section id="general-principle">
<h4>General Principle<a class="headerlink" href="#general-principle" title="Link to this heading">#</a></h4>
<ol class="arabic">
<li><p>Here’s the more general principle of broadcasting in Python. If you have an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and you add or subtract or multiply or divide with a <span class="math notranslate nohighlight">\((1,n)\)</span> matrix, then this will copy it <span class="math notranslate nohighlight">\(m\)</span> times into an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix. And then apply the addition, subtraction, and multiplication of division element wise.</p>
<p>If conversely, you were to take the <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and add, subtract, multiply, divide by an <span class="math notranslate nohighlight">\((m,1)\)</span> matrix, then also this would copy it now <span class="math notranslate nohighlight">\(n\)</span> times. And turn that into an <span class="math notranslate nohighlight">\((m,n)\)</span> matrix and then apply the operation element wise.</p>
</li>
<li><p>Just one of the broadcasting, which is if you have an <span class="math notranslate nohighlight">\((m,1)\)</span> matrix, so that’s really a column vector like <span class="math notranslate nohighlight">\([1,2,3]^T\)</span>, and you add, subtract, multiply or divide by a row number. So maybe a <span class="math notranslate nohighlight">\((1,1)\)</span> matrix. So such as that plus 100, then you end up copying this real number <span class="math notranslate nohighlight">\(m\)</span> times until you’ll also get another <span class="math notranslate nohighlight">\((m,1)\)</span> matrix. And then you perform the operation such as addition on this example element-wise. And something similar also works for row vectors.</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\left[
\begin{matrix}
1 \\
2 \\
3 
\end{matrix}
\right] + 100 = 
\left[
\begin{matrix}
101 \\
102 \\
103 
\end{matrix}
\right] \end{split}\]</div>
<div class="math notranslate nohighlight">
\[\left[
\begin{matrix}
1  &amp;  2  &amp; 3 
\end{matrix}
\right] + 100 = 
\left[
\begin{matrix}
101 &amp;  102  &amp; 103 
\end{matrix}
\right] \]</div>
<p>So, that was broadcasting in Python. I hope that when you do the programming homework that broadcasting will allow you to not only make a code run faster, but also help you get what you want done with fewer lines of code.</p>
</section>
</section>
<section id="a-note-on-python-numpy-vectors">
<h3>A Note on Python/Numpy Vectors<a class="headerlink" href="#a-note-on-python-numpy-vectors" title="Link to this heading">#</a></h3>
<p><a class="reference external" href="https://youtu.be/Yx8LWTEKaxg">Video</a></p>
<p><a class="reference external" href="https://youtu.be/TPRB5n9ckQs">Quick tour of Jupyter/iPython Notebooks</a></p>
<p>The ability of python to allow you to use broadcasting operations and more generally, the great flexibility of the python numpy program language is both a strength as well as a weakness of the programming language.</p>
<ul class="simple">
<li><p>The strength because they create expressivity of the language. A great flexibility of the language lets you get a lot done even with just a single line of code.</p></li>
<li><p>But there’s also weakness because with broadcasting and this great amount of flexibility, sometimes it’s possible you can introduce very subtle bugs or very strange looking bugs, if you’re not familiar with all of the intricacies of how broadcasting and how features like broadcasting work.</p>
<ul>
<li><p>For example, if you take a column vector and add it to a row vector, you would expect it to throw up a dimension mismatch or type error or something. But you might actually get back a matrix as a sum of a row vector and a column vector.</p></li>
</ul>
</li>
</ul>
<p>There is an internal logic to these strange effects of Python. But if you’re not familiar with Python, I’ve seen some students have very strange, very hard to find bugs. So in this section is share with you some couple tips and tricks that have been very useful for me to eliminate or simplify and eliminate all the strange looking bugs in my own code. And I hope that with these tips and tricks, you’ll also be able to much more easily write bug-free, python and numpy code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.03550454 0.62634242 0.01162295 0.47968223 0.952829  ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(5,)
</pre></div>
</div>
</div>
</div>
<p>From the coding results, the shape of <code class="docutils literal notranslate"><span class="pre">a</span></code> is <span class="math notranslate nohighlight">\((5,\ )\)</span> structure. This is called a rank 1 array in Python and it’s neither a row vector nor a column vector. And this leads it to have some slightly non-intuitive effects.  For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[0.03550454 0.62634242 0.01162295 0.47968223 0.952829  ]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.53167863641126
</pre></div>
</div>
</div>
</div>
<p>If I print <code class="docutils literal notranslate"><span class="pre">a</span></code> transpose, it ends up looking the same as <code class="docutils literal notranslate"><span class="pre">a</span></code>. So <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">a.T</span></code> end up looking the same. And if I print the inner product between <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">a.T</span></code>, you might think <code class="docutils literal notranslate"><span class="pre">a</span></code> times <code class="docutils literal notranslate"><span class="pre">a.T</span></code> is maybe the outer product should give you matrix maybe. But the result above shows that, you instead get back a number. So what I would recommend is that when you’re coding new networks, that you just not use data structures where the shape is <span class="math notranslate nohighlight">\((5,)\)</span> or <span class="math notranslate nohighlight">\((n,)\)</span> - a rank 1 array. Instead, if you set <code class="docutils literal notranslate"><span class="pre">a</span></code> to be (5,1) like below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.60765632]
 [0.1305282 ]
 [0.26909222]
 [0.6254817 ]
 [0.41021334]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.60765632 0.1305282  0.26909222 0.6254817  0.41021334]]
</pre></div>
</div>
</div>
</div>
<p>Then this commits a to be <span class="math notranslate nohighlight">\((5,1)\)</span> column vector. Now <code class="docutils literal notranslate"><span class="pre">a</span></code> transpose is a row vector. Notice one subtle difference. In this data structure, there are two square brackets when we print <code class="docutils literal notranslate"><span class="pre">a.T</span></code>. Whereas previously, there was one square bracket. So that’s the difference between this is really a 1 by 5 matrix versus one of these rank 1 arrays.</p>
<p>And if you print the product between <code class="docutils literal notranslate"><span class="pre">a</span></code> and <code class="docutils literal notranslate"><span class="pre">a.T</span></code>, then this gives you the outer product of a vector:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">a</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[0.36924621 0.07931628 0.16351559 0.38007791 0.24926873]
 [0.07931628 0.01703761 0.03512412 0.081643   0.05354441]
 [0.16351559 0.03512412 0.07241063 0.16831226 0.11038522]
 [0.38007791 0.081643   0.16831226 0.39122736 0.25658094]
 [0.24926873 0.05354441 0.11038522 0.25658094 0.16827498]]
</pre></div>
</div>
</div>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The first command that we ran, just now. And this created a data structure with <code class="docutils literal notranslate"><span class="pre">a.shape</span></code> was this funny thing <span class="math notranslate nohighlight">\((5,)\)</span> so this is called a rank 1 array. And this is a very funny data structure. It doesn’t behave consistently as either a row vector nor a column vector, which makes some of its effects nonintuitive. So what I’m going to recommend is that when you’re doing your programing exercises, or in fact when you’re implementing logistic regression or neural networks that you just do not use these rank 1 arrays.</p>
<p>Instead, if every time you create an array, you commit to making it either a column vector, so this creates a <span class="math notranslate nohighlight">\((5,1)\)</span> vector, or commit to making it a row vector, then the behavior of your vectors may be easier to understand.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>   <span class="c1">#DO NOT USE</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>One more thing that I do a lot in my code is if I’m not entirely sure what’s the dimension of one of my vectors, I’ll often throw in an assertion statement like this,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
<p>to make sure, in this case, that this is a <span class="math notranslate nohighlight">\((5,1)\)</span> vector. So this is a column vector. These assertions are really inexpensive to execute, and they also help to serve as documentation for your code. So don’t hesitate to throw in assertion statements like this whenever you feel like.</p>
<p>And then finally, if for some reason you do end up with a rank 1 array, You can reshape this.</p>
</section>
</section>
<section id="quiz">
<h2>Quiz<a class="headerlink" href="#quiz" title="Link to this heading">#</a></h2>
<ol class="arabic">
<li><p>In logistic regression given <span class="math notranslate nohighlight">\(x\)</span>, and parameters <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}, \ b \in \mathbb{R}\)</span>. How do we generate the output <span class="math notranslate nohighlight">\(\hat{y}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(\sigma(wx)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\text{tanh}(wx+b)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\sigma(wx + b)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(wx + b\)</span></p>
</li>
<li><p>In logistic regression given the input <span class="math notranslate nohighlight">\(x\)</span> and parameters <span class="math notranslate nohighlight">\(w \in \mathbb{R}^{n_x}, \ b \in \mathbb{R}\)</span>. Which of the following best expresses what we want <span class="math notranslate nohighlight">\(\hat{y}\)</span> to tell us?</p>
<p>A. <span class="math notranslate nohighlight">\(\sigma(wx)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\mathrm{P}(y=1 \mid x)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\mathrm{P}(y=\hat{y} \mid x)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\sigma(wx + b)\)</span></p>
</li>
<li><p>Which of these is the “Logistic Loss”?</p>
<p>A. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \lvert y^{(i)} - \hat{y}^{(i)} \rvert\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \text{max}(0, y^{(i)} - \hat{y}^{(i)})\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = - \Big( y^{(i)} \  \text{log}(\hat{y}^{(i)}) + (1-y^{(i)}) \ \text{log}(1-\hat{y}^{(i)}) \Big)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\mathcal{L}^{(i)}(\hat{y}^{(i)}, y^{(i)}) = \lvert y^{(i)} - \hat{y}^{(i)} \rvert^{2}\)</span></p>
</li>
<li><p>Suppose that <span class="math notranslate nohighlight">\(\hat{y} = 0.5\)</span> and <span class="math notranslate nohighlight">\(y = 0\)</span>. What is the value of the “Logistic Loss”? Choose the best option.</p>
<p>A. <span class="math notranslate nohighlight">\(+ \infty\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(0.5\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(0.693\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\mathcal{L}(\hat{y}, y) = - \Big( y \  \text{log}(\hat{y}) + (1-y) \ \text{log}(1-\hat{y}) \Big)\)</span></p>
</li>
<li><p>Suppose <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is a <span class="math notranslate nohighlight">\((8, 1)\)</span> array. Which of the following is a valid reshape?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">x.reshape(1,</span> <span class="pre">4,</span> <span class="pre">3)</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">x.reshape(2,</span> <span class="pre">2,</span> <span class="pre">2)</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">x.reshape(-1,</span> <span class="pre">3)</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">x.reshape(2,</span> <span class="pre">4,</span> <span class="pre">4)</span></code></p>
</li>
<li><p>Conside the Numpy array <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>: <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">np.array([[[1],[2]],[[3],[4]]])</span></code>. What is the shape of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((4,)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((1, 2, 2)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((2, 2, 1)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((2, 2)\)</span></p>
</li>
<li><p>Consider the following random arrays <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># a.shape = (3, 4)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>  <span class="c1"># a.shape = (1, 4)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be the shape of <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. The computation cannot happen because it is not possible to broadcast more than one dimension.</p>
<p>B. c.shape = <span class="math notranslate nohighlight">\((3, 1)\)</span></p>
<p>C. c.shape = <span class="math notranslate nohighlight">\((1, 4)\)</span></p>
<p>D. c.shape = <span class="math notranslate nohighlight">\((3, 4)\)</span></p>
</li>
<li><p>Consider the following random arrays <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># a.shape = (2, 3)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># a.shape = (2, 1)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be the shape of <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. The computation cannot happen because the sizes do not match. It’s going to be “Error”!</p>
<p>B. c.shape = <span class="math notranslate nohighlight">\((2, 1)\)</span></p>
<p>C. c.shape = <span class="math notranslate nohighlight">\((2, 3)\)</span></p>
<p>D. c.shape = <span class="math notranslate nohighlight">\((3, 2)\)</span></p>
</li>
<li><p>Consider the following random arrays <span class="math notranslate nohighlight">\(a\)</span>, <span class="math notranslate nohighlight">\(b\)</span> and <span class="math notranslate nohighlight">\(c\)</span>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># a.shape = (4, 3)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># a.shape = (1, 3)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be the shape of <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. The computation cannot happen because the sizes do not match.</p>
<p>B. c.shape = <span class="math notranslate nohighlight">\((1, 3)\)</span></p>
<p>C. The computation cannot happen because it is not possible to broadcast more than one dimension.</p>
<p>D. c.shape = <span class="math notranslate nohighlight">\((4, 3)\)</span></p>
</li>
<li><p>Suppose you have <span class="math notranslate nohighlight">\(n_x\)</span> onput features per example. Recall that <span class="math notranslate nohighlight">\(\boldsymbol{X} = [x^{(1)} \  x^{(2)} \  \cdots \  x^{(m)}]\)</span>. What is the dimension of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((m, 1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((m, n_x)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((1, m)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((n_x, m)\)</span></p>
</li>
<li><p>Suppose our input batch consists of 8 grayscale images, each of dimension <span class="math notranslate nohighlight">\(8\times8\)</span>. We reshape these images into feature column vectors <span class="math notranslate nohighlight">\(x^{j}\)</span>. Remember that <span class="math notranslate nohighlight">\(\boldsymbol{X} = [x^{(1)} \  x^{(2)} \  \cdots \  x^{(8)}]\)</span>. What is the dimension of <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((512, 1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\((8, 64)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((64, 8)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((8, 8, 8)\)</span></p>
</li>
<li><p>Consider the following array:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
</pre></div>
</div>
<p>What is the result of <code class="docutils literal notranslate"><span class="pre">a*a</span></code>?</p>
<p>A. The computation cannot happen because the sizes do not match. It’s going to be “Error”!</p>
<p>B. <span class="math notranslate nohighlight">\(\begin{pmatrix} 4 &amp; 2 \\ 6 &amp;6\\ \end{pmatrix}\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(\begin{pmatrix} 4 &amp; 1 \\ 1 &amp;9\\ \end{pmatrix}\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(\begin{pmatrix} 5 &amp; 5 \\ 5 &amp;10\\ \end{pmatrix}\)</span></p>
</li>
<li><p>Consider the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</pre></div>
</div>
<p>How do you vectorize this?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">*</span> <span class="pre">b</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b.T</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">np.dot(a,</span> <span class="pre">b)</span></code></p>
</li>
<li><p>Consider the following code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
</pre></div>
</div>
<p>How do you vectorize this?</p>
<p>A. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">b.T</span></code></p>
<p>B. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">+</span> <span class="pre">b</span></code></p>
<p>C. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a.T</span> <span class="pre">+</span> <span class="pre">b.T</span></code></p>
<p>D. <code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">+</span> <span class="pre">b</span></code></p>
</li>
<li><p>Consider the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> 
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span>
</pre></div>
</div>
<p>What will be <span class="math notranslate nohighlight">\(c\)</span>?</p>
<p>A. It will lead to an error since you cannot use <code class="docutils literal notranslate"><span class="pre">*</span></code> to operate on these two matrices. You need to instead use <code class="docutils literal notranslate"><span class="pre">np.dot(a,</span> <span class="pre">b)</span></code>.</p>
<p>B. This will multiply a <span class="math notranslate nohighlight">\(3\times3\)</span> matrix a with a <span class="math notranslate nohighlight">\(3\times1\)</span> vector, thus resulting in a <span class="math notranslate nohighlight">\(3\times1\)</span> vector. That is, c.shape = <span class="math notranslate nohighlight">\((3, 1)\)</span>.</p>
<p>C. This will invoke broadcasting, so <code class="docutils literal notranslate"><span class="pre">b</span></code> is copied three times to become <span class="math notranslate nohighlight">\((3, 3)\)</span>, and <code class="docutils literal notranslate"><span class="pre">*</span></code> is an element-wise product so c.shape will be <span class="math notranslate nohighlight">\((3, 3)\)</span>.</p>
<p>D. This will invoke broadcasting, so <code class="docutils literal notranslate"><span class="pre">b</span></code> is copied three times to become <span class="math notranslate nohighlight">\((3, 3)\)</span>, and <code class="docutils literal notranslate"><span class="pre">*</span></code> invokes a matrix multiplication operation of two <span class="math notranslate nohighlight">\(3\times3\)</span> matrices so c.shape will be <span class="math notranslate nohighlight">\((3, 3)\)</span>.</p>
</li>
<li><p>Consider the code snippet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> 
<span class="n">b</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="o">.</span><span class="n">T</span> <span class="o">**</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Which of the following gives an equivalent output for <span class="math notranslate nohighlight">\(c\)</span>?</p>
</li>
</ol>
<p>A.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>B.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>C.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
     <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
          <span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">i</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></div>
</div>
<p>D. The computation cannot happen because the sizes do not match. It’s going to be “Error”!</p>
<ol class="arabic" start="17">
<li><p>Consider the following computational graph:</p>
<figure class="align-default" id="e17">
<a class="reference internal image-reference" href="_images/2-e17.png"><img alt="_images/2-e17.png" src="_images/2-e17.png" style="height: 200px;" /></a>
</figure>
<p>What is the output of <span class="math notranslate nohighlight">\(J\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\((a + c)(b - 1)\)</span></p>
<p>B. <span class="math notranslate nohighlight">\(ab + bc + ac\)</span></p>
<p>C. <span class="math notranslate nohighlight">\((c - 1)(a + c)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\((a - 1)(b + c)\)</span></p>
</li>
<li><p>Consider the following computational graph:</p>
<figure class="align-default" id="e18">
<a class="reference internal image-reference" href="_images/2-e18.png"><img alt="_images/2-e18.png" src="_images/2-e18.png" style="height: 200px;" /></a>
</figure>
<p>What is the output of <span class="math notranslate nohighlight">\(J\)</span>?</p>
<p>A. <span class="math notranslate nohighlight">\(J = a\times b + b \times c + a \times c \)</span></p>
<p>B. <span class="math notranslate nohighlight">\(J = (c - 1)(b + a)\)</span></p>
<p>C. <span class="math notranslate nohighlight">\(J = (a - 1)(b + c)\)</span></p>
<p>D. <span class="math notranslate nohighlight">\(J = (b - 1)(c + a)\)</span></p>
</li>
</ol>
<div class="tip dropdown admonition">
<p class="admonition-title">Click here for answers!</p>
<ol class="arabic">
<li><p>C </br></p>
<p>B. Yes. in logisitc regression we use a linear function <span class="math notranslate nohighlight">\(wx + b\)</span> followed by the sigmoid function <span class="math notranslate nohighlight">\(\sigma\)</span>, to get an output <span class="math notranslate nohighlight">\(y\)</span>, referred to as <span class="math notranslate nohighlight">\(\hat{y}\)</span>, such that <span class="math notranslate nohighlight">\(0 &lt; \hat{y} &lt; 1\)</span>. </br></p>
</li>
<li><p>B </br></p>
<p>B. Yes. We want the output <span class="math notranslate nohighlight">\(\hat{y}\)</span> to tell us the probability that <span class="math notranslate nohighlight">\(y=1\)</span> given <span class="math notranslate nohighlight">\(x\)</span>. </br></p>
</li>
<li><p>C </br></p></li>
<li><p>C </br></p></li>
<li><p>B </br></p>
<p>B. Yes. This generates uses <span class="math notranslate nohighlight">\(2\times2\times2 = 8\)</span> entries. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. This array has two rows and in each row it has 2 arrays of <span class="math notranslate nohighlight">\(1\times1\)</span>. </br></p>
</li>
<li><p>D </br></p>
<p>D. Yes. Broadcasting is used, so row <code class="docutils literal notranslate"><span class="pre">b</span></code> is copied 3 times so it can be summed to each row of <code class="docutils literal notranslate"><span class="pre">a</span></code>. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. This is broadcasting.  <code class="docutils literal notranslate"><span class="pre">b</span></code> (column vector) is copied 3 times so that it can be summed to each column of <code class="docutils literal notranslate"><span class="pre">a</span></code>. </br></p>
</li>
<li><p>D </br></p>
<p>D. Yes. Broadcasting is invoked, so row <code class="docutils literal notranslate"><span class="pre">b</span></code> is multiplied element-wise each row of <code class="docutils literal notranslate"><span class="pre">a</span></code> to create <code class="docutils literal notranslate"><span class="pre">c</span></code>. </br></p>
</li>
<li><p>D </br></p></li>
<li><p>C </br></p>
<p>C. Yes. After converting the <span class="math notranslate nohighlight">\(8\times8\)</span> gray scale images to a column vector we get a vector of size 64, thus <span class="math notranslate nohighlight">\(\boldsymbol{X}\)</span> has dimension <span class="math notranslate nohighlight">\((64, 8)\)</span>. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. Recall that * indicates element-wise multiplication. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. <code class="docutils literal notranslate"><span class="pre">b.T</span></code> gives a column vector with shape <span class="math notranslate nohighlight">\((1, 4)\)</span>. The result of <code class="docutils literal notranslate"><span class="pre">c</span></code> is equivalent to broadcasting <code class="docutils literal notranslate"><span class="pre">a*b.T</span></code>. </br></p>
</li>
<li><p>C </br></p>
<p>C. Yes. <code class="docutils literal notranslate"><span class="pre">a[j][i]</span></code> being used fo <code class="docutils literal notranslate"><span class="pre">a[i][j]</span></code> indicates we are using <code class="docutils literal notranslate"><span class="pre">a.T</span></code> and the element in the row j is used in the column j thus we are using <code class="docutils literal notranslate"><span class="pre">b.T</span></code>. </br></p>
</li>
<li><p>C </br></p></li>
<li><p>C </br></p>
<p>C. Yes. Notice that to operate with <code class="docutils literal notranslate"><span class="pre">b.T</span></code> we need to use <code class="docutils literal notranslate"><span class="pre">b[j][i]</span></code>. </br></p>
</li>
<li><p>A </br></p></li>
<li><p>C </br></p></li>
</ol>
</div>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="C1.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">1 Introduction to Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="C2_Practical.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pre-Practical: Python Basics with Numpy (optional assignment)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regreesion-as-a-neural-network">Logistic Regreesion as a Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification">Binary Classification</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-cost-function">Logistic Regression Cost Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-logisitic-regression-cost-function-optional">Explanation of Logisitic Regression Cost Function (Optional)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#logisitc-regression-cost-function">Logisitc regression cost function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-on-m-example">Cost on m example</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives">Derivatives</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-derivative-examples">More Derivative Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computation-graph">Computation Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-gradient-descent">Logistic Regression Gradient Descent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-on-m-examples">Gradient Descent on m Examples</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-and-vectoraization">Python and Vectoraization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorization">Vectorization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#more-vectorization-examples">More Vectorization Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-and-matrix-valued-functions">Vectors and matrix valued functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-implementation">Gradient descent implementation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorizting-logistic-regression">Vectorizting Logistic Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vectorizing-logistic-regresion-s-gradient-output">Vectorizing Logistic Regresion’s Gradient Output</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting-in-python">Broadcasting in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#broadcasting-examples">Broadcasting examples</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#general-principle">General Principle</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-note-on-python-numpy-vectors">A Note on Python/Numpy Vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quiz">Quiz</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>