
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Practical 5: Initialization &#8212; Deep Learning Specialization</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'C5_Initialization';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Practical 6: Regularization" href="C5_Regularization.html" />
    <link rel="prev" title="5 Practical Aspects of Deep Learning" href="C5.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Deep Learning Specialization - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Deep Learning Specialization - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to Deep Learning Specialization
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="C1.html">1 Introduction to Deep Learning</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C2.html">2 Neural Networks Basics</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C2_Practical.html">Pre-Practical: Python Basics with Numpy (optional assignment)</a></li>
<li class="toctree-l2"><a class="reference internal" href="C2_Practical_Test.html">Practicel 1: Logistic Regression with a Neural Network mindset</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C3.html">3 Shallow Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C3_Practical_Test.html">Practical 2: Planar data classification with one hidden layer</a></li>


</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="C4.html">4 Deep Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="C4_Practical_Test_P1.html">Practical 3: Building your Deep Neural Network: Step by Step</a></li>
<li class="toctree-l2"><a class="reference internal" href="C4_Practical_Test_P2.html">Practical 4: Deep Neural Network for Image Classification: Application</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="C5.html">5 Practical Aspects of Deep Learning</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Practical 5: Initialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="C5_Regularization.html">Practical 6: Regularization</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FC5_Initialization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/C5_Initialization.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Practical 5: Initialization</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packages">1 - Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">2 - Loading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-model">3 - Neural Network Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-initialization">4 - Zero Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-initialize-parameters-zeros">Exercise 1 - initialize_parameters_zeros</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-initialization">5 - Random Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-initialize-parameters-random">Exercise 2 - initialize_parameters_random</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#he-initialization">6 - He Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-initialize-parameters-he">Exercise 3 - initialize_parameters_he</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">7 - Conclusions</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="practical-5-initialization">
<h1>Practical 5: Initialization<a class="headerlink" href="#practical-5-initialization" title="Link to this heading">#</a></h1>
<p>Training your neural network requires specifying an initial value of the weights. A well-chosen initialization method helps the learning process.</p>
<p>If you completed the previous course of this specialization, you probably followed the instructions for weight initialization, and seen that it’s worked pretty well so far. But how do you choose the initialization for a new neural network? In this notebook, you’ll try out a few different initializations, including random, zeros, and He initialization, and see how each leads to different results.</p>
<p>A well-chosen initialization can:</p>
<ul class="simple">
<li><p>Speed up the convergence of gradient descent</p></li>
<li><p>Increase the odds of gradient descent converging to a lower training (and generalization) error</p></li>
</ul>
<p>Let’s get started!</p>
<section id="packages">
<h2>1 - Packages<a class="headerlink" href="#packages" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">sklearn.datasets</span>
<span class="kn">from</span> <span class="nn">init_utils_c2w1a1</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">relu</span><span class="p">,</span> <span class="n">compute_loss</span><span class="p">,</span> <span class="n">forward_propagation</span><span class="p">,</span> <span class="n">backward_propagation</span>
<span class="kn">from</span> <span class="nn">init_utils_c2w1a1</span> <span class="kn">import</span> <span class="n">update_parameters</span><span class="p">,</span> <span class="n">predict</span><span class="p">,</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">plot_decision_boundary</span><span class="p">,</span> <span class="n">predict_dec</span>

<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mf">7.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">)</span> <span class="c1"># set default size of plots</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.interpolation&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;nearest&#39;</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;image.cmap&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;gray&#39;</span>

<span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2

<span class="c1"># load image dataset: blue/red dots in circles</span>
<span class="c1"># train_X, train_Y, test_X, test_Y = load_dataset()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="loading-the-dataset">
<h2>2 - Loading the Dataset<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/78b92f161a025e8f8e4f8eebc98c3db03235993c3b2b4ac377a293562a2ae051.png" src="_images/78b92f161a025e8f8e4f8eebc98c3db03235993c3b2b4ac377a293562a2ae051.png" />
</div>
</div>
<p>For this classifier, you want to separate the blue dots from the red dots.</p>
</section>
<section id="neural-network-model">
<h2>3 - Neural Network Model<a class="headerlink" href="#neural-network-model" title="Link to this heading">#</a></h2>
<p>You’ll use a 3-layer neural network (already implemented for you). These are the initialization methods you’ll experiment with:</p>
<ul class="simple">
<li><p><em>Zeros initialization</em> –  setting <code class="docutils literal notranslate"><span class="pre">initialization</span> <span class="pre">=</span> <span class="pre">&quot;zeros&quot;</span></code> in the input argument.</p></li>
<li><p><em>Random initialization</em> – setting <code class="docutils literal notranslate"><span class="pre">initialization</span> <span class="pre">=</span> <span class="pre">&quot;random&quot;</span></code> in the input argument. This initializes the weights to large random values.</p></li>
<li><p><em>He initialization</em> – setting <code class="docutils literal notranslate"><span class="pre">initialization</span> <span class="pre">=</span> <span class="pre">&quot;he&quot;</span></code> in the input argument. This initializes the weights to random values scaled according to a paper by He et al., 2015.</p></li>
</ul>
<p><strong>Instructions</strong>: Instructions: Read over the code below, and run it. In the next part, you’ll implement the three initialization methods that this <code class="docutils literal notranslate"><span class="pre">model()</span></code> calls.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iterations</span> <span class="o">=</span> <span class="mi">15000</span><span class="p">,</span> <span class="n">print_cost</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;he&quot;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Implements a three-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.</span>
<span class="sd">    </span>
<span class="sd">    Arguments:</span>
<span class="sd">    X -- input data, of shape (2, number of examples)</span>
<span class="sd">    Y -- true &quot;label&quot; vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)</span>
<span class="sd">    learning_rate -- learning rate for gradient descent </span>
<span class="sd">    num_iterations -- number of iterations to run gradient descent</span>
<span class="sd">    print_cost -- if True, print the cost every 1000 iterations</span>
<span class="sd">    initialization -- flag to choose which initialization to use (&quot;zeros&quot;,&quot;random&quot; or &quot;he&quot;)</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- parameters learnt by the model</span>
<span class="sd">    &quot;&quot;&quot;</span>
        
    <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">costs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># to keep track of the loss</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># number of examples</span>
    <span class="n">layers_dims</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># Initialize parameters dictionary.</span>
    <span class="k">if</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_zeros</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;random&quot;</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_random</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">initialization</span> <span class="o">==</span> <span class="s2">&quot;he&quot;</span><span class="p">:</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_he</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>

    <span class="c1"># Loop (gradient descent)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">):</span>

        <span class="c1"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>
        <span class="n">a3</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="n">forward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
        
        <span class="c1"># Loss</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">a3</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

        <span class="c1"># Backward propagation.</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">backward_propagation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">cache</span><span class="p">)</span>
        
        <span class="c1"># Update parameters.</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">)</span>
        
        <span class="c1"># Print the loss every 1000 iterations</span>
        <span class="k">if</span> <span class="n">print_cost</span> <span class="ow">and</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Cost after iteration </span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
            <span class="n">costs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
            
    <span class="c1"># plot the loss</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">costs</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;cost&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iterations (per hundreds)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Learning rate =&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="zero-initialization">
<h2>4 - Zero Initialization<a class="headerlink" href="#zero-initialization" title="Link to this heading">#</a></h2>
<p>There are two types of parameters to initialize in a neural network:</p>
<ul class="simple">
<li><p>the weight matrices <span class="math notranslate nohighlight">\((W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})\)</span></p></li>
<li><p>the bias vectors <span class="math notranslate nohighlight">\((b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})\)</span></p></li>
</ul>
<section id="exercise-1-initialize-parameters-zeros">
<h3>Exercise 1 - initialize_parameters_zeros<a class="headerlink" href="#exercise-1-initialize-parameters-zeros" title="Link to this heading">#</a></h3>
<p>Implement the following function to initialize all parameters to zeros. You’ll see later that this does not work well since it fails to “break symmetry,” but try it anyway and see what happens. Use <code class="docutils literal notranslate"><span class="pre">np.zeros((..,..))</span></code> with the correct shapes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_zeros </span>

<span class="k">def</span> <span class="nf">initialize_parameters_zeros</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the size of each layer.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span>
<span class="sd">                    b1 -- bias vector of shape (layers_dims[1], 1)</span>
<span class="sd">                    ...</span>
<span class="sd">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span>
<span class="sd">                    bL -- bias vector of shape (layers_dims[L], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>            <span class="c1"># number of layers in the network</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = </span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] = </span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_zeros</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[0. 0. 0.]
 [0. 0. 0.]]
b1 = [[0.]
 [0.]]
W2 = [[0. 0.]]
b2 = [[0.]]
</pre></div>
</div>
</div>
</div>
<p>Run the following code to train your model on 15,000 iterations using zeros initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.6931471805599453
Cost after iteration 1000: 0.6931471805599453
Cost after iteration 2000: 0.6931471805599453
Cost after iteration 3000: 0.6931471805599453
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 4000: 0.6931471805599453
Cost after iteration 5000: 0.6931471805599453
Cost after iteration 6000: 0.6931471805599453
Cost after iteration 7000: 0.6931471805599453
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 8000: 0.6931471805599453
Cost after iteration 9000: 0.6931471805599453
Cost after iteration 10000: 0.6931471805599453
Cost after iteration 11000: 0.6931471805599453
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 12000: 0.6931471805599453
Cost after iteration 13000: 0.6931471805599453
Cost after iteration 14000: 0.6931471805599453
</pre></div>
</div>
<img alt="_images/a33578c59351da63463c61a1e05a2bbbbf48fcf861655eaa69029fcbe391a87f.png" src="_images/a33578c59351da63463c61a1e05a2bbbbf48fcf861655eaa69029fcbe391a87f.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.5
On the test set:
Accuracy: 0.5
</pre></div>
</div>
</div>
</div>
<p>The performance is terrible, the cost doesn’t decrease, and the algorithm performs no better than random guessing. Why? Take a look at the details of the predictions and the decision boundary:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;predictions_train = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">predictions_train</span><span class="p">))</span>
<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;predictions_test = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">predictions_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>predictions_train = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0]]
predictions_test = [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model with Zeros initialization&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c253b732069bc0bc104468614d8c3ba47f77cf095f89fb3c02142a72a137a5cf.png" src="_images/c253b732069bc0bc104468614d8c3ba47f77cf095f89fb3c02142a72a137a5cf.png" />
</div>
</div>
<p>For a comprehensive explanation of this, you can read <em>Paul Mielke</em>’s post, <a class="reference external" href="https://community.deeplearning.ai/t/symmetry-breaking-versus-zero-initialization/16061">Symmetry Breaking versus Zero Initialization</a>.</p>
<p>A simple explanation is provided below:</p>
<p><strong>Note</strong>: For sake of simplicity calculations below are done using only one example at a time.</p>
<p>Since the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">0</span></code></p>
<div class="math notranslate nohighlight">
\[a = ReLU(z) = max(0, z) = 0\]</div>
<p>At the classification layer, where the activation function is sigmoid you then get (for either input):</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{ 1 + e^{-(z)}} = \frac{1}{2} = y_{pred}\]</div>
<p>As for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.</p>
<p>Your loss function:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(a, y) =  - y  \ln(y_{pred}) - (1-y)  \ln(1-y_{pred})\]</div>
<p>For <code class="docutils literal notranslate"><span class="pre">y=1</span></code>, <code class="docutils literal notranslate"><span class="pre">y_pred=0.5</span></code> it becomes:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(0, 1) =  - (1)  \ln(\frac{1}{2}) = 0.6931471805599453\]</div>
<p>For <code class="docutils literal notranslate"><span class="pre">y=0</span></code>, <code class="docutils literal notranslate"><span class="pre">y_pred=0.5</span></code> it becomes:</p>
<div class="math notranslate nohighlight">
\[ \mathcal{L}(0, 0) =  - (1)  \ln(\frac{1}{2}) = 0.6931471805599453\]</div>
<p>As you can see with the prediction being 0.5 whether the actual (<code class="docutils literal notranslate"><span class="pre">y</span></code>) value is 1 or 0 you get the same loss value for both, so none of the weights get adjusted and you are stuck with the same old value of the weights.</p>
<p>This is why you can see that the model is predicting 0 for every example! No wonder it’s doing so badly.</p>
<p>In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so you might as well be training a neural network with <span class="math notranslate nohighlight">\(n^{[l]}=1\)</span> for every layer. This way, the network is no more powerful than a linear classifier like logistic regression.</p>
<div class="warning admonition">
<p class="admonition-title">What you should remember</p>
<ul class="simple">
<li><p>The weights <span class="math notranslate nohighlight">\(W^{[l]}\)</span> should be initialized randomly to break symmetry.</p></li>
<li><p>However, it’s okay to initialize the biases <span class="math notranslate nohighlight">\(b^{[l]}\)</span> to zeros. Symmetry is still broken so long as <span class="math notranslate nohighlight">\(W^{[l]}\)</span> is initialized randomly.</p></li>
</ul>
</div>
</section>
</section>
<section id="random-initialization">
<h2>5 - Random Initialization<a class="headerlink" href="#random-initialization" title="Link to this heading">#</a></h2>
<p>To break symmetry, initialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs. In this exercise, you’ll see what happens when the weights are initialized randomly, but to very large values.</p>
<section id="exercise-2-initialize-parameters-random">
<h3>Exercise 2 - initialize_parameters_random<a class="headerlink" href="#exercise-2-initialize-parameters-random" title="Link to this heading">#</a></h3>
<p>Implement the following function to initialize your weights to large random values (scaled by *10) and your biases to zeros. Use <code class="docutils literal notranslate"><span class="pre">np.random.randn(..,..)</span> <span class="pre">*</span> <span class="pre">10</span></code> for weights and <code class="docutils literal notranslate"><span class="pre">np.zeros((..,</span> <span class="pre">..))</span></code> for biases. You’re using a fixed <code class="docutils literal notranslate"><span class="pre">np.random.seed(..)</span></code> to make sure your “random” weights  match ours, so don’t worry if running your code several times always gives you the same initial values for the parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_random</span>

<span class="k">def</span> <span class="nf">initialize_parameters_random</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the size of each layer.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span>
<span class="sd">                    b1 -- bias vector of shape (layers_dims[1], 1)</span>
<span class="sd">                    ...</span>
<span class="sd">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span>
<span class="sd">                    bL -- bias vector of shape (layers_dims[L], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>               <span class="c1"># This seed makes sure your &quot;random&quot; numbers will be the as ours</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span>            <span class="c1"># integer representing the number of layers</span>
    
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = </span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] =</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="mi">10</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>

    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_random</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[ 17.88628473   4.36509851   0.96497468]
 [-18.63492703  -2.77388203  -3.54758979]]
b1 = [[0.]
 [0.]]
W2 = [[-0.82741481 -6.27000677]]
b2 = [[0.]]
</pre></div>
</div>
</div>
</div>
<p>Run the following code to train your model on 15,000 iterations using random initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;random&quot;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: inf
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1000: 0.6229180433609748
Cost after iteration 2000: 0.5977815578395076
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/chao/Downloads/DLS_C1/init_utils_c2w1a1.py:145: RuntimeWarning: divide by zero encountered in log
  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)
/Users/chao/Downloads/DLS_C1/init_utils_c2w1a1.py:145: RuntimeWarning: invalid value encountered in multiply
  logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 3000: 0.5635780597997223
Cost after iteration 4000: 0.5500778186719544
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 5000: 0.5443350780361031
Cost after iteration 6000: 0.5373501033809833
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 7000: 0.46969127036420355
Cost after iteration 8000: 0.3976580931050209
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 9000: 0.3934419967603351
Cost after iteration 10000: 0.39201738235219885
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 11000: 0.3891208899328107
Cost after iteration 12000: 0.3861245285297949
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 13000: 0.38496976596639154
Cost after iteration 14000: 0.3827514414987576
</pre></div>
</div>
<img alt="_images/b26456635ed02a62189289a0965cc74027809c94b2d0b61477c2da6c518fcfc4.png" src="_images/b26456635ed02a62189289a0965cc74027809c94b2d0b61477c2da6c518fcfc4.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.83
On the test set:
Accuracy: 0.86
</pre></div>
</div>
</div>
</div>
<p>If you see “inf” as the cost after the iteration 0, this is because of numerical roundoff. A more numerically sophisticated implementation would fix this, but for the purposes of this notebook, it isn’t really worth worrying about.</p>
<p>In any case, you’ve now broken the symmetry, and this gives noticeably better accuracy than before. The model is no longer outputting all 0s. Progress!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span> <span class="p">(</span><span class="n">predictions_train</span><span class="p">)</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">predictions_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[1 0 1 1 0 0 1 1 1 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 1 1 0 1 1 0 0 1
  1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1 1 0
  0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 0 1 1 0 1 1 0
  1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0
  0 0 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1 1 0 1 1 1
  1 0 1 0 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 1 0 1 0 1 0 0 1
  0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 0 0 1 1 0 1 1
  1 1 0 1 1 0 1 1 1 0 0 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1
  1 1 1 1 0 0 0 1 1 1 1 0]]
[[1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 1 1 1 0 0 0 0 1
  0 1 1 0 0 1 1 1 1 1 0 1 1 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 0
  1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 0 1 1 0 1 1 0 0]]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/678e035ab188806e520958f9fe697da5821cebe4085eaf6f8d971e7f09a57a23.png" src="_images/678e035ab188806e520958f9fe697da5821cebe4085eaf6f8d971e7f09a57a23.png" />
</div>
</div>
<p><strong>Observations</strong>:</p>
<ul class="simple">
<li><p>The cost starts very high. This is because with large random-valued weights, the last activation (sigmoid) outputs results that are very close to 0 or 1 for some examples, and when it gets that example wrong it incurs a very high loss for that example. Indeed, when <span class="math notranslate nohighlight">\(\log(a^{[3]}) = \log(0)\)</span>, the loss goes to infinity.</p></li>
<li><p>Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm.</p></li>
<li><p>If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.</p></li>
</ul>
<div class="warning admonition">
<p class="admonition-title">In summary</p>
<ul class="simple">
<li><p>Initializing weights to very large random values doesn’t work well.</p></li>
<li><p>Initializing with small random values should do better. The important question is, how small should be these random values be? Let’s find out up next!</p></li>
</ul>
</div>
<p><strong>Optional Read:</strong></p>
<p>The main difference between Gaussian variable (<code class="docutils literal notranslate"><span class="pre">numpy.random.randn()</span></code>) and uniform random variable is the distribution of the generated random numbers:</p>
<ul class="simple">
<li><p>numpy.random.rand() produces numbers in a <a class="reference external" href="https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/rand.jpg">uniform distribution</a>.</p></li>
<li><p>and numpy.random.randn() produces numbers in a <a class="reference external" href="https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/randn.jpg">normal distribution</a>.</p></li>
</ul>
<p>When used for weight initialization, randn() helps most the weights to Avoid being close to the extremes, allocating most of them in the center of the range.</p>
<p>An intuitive way to see it is, for example, if you take the <a class="reference external" href="https://raw.githubusercontent.com/jahnog/deeplearning-notes/master/Course2/images/sigmoid.jpg">sigmoid() activation function</a>.</p>
<p>You’ll remember that the slope near 0 or near 1 is extremely small, so the weights near those extremes will converge much more slowly to the solution, and having most of them near the center will speed the convergence.</p>
</section>
</section>
<section id="he-initialization">
<h2>6 - He Initialization<a class="headerlink" href="#he-initialization" title="Link to this heading">#</a></h2>
<p>Finally, try “He Initialization”; this is named for the first author of He et al., 2015. (If you have heard of “Xavier initialization”, this is similar except Xavier initialization uses a scaling factor for the weights <span class="math notranslate nohighlight">\(W^{[l]}\)</span> of <code class="docutils literal notranslate"><span class="pre">sqrt(1./layers_dims[l-1])</span></code> where He initialization would use <code class="docutils literal notranslate"><span class="pre">sqrt(2./layers_dims[l-1])</span></code>.)</p>
<section id="exercise-3-initialize-parameters-he">
<h3>Exercise 3 - initialize_parameters_he<a class="headerlink" href="#exercise-3-initialize-parameters-he" title="Link to this heading">#</a></h3>
<p>Implement the following function to initialize your parameters with He initialization. This function is similar to the previous <code class="docutils literal notranslate"><span class="pre">initialize_parameters_random(...)</span></code>. The only difference is that instead of multiplying <code class="docutils literal notranslate"><span class="pre">np.random.randn(..,..)</span></code> by 10, you will multiply it by <span class="math notranslate nohighlight">\(\sqrt{\frac{2}{\text{dimension of the previous layer}}}\)</span>, which is what He initialization recommends for layers with a ReLU activation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># GRADED FUNCTION: initialize_parameters_he</span>

<span class="k">def</span> <span class="nf">initialize_parameters_he</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Arguments:</span>
<span class="sd">    layer_dims -- python array (list) containing the size of each layer.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    parameters -- python dictionary containing your parameters &quot;W1&quot;, &quot;b1&quot;, ..., &quot;WL&quot;, &quot;bL&quot;:</span>
<span class="sd">                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])</span>
<span class="sd">                    b1 -- bias vector of shape (layers_dims[1], 1)</span>
<span class="sd">                    ...</span>
<span class="sd">                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])</span>
<span class="sd">                    bL -- bias vector of shape (layers_dims[L], 1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1"># integer representing the number of layers</span>
     
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">L</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1">#(≈ 2 lines of code)</span>
        <span class="c1"># parameters[&#39;W&#39; + str(l)] = </span>
        <span class="c1"># parameters[&#39;b&#39; + str(l)] =</span>
        <span class="c1"># YOUR CODE STARTS HERE</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;W&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">2.</span><span class="o">/</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s1">&#39;b&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">layers_dims</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        
        <span class="c1"># YOUR CODE ENDS HERE</span>
        
    <span class="k">return</span> <span class="n">parameters</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">initialize_parameters_he</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b1 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b1&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;W2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;W2&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;b2 = &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">parameters</span><span class="p">[</span><span class="s2">&quot;b2&quot;</span><span class="p">]))</span>

<span class="c1"># parameters</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W1 = [[ 1.78862847  0.43650985]
 [ 0.09649747 -1.8634927 ]
 [-0.2773882  -0.35475898]
 [-0.08274148 -0.62700068]]
b1 = [[0.]
 [0.]
 [0.]
 [0.]]
W2 = [[-0.03098412 -0.33744411 -0.92904268  0.62552248]]
b2 = [[0.]]
</pre></div>
</div>
</div>
</div>
<p>Run the following code to train your model on 15,000 iterations using He initialization.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">initialization</span> <span class="o">=</span> <span class="s2">&quot;he&quot;</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the train set:&quot;</span><span class="p">)</span>
<span class="n">predictions_train</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>

<span class="nb">print</span> <span class="p">(</span><span class="s2">&quot;On the test set:&quot;</span><span class="p">)</span>
<span class="n">predictions_test</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_Y</span><span class="p">,</span> <span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 0: 0.8830537463419761
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 1000: 0.6879825919728063
Cost after iteration 2000: 0.6751286264523371
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 3000: 0.6526117768893807
Cost after iteration 4000: 0.6082958970572938
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 5000: 0.5304944491717495
Cost after iteration 6000: 0.41386458170717944
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 7000: 0.31178034648444414
Cost after iteration 8000: 0.23696215330322562
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 9000: 0.18597287209206834
Cost after iteration 10000: 0.15015556280371808
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 11000: 0.12325079292273548
Cost after iteration 12000: 0.09917746546525932
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Cost after iteration 13000: 0.08457055954024276
Cost after iteration 14000: 0.07357895962677367
</pre></div>
</div>
<img alt="_images/52ea93ba66ced89cb46d825053f5a727c5f7a4e9f0c3bbe86bb8ca31efb08068.png" src="_images/52ea93ba66ced89cb46d825053f5a727c5f7a4e9f0c3bbe86bb8ca31efb08068.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>On the train set:
Accuracy: 0.9933333333333333
On the test set:
Accuracy: 0.96
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model with He initialization&quot;</span><span class="p">)</span>
<span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">axes</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span><span class="mf">1.5</span><span class="p">])</span>
<span class="n">plot_decision_boundary</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">predict_dec</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">train_X</span><span class="p">,</span> <span class="n">train_Y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d39c7f928e9d715ad7ae2513fdd3e874b32044c7ed5647c783e29eeb9d5a1c38.png" src="_images/d39c7f928e9d715ad7ae2513fdd3e874b32044c7ed5647c783e29eeb9d5a1c38.png" />
</div>
</div>
<p><strong>Observations</strong>:</p>
<ul class="simple">
<li><p>The model with He initialization separates the blue and the red dots very well in a small number of iterations.</p></li>
</ul>
</section>
</section>
<section id="conclusions">
<h2>7 - Conclusions<a class="headerlink" href="#conclusions" title="Link to this heading">#</a></h2>
<p><strong>Congratulations</strong>! You’ve completed this notebook on Initialization.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>Train accuracy</p></th>
<th class="head"><p>Problem/Comment</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3-layer NN with zeros initialization</p></td>
<td><p>50%</p></td>
<td><p>fails to break symmetry</p></td>
</tr>
<tr class="row-odd"><td><p>3-layer NN with large random initialization</p></td>
<td><p>83%</p></td>
<td><p>too large weights</p></td>
</tr>
<tr class="row-even"><td><p>3-layer NN with He initialization</p></td>
<td><p>99%</p></td>
<td><p>recommended method</p></td>
</tr>
</tbody>
</table>
<div class="warning admonition">
<p class="admonition-title">In summary</p>
<p>Here’s a quick recap of the main takeaways:</p>
<ul class="simple">
<li><p>Different initializations lead to very different results</p></li>
<li><p>Random initialization is used to break symmetry and make sure different hidden units can learn different things</p></li>
<li><p>Resist initializing to values that are too large!</p></li>
<li><p>He initialization works well for networks with ReLU activations</p></li>
</ul>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="C5.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">5 Practical Aspects of Deep Learning</p>
      </div>
    </a>
    <a class="right-next"
       href="C5_Regularization.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Practical 6: Regularization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#packages">1 - Packages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">2 - Loading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-model">3 - Neural Network Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-initialization">4 - Zero Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-initialize-parameters-zeros">Exercise 1 - initialize_parameters_zeros</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-initialization">5 - Random Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-initialize-parameters-random">Exercise 2 - initialize_parameters_random</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#he-initialization">6 - He Initialization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-3-initialize-parameters-he">Exercise 3 - initialize_parameters_he</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusions">7 - Conclusions</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Chao
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>